Notes on 'Hacker's guide to deep-learning side-channel attacks: the theory' [here](https://elie.net/blog/security/hacker-guide-to-deep-learning-side-channel-attacks-the-theory/)

# Overview
 - Guide for deep-learning-based side-channel attack
   - TinyAES implementation on STM32F415 ARM CPU
   - Uses power-consumption traces
 - Hardware-backed encryption common
   - Dedicated hardware to assist or replace software for encryption, e.g. instructions or co-processor
 - Team in partnership with Google developing 'deep-learning side-channel attacks (SCAAML)'

# What is a side-channel attack?
 - Implementation-specific attack
   - Different inputs cause different behavior
   - Can extract knowledge about secret value, e.g. cryptographic key
 - e.g. Timing attack -- time to execute algorithm may depend on characteristics of key
   - Naive RSA modular exponentiation runtime changes linearly with number of 1's in key
 - Popular measurements to take: power consumption, computation time, EM emissions
 - Generally do repeated measurements with different inputs

# Side-channel attacks main applications
 - One of most-effective ways to attack secure hardware
   - Algorithm usually scrutinized/secure but hardware less-so
   - Hard and complicated to avoid leaking information
   - Countermeasures increase cost and degrade performance
 - Generally agreed that almost all implementations are vulnerable to SCAs
   - Crypto-chip certification measures cost and skill to recover key
 - Can also be used to determine system output where not visible to attackers

# Side-channel attacks: brief and incomplete history
 - 1943 @ Bell Labs: when typing into encryption machine, spikes visible on oscilloscope across room
   - Can use to retrieve plaintext entered into machine
 - 1996: can extract private keys using timing attack
   - Led to power consumption attacks -- 'Differential Power Analysis' paper
 - 2017: Spectre vulnaribility in Intel CPU
   - Trick CPU into accessing private data, and read data using timing attack

# Deep learning in a nutshell
 - Techniques use 1D CNNs and RNNs for time-series data
 - Power traces -> key byte or an intermediate value from which it can be extracted
 - Advantages of deep learning for side-channel attacks:
   - Fewer human-engineered features/assumptions
   - Can directly predict some intermediate value -- no need for approximate models
   - Can accumulate outputs over multiple traces to probabilistically predict byte

# Power based side-channel attack overview
 - Known plaintext attack: attacker controls plaintext fed to encryption device but does not control key
 - Try to look at power consumption during encryption process to infer AES keys
 - 3 phases in attack
   1. Data collection: collect dataset of traces
   2. Training: train the ML model on this dataset
   3. Attack phase: use model to recover secret keys and evaluate resilience of implementation
      - Each prediction based on combinination of probability distributions of multiple traces
 - Attack recovers single byte at a time
   - Each byte in key requires additional attack and model

# Collecting power traces
 - Traces measured using oscilloscope controlled by computer
   - Group uses ChipWhisperer boards to facilitate experiments
   - External high-frequency oscilloscope to do oversampling for asynchronous collection on high-frequency chips
   - Traces stored with 8-bit precision
 - Computer triggers encryption of plaintext with chosen or random key
 - After encryption, (power trace, key, plaintext) appended to dataset
   - Group's research datasets consist of ~1.2M traces
   - 256 keys x 256 plaintexts sufficient for TinyAES and other unprotected implementations
   - Training corpus: freely choose key and plaintext
     - More traces needed for more-protected implementations
     - Group records ~1M traces -- takes several days
   - Evaluation corpus: random keys and different chip to ensure generalization
 - Group synchronously samples power traces
   - Intended to help chip designers fix vulnerabilities -- have access to chip clock
   - Practical attack would capture data asynchronously
     - Harder to train -- need 4x as much data

# Training phase
 - Train neural network or template using training data collected earlier
   - Traditionally -- template attacks fit human-engineered model to data using statistics
 - Pick attack point -- which values the attack predicts
   - Direct prediction of key bytes is obvious, but hard to do
   - More-effective to predict attack poits -- points in algorithm where computation causes memory change
     - Causes power consumption change, so visible in power trace
     - Direct relation to value we want to recover
   - AES algorithm has many attack points, but most are non-invertible
   - For TinyAES, group targets two attack points called sub_bytes_in and sub_bytes_out
 - Hard to find appropriate model architecture and hyperparameters
   - Both RNNs and CNNs can work
   - Group found ResNet v2 consistently works well on easy targets
   - Must scale inputs appropriately
   - Use softmax activation and categorical cross-entropy loss

# Attack phase
 - Try to recover keys not seen during training to evaluate model effectiveness
   - Keys drawn in random fashion to mimic real-world scenario
 - Accumulate probabilities over multiple traces
   - Directly sum over traces the probability of each key
   - Allows creation of ordered list of keys for easier brute-force attacks and nuanced evaluation of model performance
 - Cryptanalysis attacks (as opposed to SCAs) analyze success based on how much we reduce computational cost of breaking algorithm
   - Algorithm secure provided attack not technically feasible
 - In contrast, SCAs evaluated based on cost (equipment, development time) of attack and number of traces needed per key
   - Generally accepted that any implementation is vulnerable to SCAs
   - May use worst-, average-, best-case scenario
   - Sometimes represent success rate as function of number of traces
 
























