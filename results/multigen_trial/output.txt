Beginning trial described in ./config/multigen_trial.json.
Experiment type: multiple generators each corresponding to 1 key.
Experiment settings:
	byte: 0
	keys: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15]
	key_dataset_kwargs:
		keep_data_in_memory: True
		data_path: ./data
		download: True
		extract: True
		preprocess: True
		delete_download_after_extraction: False
		delete_extracted_after_preprocess: False
	dataloader_kwargs:
		batch_size: 16
		shuffle: True
	dataset_prop_for_validation: 0.2
	trace_map_constructor: <function get_mlp_map at 0x7f5a18b9a430>
	trace_map_kwargs:
		layers: [256]
		hidden_activation: <class 'torch.nn.modules.activation.ReLU'>
	plaintext_map_constructor: <function get_mlp_map at 0x7f5a18b9a430>
	plaintext_map_kwargs:
		layers: [64]
		hidden_activation: <class 'torch.nn.modules.activation.ReLU'>
	key_map_constructor: <function get_zero_map at 0x7f5a1c10b310>
	key_map_kwargs:
	cumulative_map_constructor: <function get_mlp_map at 0x7f5a18b9a430>
	cumulative_map_kwargs:
		layers: [256]
		hidden_activation: <class 'torch.nn.modules.activation.ReLU'>
	discriminator_constructor: <function get_google_style_resnet_discriminator at 0x7f5a18b9a5e0>
	discriminator_kwargs:
	discriminator_loss_constructor: <class 'torch.nn.modules.loss.CrossEntropyLoss'>
	discriminator_loss_kwargs:
	discriminator_optimizer_constructor: <class 'torch.optim.adam.Adam'>
	discriminator_optimizer_kwargs:
	generator_loss_constructor: <class 'torch.nn.modules.loss.CrossEntropyLoss'>
	generator_loss_kwargs:
	generator_optimizer_constructor: <class 'torch.optim.adam.Adam'>
	generator_optimizer_kwargs:
	device: cuda
	discriminator_pretraining_epochs: 100
	generator_pretraining_epochs: 0
	gan_training_epochs: 0
	discriminator_posttraining_epochs: 0
	seed: 0
Loading datasets.
AesKeyGroupDataset:
	Available keys: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15]
	Key transform: Compose(
    IntToBinary()
    ToTensor1D()
)
	Byte: 0
	Number of samples available: 10112
	Trace size: torch.Size([3000])
	Key size: torch.Size([8])
	Plaintext size: torch.Size([8])
	Key index size: ()
Constructing generator.
CompositeGenerator(
  (generators): ModuleList(
    (0): Generator(
      (trace_map): Sequential(
        (0): Flatten(start_dim=1, end_dim=-1)
        (1): Linear(in_features=3000, out_features=256, bias=True)
        (2): ReLU()
        (3): Linear(in_features=256, out_features=3000, bias=True)
        (4): Unflatten(dim=-1, unflattened_size=torch.Size([3000]))
      )
      (plaintext_map): Sequential(
        (0): Flatten(start_dim=1, end_dim=-1)
        (1): Linear(in_features=8, out_features=64, bias=True)
        (2): ReLU()
        (3): Linear(in_features=64, out_features=8, bias=True)
        (4): Unflatten(dim=-1, unflattened_size=torch.Size([8]))
      )
      (key_map): Sequential(
        (0): Flatten(start_dim=1, end_dim=-1)
        (1): Linear(in_features=8, out_features=8, bias=False)
        (2): Unflatten(dim=-1, unflattened_size=torch.Size([8]))
      )
      (cumulative_map): Sequential(
        (0): Flatten(start_dim=1, end_dim=-1)
        (1): Linear(in_features=3016, out_features=256, bias=True)
        (2): ReLU()
        (3): Linear(in_features=256, out_features=3000, bias=True)
        (4): Unflatten(dim=-1, unflattened_size=torch.Size([3000]))
      )
    )
    (1): Generator(
      (trace_map): Sequential(
        (0): Flatten(start_dim=1, end_dim=-1)
        (1): Linear(in_features=3000, out_features=256, bias=True)
        (2): ReLU()
        (3): Linear(in_features=256, out_features=3000, bias=True)
        (4): Unflatten(dim=-1, unflattened_size=torch.Size([3000]))
      )
      (plaintext_map): Sequential(
        (0): Flatten(start_dim=1, end_dim=-1)
        (1): Linear(in_features=8, out_features=64, bias=True)
        (2): ReLU()
        (3): Linear(in_features=64, out_features=8, bias=True)
        (4): Unflatten(dim=-1, unflattened_size=torch.Size([8]))
      )
      (key_map): Sequential(
        (0): Flatten(start_dim=1, end_dim=-1)
        (1): Linear(in_features=8, out_features=8, bias=False)
        (2): Unflatten(dim=-1, unflattened_size=torch.Size([8]))
      )
      (cumulative_map): Sequential(
        (0): Flatten(start_dim=1, end_dim=-1)
        (1): Linear(in_features=3016, out_features=256, bias=True)
        (2): ReLU()
        (3): Linear(in_features=256, out_features=3000, bias=True)
        (4): Unflatten(dim=-1, unflattened_size=torch.Size([3000]))
      )
    )
    (2): Generator(
      (trace_map): Sequential(
        (0): Flatten(start_dim=1, end_dim=-1)
        (1): Linear(in_features=3000, out_features=256, bias=True)
        (2): ReLU()
        (3): Linear(in_features=256, out_features=3000, bias=True)
        (4): Unflatten(dim=-1, unflattened_size=torch.Size([3000]))
      )
      (plaintext_map): Sequential(
        (0): Flatten(start_dim=1, end_dim=-1)
        (1): Linear(in_features=8, out_features=64, bias=True)
        (2): ReLU()
        (3): Linear(in_features=64, out_features=8, bias=True)
        (4): Unflatten(dim=-1, unflattened_size=torch.Size([8]))
      )
      (key_map): Sequential(
        (0): Flatten(start_dim=1, end_dim=-1)
        (1): Linear(in_features=8, out_features=8, bias=False)
        (2): Unflatten(dim=-1, unflattened_size=torch.Size([8]))
      )
      (cumulative_map): Sequential(
        (0): Flatten(start_dim=1, end_dim=-1)
        (1): Linear(in_features=3016, out_features=256, bias=True)
        (2): ReLU()
        (3): Linear(in_features=256, out_features=3000, bias=True)
        (4): Unflatten(dim=-1, unflattened_size=torch.Size([3000]))
      )
    )
    (3): Generator(
      (trace_map): Sequential(
        (0): Flatten(start_dim=1, end_dim=-1)
        (1): Linear(in_features=3000, out_features=256, bias=True)
        (2): ReLU()
        (3): Linear(in_features=256, out_features=3000, bias=True)
        (4): Unflatten(dim=-1, unflattened_size=torch.Size([3000]))
      )
      (plaintext_map): Sequential(
        (0): Flatten(start_dim=1, end_dim=-1)
        (1): Linear(in_features=8, out_features=64, bias=True)
        (2): ReLU()
        (3): Linear(in_features=64, out_features=8, bias=True)
        (4): Unflatten(dim=-1, unflattened_size=torch.Size([8]))
      )
      (key_map): Sequential(
        (0): Flatten(start_dim=1, end_dim=-1)
        (1): Linear(in_features=8, out_features=8, bias=False)
        (2): Unflatten(dim=-1, unflattened_size=torch.Size([8]))
      )
      (cumulative_map): Sequential(
        (0): Flatten(start_dim=1, end_dim=-1)
        (1): Linear(in_features=3016, out_features=256, bias=True)
        (2): ReLU()
        (3): Linear(in_features=256, out_features=3000, bias=True)
        (4): Unflatten(dim=-1, unflattened_size=torch.Size([3000]))
      )
    )
    (4): Generator(
      (trace_map): Sequential(
        (0): Flatten(start_dim=1, end_dim=-1)
        (1): Linear(in_features=3000, out_features=256, bias=True)
        (2): ReLU()
        (3): Linear(in_features=256, out_features=3000, bias=True)
        (4): Unflatten(dim=-1, unflattened_size=torch.Size([3000]))
      )
      (plaintext_map): Sequential(
        (0): Flatten(start_dim=1, end_dim=-1)
        (1): Linear(in_features=8, out_features=64, bias=True)
        (2): ReLU()
        (3): Linear(in_features=64, out_features=8, bias=True)
        (4): Unflatten(dim=-1, unflattened_size=torch.Size([8]))
      )
      (key_map): Sequential(
        (0): Flatten(start_dim=1, end_dim=-1)
        (1): Linear(in_features=8, out_features=8, bias=False)
        (2): Unflatten(dim=-1, unflattened_size=torch.Size([8]))
      )
      (cumulative_map): Sequential(
        (0): Flatten(start_dim=1, end_dim=-1)
        (1): Linear(in_features=3016, out_features=256, bias=True)
        (2): ReLU()
        (3): Linear(in_features=256, out_features=3000, bias=True)
        (4): Unflatten(dim=-1, unflattened_size=torch.Size([3000]))
      )
    )
    (5): Generator(
      (trace_map): Sequential(
        (0): Flatten(start_dim=1, end_dim=-1)
        (1): Linear(in_features=3000, out_features=256, bias=True)
        (2): ReLU()
        (3): Linear(in_features=256, out_features=3000, bias=True)
        (4): Unflatten(dim=-1, unflattened_size=torch.Size([3000]))
      )
      (plaintext_map): Sequential(
        (0): Flatten(start_dim=1, end_dim=-1)
        (1): Linear(in_features=8, out_features=64, bias=True)
        (2): ReLU()
        (3): Linear(in_features=64, out_features=8, bias=True)
        (4): Unflatten(dim=-1, unflattened_size=torch.Size([8]))
      )
      (key_map): Sequential(
        (0): Flatten(start_dim=1, end_dim=-1)
        (1): Linear(in_features=8, out_features=8, bias=False)
        (2): Unflatten(dim=-1, unflattened_size=torch.Size([8]))
      )
      (cumulative_map): Sequential(
        (0): Flatten(start_dim=1, end_dim=-1)
        (1): Linear(in_features=3016, out_features=256, bias=True)
        (2): ReLU()
        (3): Linear(in_features=256, out_features=3000, bias=True)
        (4): Unflatten(dim=-1, unflattened_size=torch.Size([3000]))
      )
    )
    (6): Generator(
      (trace_map): Sequential(
        (0): Flatten(start_dim=1, end_dim=-1)
        (1): Linear(in_features=3000, out_features=256, bias=True)
        (2): ReLU()
        (3): Linear(in_features=256, out_features=3000, bias=True)
        (4): Unflatten(dim=-1, unflattened_size=torch.Size([3000]))
      )
      (plaintext_map): Sequential(
        (0): Flatten(start_dim=1, end_dim=-1)
        (1): Linear(in_features=8, out_features=64, bias=True)
        (2): ReLU()
        (3): Linear(in_features=64, out_features=8, bias=True)
        (4): Unflatten(dim=-1, unflattened_size=torch.Size([8]))
      )
      (key_map): Sequential(
        (0): Flatten(start_dim=1, end_dim=-1)
        (1): Linear(in_features=8, out_features=8, bias=False)
        (2): Unflatten(dim=-1, unflattened_size=torch.Size([8]))
      )
      (cumulative_map): Sequential(
        (0): Flatten(start_dim=1, end_dim=-1)
        (1): Linear(in_features=3016, out_features=256, bias=True)
        (2): ReLU()
        (3): Linear(in_features=256, out_features=3000, bias=True)
        (4): Unflatten(dim=-1, unflattened_size=torch.Size([3000]))
      )
    )
    (7): Generator(
      (trace_map): Sequential(
        (0): Flatten(start_dim=1, end_dim=-1)
        (1): Linear(in_features=3000, out_features=256, bias=True)
        (2): ReLU()
        (3): Linear(in_features=256, out_features=3000, bias=True)
        (4): Unflatten(dim=-1, unflattened_size=torch.Size([3000]))
      )
      (plaintext_map): Sequential(
        (0): Flatten(start_dim=1, end_dim=-1)
        (1): Linear(in_features=8, out_features=64, bias=True)
        (2): ReLU()
        (3): Linear(in_features=64, out_features=8, bias=True)
        (4): Unflatten(dim=-1, unflattened_size=torch.Size([8]))
      )
      (key_map): Sequential(
        (0): Flatten(start_dim=1, end_dim=-1)
        (1): Linear(in_features=8, out_features=8, bias=False)
        (2): Unflatten(dim=-1, unflattened_size=torch.Size([8]))
      )
      (cumulative_map): Sequential(
        (0): Flatten(start_dim=1, end_dim=-1)
        (1): Linear(in_features=3016, out_features=256, bias=True)
        (2): ReLU()
        (3): Linear(in_features=256, out_features=3000, bias=True)
        (4): Unflatten(dim=-1, unflattened_size=torch.Size([3000]))
      )
    )
    (8): Generator(
      (trace_map): Sequential(
        (0): Flatten(start_dim=1, end_dim=-1)
        (1): Linear(in_features=3000, out_features=256, bias=True)
        (2): ReLU()
        (3): Linear(in_features=256, out_features=3000, bias=True)
        (4): Unflatten(dim=-1, unflattened_size=torch.Size([3000]))
      )
      (plaintext_map): Sequential(
        (0): Flatten(start_dim=1, end_dim=-1)
        (1): Linear(in_features=8, out_features=64, bias=True)
        (2): ReLU()
        (3): Linear(in_features=64, out_features=8, bias=True)
        (4): Unflatten(dim=-1, unflattened_size=torch.Size([8]))
      )
      (key_map): Sequential(
        (0): Flatten(start_dim=1, end_dim=-1)
        (1): Linear(in_features=8, out_features=8, bias=False)
        (2): Unflatten(dim=-1, unflattened_size=torch.Size([8]))
      )
      (cumulative_map): Sequential(
        (0): Flatten(start_dim=1, end_dim=-1)
        (1): Linear(in_features=3016, out_features=256, bias=True)
        (2): ReLU()
        (3): Linear(in_features=256, out_features=3000, bias=True)
        (4): Unflatten(dim=-1, unflattened_size=torch.Size([3000]))
      )
    )
    (9): Generator(
      (trace_map): Sequential(
        (0): Flatten(start_dim=1, end_dim=-1)
        (1): Linear(in_features=3000, out_features=256, bias=True)
        (2): ReLU()
        (3): Linear(in_features=256, out_features=3000, bias=True)
        (4): Unflatten(dim=-1, unflattened_size=torch.Size([3000]))
      )
      (plaintext_map): Sequential(
        (0): Flatten(start_dim=1, end_dim=-1)
        (1): Linear(in_features=8, out_features=64, bias=True)
        (2): ReLU()
        (3): Linear(in_features=64, out_features=8, bias=True)
        (4): Unflatten(dim=-1, unflattened_size=torch.Size([8]))
      )
      (key_map): Sequential(
        (0): Flatten(start_dim=1, end_dim=-1)
        (1): Linear(in_features=8, out_features=8, bias=False)
        (2): Unflatten(dim=-1, unflattened_size=torch.Size([8]))
      )
      (cumulative_map): Sequential(
        (0): Flatten(start_dim=1, end_dim=-1)
        (1): Linear(in_features=3016, out_features=256, bias=True)
        (2): ReLU()
        (3): Linear(in_features=256, out_features=3000, bias=True)
        (4): Unflatten(dim=-1, unflattened_size=torch.Size([3000]))
      )
    )
    (10): Generator(
      (trace_map): Sequential(
        (0): Flatten(start_dim=1, end_dim=-1)
        (1): Linear(in_features=3000, out_features=256, bias=True)
        (2): ReLU()
        (3): Linear(in_features=256, out_features=3000, bias=True)
        (4): Unflatten(dim=-1, unflattened_size=torch.Size([3000]))
      )
      (plaintext_map): Sequential(
        (0): Flatten(start_dim=1, end_dim=-1)
        (1): Linear(in_features=8, out_features=64, bias=True)
        (2): ReLU()
        (3): Linear(in_features=64, out_features=8, bias=True)
        (4): Unflatten(dim=-1, unflattened_size=torch.Size([8]))
      )
      (key_map): Sequential(
        (0): Flatten(start_dim=1, end_dim=-1)
        (1): Linear(in_features=8, out_features=8, bias=False)
        (2): Unflatten(dim=-1, unflattened_size=torch.Size([8]))
      )
      (cumulative_map): Sequential(
        (0): Flatten(start_dim=1, end_dim=-1)
        (1): Linear(in_features=3016, out_features=256, bias=True)
        (2): ReLU()
        (3): Linear(in_features=256, out_features=3000, bias=True)
        (4): Unflatten(dim=-1, unflattened_size=torch.Size([3000]))
      )
    )
    (11): Generator(
      (trace_map): Sequential(
        (0): Flatten(start_dim=1, end_dim=-1)
        (1): Linear(in_features=3000, out_features=256, bias=True)
        (2): ReLU()
        (3): Linear(in_features=256, out_features=3000, bias=True)
        (4): Unflatten(dim=-1, unflattened_size=torch.Size([3000]))
      )
      (plaintext_map): Sequential(
        (0): Flatten(start_dim=1, end_dim=-1)
        (1): Linear(in_features=8, out_features=64, bias=True)
        (2): ReLU()
        (3): Linear(in_features=64, out_features=8, bias=True)
        (4): Unflatten(dim=-1, unflattened_size=torch.Size([8]))
      )
      (key_map): Sequential(
        (0): Flatten(start_dim=1, end_dim=-1)
        (1): Linear(in_features=8, out_features=8, bias=False)
        (2): Unflatten(dim=-1, unflattened_size=torch.Size([8]))
      )
      (cumulative_map): Sequential(
        (0): Flatten(start_dim=1, end_dim=-1)
        (1): Linear(in_features=3016, out_features=256, bias=True)
        (2): ReLU()
        (3): Linear(in_features=256, out_features=3000, bias=True)
        (4): Unflatten(dim=-1, unflattened_size=torch.Size([3000]))
      )
    )
    (12): Generator(
      (trace_map): Sequential(
        (0): Flatten(start_dim=1, end_dim=-1)
        (1): Linear(in_features=3000, out_features=256, bias=True)
        (2): ReLU()
        (3): Linear(in_features=256, out_features=3000, bias=True)
        (4): Unflatten(dim=-1, unflattened_size=torch.Size([3000]))
      )
      (plaintext_map): Sequential(
        (0): Flatten(start_dim=1, end_dim=-1)
        (1): Linear(in_features=8, out_features=64, bias=True)
        (2): ReLU()
        (3): Linear(in_features=64, out_features=8, bias=True)
        (4): Unflatten(dim=-1, unflattened_size=torch.Size([8]))
      )
      (key_map): Sequential(
        (0): Flatten(start_dim=1, end_dim=-1)
        (1): Linear(in_features=8, out_features=8, bias=False)
        (2): Unflatten(dim=-1, unflattened_size=torch.Size([8]))
      )
      (cumulative_map): Sequential(
        (0): Flatten(start_dim=1, end_dim=-1)
        (1): Linear(in_features=3016, out_features=256, bias=True)
        (2): ReLU()
        (3): Linear(in_features=256, out_features=3000, bias=True)
        (4): Unflatten(dim=-1, unflattened_size=torch.Size([3000]))
      )
    )
    (13): Generator(
      (trace_map): Sequential(
        (0): Flatten(start_dim=1, end_dim=-1)
        (1): Linear(in_features=3000, out_features=256, bias=True)
        (2): ReLU()
        (3): Linear(in_features=256, out_features=3000, bias=True)
        (4): Unflatten(dim=-1, unflattened_size=torch.Size([3000]))
      )
      (plaintext_map): Sequential(
        (0): Flatten(start_dim=1, end_dim=-1)
        (1): Linear(in_features=8, out_features=64, bias=True)
        (2): ReLU()
        (3): Linear(in_features=64, out_features=8, bias=True)
        (4): Unflatten(dim=-1, unflattened_size=torch.Size([8]))
      )
      (key_map): Sequential(
        (0): Flatten(start_dim=1, end_dim=-1)
        (1): Linear(in_features=8, out_features=8, bias=False)
        (2): Unflatten(dim=-1, unflattened_size=torch.Size([8]))
      )
      (cumulative_map): Sequential(
        (0): Flatten(start_dim=1, end_dim=-1)
        (1): Linear(in_features=3016, out_features=256, bias=True)
        (2): ReLU()
        (3): Linear(in_features=256, out_features=3000, bias=True)
        (4): Unflatten(dim=-1, unflattened_size=torch.Size([3000]))
      )
    )
    (14): Generator(
      (trace_map): Sequential(
        (0): Flatten(start_dim=1, end_dim=-1)
        (1): Linear(in_features=3000, out_features=256, bias=True)
        (2): ReLU()
        (3): Linear(in_features=256, out_features=3000, bias=True)
        (4): Unflatten(dim=-1, unflattened_size=torch.Size([3000]))
      )
      (plaintext_map): Sequential(
        (0): Flatten(start_dim=1, end_dim=-1)
        (1): Linear(in_features=8, out_features=64, bias=True)
        (2): ReLU()
        (3): Linear(in_features=64, out_features=8, bias=True)
        (4): Unflatten(dim=-1, unflattened_size=torch.Size([8]))
      )
      (key_map): Sequential(
        (0): Flatten(start_dim=1, end_dim=-1)
        (1): Linear(in_features=8, out_features=8, bias=False)
        (2): Unflatten(dim=-1, unflattened_size=torch.Size([8]))
      )
      (cumulative_map): Sequential(
        (0): Flatten(start_dim=1, end_dim=-1)
        (1): Linear(in_features=3016, out_features=256, bias=True)
        (2): ReLU()
        (3): Linear(in_features=256, out_features=3000, bias=True)
        (4): Unflatten(dim=-1, unflattened_size=torch.Size([3000]))
      )
    )
    (15): Generator(
      (trace_map): Sequential(
        (0): Flatten(start_dim=1, end_dim=-1)
        (1): Linear(in_features=3000, out_features=256, bias=True)
        (2): ReLU()
        (3): Linear(in_features=256, out_features=3000, bias=True)
        (4): Unflatten(dim=-1, unflattened_size=torch.Size([3000]))
      )
      (plaintext_map): Sequential(
        (0): Flatten(start_dim=1, end_dim=-1)
        (1): Linear(in_features=8, out_features=64, bias=True)
        (2): ReLU()
        (3): Linear(in_features=64, out_features=8, bias=True)
        (4): Unflatten(dim=-1, unflattened_size=torch.Size([8]))
      )
      (key_map): Sequential(
        (0): Flatten(start_dim=1, end_dim=-1)
        (1): Linear(in_features=8, out_features=8, bias=False)
        (2): Unflatten(dim=-1, unflattened_size=torch.Size([8]))
      )
      (cumulative_map): Sequential(
        (0): Flatten(start_dim=1, end_dim=-1)
        (1): Linear(in_features=3016, out_features=256, bias=True)
        (2): ReLU()
        (3): Linear(in_features=256, out_features=3000, bias=True)
        (4): Unflatten(dim=-1, unflattened_size=torch.Size([3000]))
      )
    )
  )
)

Constructing discriminator.
Discriminator(
  (model): Sequential(
    (0): MaxPool1d(kernel_size=4, stride=4, padding=0, dilation=1, ceil_mode=False)
    (1): Stack(
      (F): Sequential(
        (0): Block(
          (F): Sequential(
            (0): BatchNorm1d(1, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (1): ReLU()
            (2): Conv1d(1, 16, kernel_size=(1,), stride=(1,), bias=False)
            (3): BatchNorm1d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (4): ReLU()
            (5): Conv1d(16, 16, kernel_size=(3,), stride=(1,), padding=(1,), bias=False)
            (6): BatchNorm1d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (7): ReLU()
            (8): Conv1d(16, 64, kernel_size=(1,), stride=(1,))
          )
          (Id): Conv1d(1, 64, kernel_size=(1,), stride=(1,))
        )
        (1): Block(
          (F): Sequential(
            (0): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (1): ReLU()
            (2): Conv1d(64, 16, kernel_size=(1,), stride=(1,), bias=False)
            (3): BatchNorm1d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (4): ReLU()
            (5): Conv1d(16, 16, kernel_size=(3,), stride=(1,), padding=(1,), bias=False)
            (6): BatchNorm1d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (7): ReLU()
            (8): Conv1d(16, 64, kernel_size=(1,), stride=(1,))
          )
          (Id): Identity()
        )
        (2): Block(
          (F): Sequential(
            (0): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (1): ReLU()
            (2): Conv1d(64, 16, kernel_size=(1,), stride=(1,), bias=False)
            (3): BatchNorm1d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (4): ReLU()
            (5): Conv1d(16, 16, kernel_size=(3,), stride=(2,), padding=(1,), bias=False)
            (6): BatchNorm1d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (7): ReLU()
            (8): Conv1d(16, 64, kernel_size=(1,), stride=(1,))
          )
          (Id): MaxPool1d(kernel_size=1, stride=2, padding=0, dilation=1, ceil_mode=False)
        )
      )
    )
    (2): Stack(
      (F): Sequential(
        (0): Block(
          (F): Sequential(
            (0): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (1): ReLU()
            (2): Conv1d(64, 32, kernel_size=(1,), stride=(1,), bias=False)
            (3): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (4): ReLU()
            (5): Conv1d(32, 32, kernel_size=(3,), stride=(1,), padding=(1,), bias=False)
            (6): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (7): ReLU()
            (8): Conv1d(32, 128, kernel_size=(1,), stride=(1,))
          )
          (Id): Conv1d(64, 128, kernel_size=(1,), stride=(1,))
        )
        (1): Block(
          (F): Sequential(
            (0): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (1): ReLU()
            (2): Conv1d(128, 32, kernel_size=(1,), stride=(1,), bias=False)
            (3): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (4): ReLU()
            (5): Conv1d(32, 32, kernel_size=(3,), stride=(1,), padding=(1,), bias=False)
            (6): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (7): ReLU()
            (8): Conv1d(32, 128, kernel_size=(1,), stride=(1,))
          )
          (Id): Identity()
        )
        (2): Block(
          (F): Sequential(
            (0): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (1): ReLU()
            (2): Conv1d(128, 32, kernel_size=(1,), stride=(1,), bias=False)
            (3): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (4): ReLU()
            (5): Conv1d(32, 32, kernel_size=(3,), stride=(1,), padding=(1,), bias=False)
            (6): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (7): ReLU()
            (8): Conv1d(32, 128, kernel_size=(1,), stride=(1,))
          )
          (Id): Identity()
        )
        (3): Block(
          (F): Sequential(
            (0): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (1): ReLU()
            (2): Conv1d(128, 32, kernel_size=(1,), stride=(1,), bias=False)
            (3): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (4): ReLU()
            (5): Conv1d(32, 32, kernel_size=(3,), stride=(2,), padding=(1,), bias=False)
            (6): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (7): ReLU()
            (8): Conv1d(32, 128, kernel_size=(1,), stride=(1,))
          )
          (Id): MaxPool1d(kernel_size=1, stride=2, padding=0, dilation=1, ceil_mode=False)
        )
      )
    )
    (3): Stack(
      (F): Sequential(
        (0): Block(
          (F): Sequential(
            (0): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (1): ReLU()
            (2): Conv1d(128, 64, kernel_size=(1,), stride=(1,), bias=False)
            (3): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (4): ReLU()
            (5): Conv1d(64, 64, kernel_size=(3,), stride=(1,), padding=(1,), bias=False)
            (6): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (7): ReLU()
            (8): Conv1d(64, 256, kernel_size=(1,), stride=(1,))
          )
          (Id): Conv1d(128, 256, kernel_size=(1,), stride=(1,))
        )
        (1): Block(
          (F): Sequential(
            (0): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (1): ReLU()
            (2): Conv1d(256, 64, kernel_size=(1,), stride=(1,), bias=False)
            (3): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (4): ReLU()
            (5): Conv1d(64, 64, kernel_size=(3,), stride=(1,), padding=(1,), bias=False)
            (6): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (7): ReLU()
            (8): Conv1d(64, 256, kernel_size=(1,), stride=(1,))
          )
          (Id): Identity()
        )
        (2): Block(
          (F): Sequential(
            (0): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (1): ReLU()
            (2): Conv1d(256, 64, kernel_size=(1,), stride=(1,), bias=False)
            (3): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (4): ReLU()
            (5): Conv1d(64, 64, kernel_size=(3,), stride=(1,), padding=(1,), bias=False)
            (6): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (7): ReLU()
            (8): Conv1d(64, 256, kernel_size=(1,), stride=(1,))
          )
          (Id): Identity()
        )
        (3): Block(
          (F): Sequential(
            (0): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (1): ReLU()
            (2): Conv1d(256, 64, kernel_size=(1,), stride=(1,), bias=False)
            (3): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (4): ReLU()
            (5): Conv1d(64, 64, kernel_size=(3,), stride=(2,), padding=(1,), bias=False)
            (6): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (7): ReLU()
            (8): Conv1d(64, 256, kernel_size=(1,), stride=(1,))
          )
          (Id): MaxPool1d(kernel_size=1, stride=2, padding=0, dilation=1, ceil_mode=False)
        )
      )
    )
    (4): Stack(
      (F): Sequential(
        (0): Block(
          (F): Sequential(
            (0): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (1): ReLU()
            (2): Conv1d(256, 128, kernel_size=(1,), stride=(1,), bias=False)
            (3): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (4): ReLU()
            (5): Conv1d(128, 128, kernel_size=(3,), stride=(1,), padding=(1,), bias=False)
            (6): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (7): ReLU()
            (8): Conv1d(128, 512, kernel_size=(1,), stride=(1,))
          )
          (Id): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
        )
        (1): Block(
          (F): Sequential(
            (0): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (1): ReLU()
            (2): Conv1d(512, 128, kernel_size=(1,), stride=(1,), bias=False)
            (3): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (4): ReLU()
            (5): Conv1d(128, 128, kernel_size=(3,), stride=(1,), padding=(1,), bias=False)
            (6): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (7): ReLU()
            (8): Conv1d(128, 512, kernel_size=(1,), stride=(1,))
          )
          (Id): Identity()
        )
        (2): Block(
          (F): Sequential(
            (0): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (1): ReLU()
            (2): Conv1d(512, 128, kernel_size=(1,), stride=(1,), bias=False)
            (3): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (4): ReLU()
            (5): Conv1d(128, 128, kernel_size=(3,), stride=(2,), padding=(1,), bias=False)
            (6): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (7): ReLU()
            (8): Conv1d(128, 512, kernel_size=(1,), stride=(1,))
          )
          (Id): MaxPool1d(kernel_size=1, stride=2, padding=0, dilation=1, ceil_mode=False)
        )
      )
    )
    (5): Flatten(start_dim=1, end_dim=-1)
    (6): LazyLinear(in_features=0, out_features=256, bias=True)
  )
)

Calculating initial results.
	Done.
	Discriminator:
		Training loss: -5.537492
		Training accuracy: 0.0
		Validation loss: -5.540614
		Validation accuracy: 0.0
	Generator:
		Training loss: -5.537492
		Training accuracy: 0.0
		Validation loss: -5.540614
		Validation accuracy: 0.0

Pretraining discriminator.
	Initial performamce
		Training loss: 5.5362
		Training accuracy: 0.0
		Validation loss: 5.539698
		Validation accuracy: 0.0

	Epoch 1
		Training loss: 1.9551861
		Training accuracy: 0.40470297029702973
		Validation loss: 1.1100335
		Validation accuracy: 0.566468253968254

	Epoch 2
		Training loss: 0.8937912
		Training accuracy: 0.6523514851485148
		Validation loss: 0.71270865
		Validation accuracy: 0.7366071428571429

	Epoch 3
		Training loss: 0.6180468
		Training accuracy: 0.7629950495049505
		Validation loss: 0.7317673
		Validation accuracy: 0.7276785714285714

	Epoch 4
		Training loss: 0.52876264
		Training accuracy: 0.8023514851485148
		Validation loss: 0.6089036
		Validation accuracy: 0.7624007936507936

	Epoch 5
		Training loss: 0.451164
		Training accuracy: 0.8383663366336633
		Validation loss: 0.5109805
		Validation accuracy: 0.8253968253968254

	Epoch 6
		Training loss: 0.47618854
		Training accuracy: 0.8325495049504951
		Validation loss: 0.3954366
		Validation accuracy: 0.8432539682539683

	Epoch 7
		Training loss: 0.3916991
		Training accuracy: 0.8591584158415841
		Validation loss: 0.4409231
		Validation accuracy: 0.8392857142857143

	Epoch 8
		Training loss: 0.40357697
		Training accuracy: 0.8595297029702971
		Validation loss: 0.8159845
		Validation accuracy: 0.7579365079365079

	Epoch 9
		Training loss: 0.35080853
		Training accuracy: 0.8767326732673267
		Validation loss: 0.33663684
		Validation accuracy: 0.871031746031746

	Epoch 10
		Training loss: 0.42040563
		Training accuracy: 0.8612623762376238
		Validation loss: 0.5322492
		Validation accuracy: 0.8115079365079365

	Epoch 11
		Training loss: 0.3041774
		Training accuracy: 0.8949257425742574
		Validation loss: 0.40340644
		Validation accuracy: 0.8655753968253969

	Epoch 12
		Training loss: 0.31238115
		Training accuracy: 0.8918316831683168
		Validation loss: 0.40953597
		Validation accuracy: 0.8625992063492064

	Epoch 13
		Training loss: 0.30311486
		Training accuracy: 0.8974009900990099
		Validation loss: 0.48003298
		Validation accuracy: 0.8591269841269841

	Epoch 14
		Training loss: 0.30223942
		Training accuracy: 0.9016089108910891
		Validation loss: 0.712098
		Validation accuracy: 0.7986111111111112

	Epoch 15
		Training loss: 0.3171809
		Training accuracy: 0.8959158415841584
		Validation loss: 0.63006383
		Validation accuracy: 0.847718253968254

	Epoch 16
		Training loss: 0.2687996
		Training accuracy: 0.908539603960396
		Validation loss: 0.41711396
		Validation accuracy: 0.8779761904761905

	Epoch 17
		Training loss: 0.2960265
		Training accuracy: 0.906559405940594
		Validation loss: 0.30556533
		Validation accuracy: 0.9007936507936508

	Epoch 18
		Training loss: 0.26238418
		Training accuracy: 0.9201732673267327
		Validation loss: 0.6303184
		Validation accuracy: 0.8536706349206349

	Epoch 19
		Training loss: 0.22401553
		Training accuracy: 0.9230198019801981
		Validation loss: 0.331423
		Validation accuracy: 0.908234126984127

	Epoch 20
		Training loss: 0.24700171
		Training accuracy: 0.924009900990099
		Validation loss: 0.31869608
		Validation accuracy: 0.8993055555555556

	Epoch 21
		Training loss: 0.24553652
		Training accuracy: 0.9287128712871288
		Validation loss: 0.4805476
		Validation accuracy: 0.8650793650793651

	Epoch 22
		Training loss: 0.24517128
		Training accuracy: 0.9254950495049505
		Validation loss: 0.32944962
		Validation accuracy: 0.90625

	Epoch 23
		Training loss: 0.22517304
		Training accuracy: 0.9316831683168317
		Validation loss: 0.40294033
		Validation accuracy: 0.8864087301587301

	Epoch 24
		Training loss: 0.20532085
		Training accuracy: 0.938490099009901
		Validation loss: 0.3549554
		Validation accuracy: 0.8928571428571429

	Epoch 25
		Training loss: 0.22454968
		Training accuracy: 0.9332920792079208
		Validation loss: 0.59147954
		Validation accuracy: 0.8606150793650794

	Epoch 26
		Training loss: 0.18105665
		Training accuracy: 0.9445544554455445
		Validation loss: 0.53620166
		Validation accuracy: 0.8606150793650794

	Epoch 27
		Training loss: 0.18671073
		Training accuracy: 0.9454207920792079
		Validation loss: 1.2374884
		Validation accuracy: 0.7653769841269841

	Epoch 28
		Training loss: 0.3352135
		Training accuracy: 0.9136138613861386
		Validation loss: 0.37954283
		Validation accuracy: 0.8978174603174603

	Epoch 29
		Training loss: 0.14890353
		Training accuracy: 0.9542079207920792
		Validation loss: 0.5318085
		Validation accuracy: 0.875

	Epoch 30
		Training loss: 0.13458182
		Training accuracy: 0.959529702970297
		Validation loss: 0.38435477
		Validation accuracy: 0.908234126984127

	Epoch 31
		Training loss: 0.15687202
		Training accuracy: 0.9519801980198019
		Validation loss: 0.26139972
		Validation accuracy: 0.9196428571428571

	Epoch 32
		Training loss: 0.22677459
		Training accuracy: 0.9392326732673267
		Validation loss: 0.72396296
		Validation accuracy: 0.8606150793650794

	Epoch 33
		Training loss: 0.2860637
		Training accuracy: 0.9303217821782178
		Validation loss: 0.2347456
		Validation accuracy: 0.9350198412698413

	Epoch 34
		Training loss: 0.16091125
		Training accuracy: 0.9565594059405941
		Validation loss: 0.43764433
		Validation accuracy: 0.9002976190476191

	Epoch 35
		Training loss: 0.16315472
		Training accuracy: 0.9521039603960396
		Validation loss: 0.40961537
		Validation accuracy: 0.9007936507936508

	Epoch 36
		Training loss: 0.14884885
		Training accuracy: 0.9550742574257426
		Validation loss: 0.6060451
		Validation accuracy: 0.871031746031746

	Epoch 37
		Training loss: 0.21021184
		Training accuracy: 0.9422029702970297
		Validation loss: 0.5024065
		Validation accuracy: 0.9087301587301587

	Epoch 38
		Training loss: 0.21370442
		Training accuracy: 0.9445544554455445
		Validation loss: 0.3785317
		Validation accuracy: 0.9136904761904762

	Epoch 39
		Training loss: 0.09330529
		Training accuracy: 0.9707920792079208
		Validation loss: 0.32356924
		Validation accuracy: 0.9226190476190477

	Epoch 40
		Training loss: 0.11814044
		Training accuracy: 0.9663366336633663
		Validation loss: 0.40533143
		Validation accuracy: 0.9171626984126984

	Epoch 41
		Training loss: 0.27748328
		Training accuracy: 0.940470297029703
		Validation loss: 0.89514184
		Validation accuracy: 0.871031746031746

	Epoch 42
		Training loss: 0.23685777
		Training accuracy: 0.9438118811881188
		Validation loss: 0.40006176
		Validation accuracy: 0.9151785714285714

	Epoch 43
		Training loss: 0.06559255
		Training accuracy: 0.9808168316831684
		Validation loss: 0.28263748
		Validation accuracy: 0.9330357142857143

	Epoch 44
		Training loss: 0.08458829
		Training accuracy: 0.9785891089108911
		Validation loss: 0.49330088
		Validation accuracy: 0.9027777777777778

	Epoch 45
		Training loss: 0.20774598
		Training accuracy: 0.9506188118811881
		Validation loss: 0.9205786
		Validation accuracy: 0.8665674603174603

	Epoch 46
		Training loss: 0.16773205
		Training accuracy: 0.9579207920792079
		Validation loss: 0.80235565
		Validation accuracy: 0.8814484126984127

	Epoch 47
		Training loss: 0.16085114
		Training accuracy: 0.9618811881188118
		Validation loss: 0.5684568
		Validation accuracy: 0.9087301587301587

	Epoch 48
		Training loss: 0.15248503
		Training accuracy: 0.9617574257425743
		Validation loss: 0.54571176
		Validation accuracy: 0.9067460317460317

	Epoch 49
		Training loss: 0.16394319
		Training accuracy: 0.9618811881188118
		Validation loss: 0.55201787
		Validation accuracy: 0.8978174603174603

	Epoch 50
		Training loss: 0.10738687
		Training accuracy: 0.9728960396039604
		Validation loss: 0.68975484
		Validation accuracy: 0.8839285714285714

	Epoch 51
		Training loss: 0.22586048
		Training accuracy: 0.9534653465346534
		Validation loss: 0.6660173
		Validation accuracy: 0.8913690476190477

	Epoch 52
		Training loss: 0.11551033
		Training accuracy: 0.9699257425742575
		Validation loss: 0.38432422
		Validation accuracy: 0.9290674603174603

	Epoch 53
		Training loss: 0.112841256
		Training accuracy: 0.9726485148514852
		Validation loss: 0.7030331
		Validation accuracy: 0.8893849206349206

	Epoch 54
		Training loss: 0.34850052
		Training accuracy: 0.9444306930693069
		Validation loss: 0.5469415
		Validation accuracy: 0.9181547619047619

	Epoch 55
		Training loss: 0.10412424
		Training accuracy: 0.9775990099009901
		Validation loss: 0.51651305
		Validation accuracy: 0.9186507936507936

	Epoch 56
		Training loss: 0.11641511
		Training accuracy: 0.9733910891089109
		Validation loss: 0.5144538
		Validation accuracy: 0.9191468253968254

	Epoch 57
		Training loss: 0.117876016
		Training accuracy: 0.9717821782178218
		Validation loss: 0.90929836
		Validation accuracy: 0.8804563492063492

	Epoch 58
		Training loss: 0.15249099
		Training accuracy: 0.968069306930693
		Validation loss: 0.77386546
		Validation accuracy: 0.8913690476190477

	Epoch 59
		Training loss: 0.1729725
		Training accuracy: 0.9648514851485148
		Validation loss: 0.6654994
		Validation accuracy: 0.9107142857142857

	Epoch 60
		Training loss: 0.08795001
		Training accuracy: 0.9778465346534654
		Validation loss: 0.61367613
		Validation accuracy: 0.9176587301587301

	Epoch 61
		Training loss: 0.29257086
		Training accuracy: 0.9492574257425742
		Validation loss: 0.49161527
		Validation accuracy: 0.9231150793650794

	Epoch 62
		Training loss: 0.12101086
		Training accuracy: 0.9715346534653465
		Validation loss: 0.41230237
		Validation accuracy: 0.9310515873015873

	Epoch 63
		Training loss: 0.08269211
		Training accuracy: 0.9810643564356436
		Validation loss: 0.539369
		Validation accuracy: 0.9166666666666666

	Epoch 64
		Training loss: 0.19188415
		Training accuracy: 0.9627475247524753
		Validation loss: 0.664961
		Validation accuracy: 0.9117063492063492

	Epoch 65
		Training loss: 0.18583083
		Training accuracy: 0.9662128712871287
		Validation loss: 0.7876478
		Validation accuracy: 0.8948412698412699

	Epoch 66
		Training loss: 0.1534959
		Training accuracy: 0.9701732673267327
		Validation loss: 0.63948417
		Validation accuracy: 0.9146825396825397

	Epoch 67
		Training loss: 0.06758957
		Training accuracy: 0.9842821782178218
		Validation loss: 0.41339675
		Validation accuracy: 0.9320436507936508

	Epoch 68
		Training loss: 0.10388059
		Training accuracy: 0.9764851485148515
		Validation loss: 0.7081269
		Validation accuracy: 0.9087301587301587

	Epoch 69
		Training loss: 0.2504645
		Training accuracy: 0.9564356435643564
		Validation loss: 1.1608115
		Validation accuracy: 0.8725198412698413

	Epoch 70
		Training loss: 0.1300551
		Training accuracy: 0.9748762376237624
		Validation loss: 0.64517266
		Validation accuracy: 0.9265873015873016

	Epoch 71
		Training loss: 0.07789944
		Training accuracy: 0.9821782178217822
		Validation loss: 0.98657525
		Validation accuracy: 0.8809523809523809

	Epoch 72
		Training loss: 0.17419231
		Training accuracy: 0.9679455445544555
		Validation loss: 0.59059566
		Validation accuracy: 0.9270833333333334

	Epoch 73
		Training loss: 0.13736728
		Training accuracy: 0.9732673267326732
		Validation loss: 1.0648102
		Validation accuracy: 0.8893849206349206

	Epoch 74
		Training loss: 0.13382342
		Training accuracy: 0.9725247524752475
		Validation loss: 0.7237612
		Validation accuracy: 0.9131944444444444

	Epoch 75
		Training loss: 0.2715474
		Training accuracy: 0.9535891089108911
		Validation loss: 0.82777065
		Validation accuracy: 0.8968253968253969

	Epoch 76
		Training loss: 0.11341143
		Training accuracy: 0.9790841584158416
		Validation loss: 0.6022717
		Validation accuracy: 0.9280753968253969

	Epoch 77
		Training loss: 0.100389905
		Training accuracy: 0.9810643564356436
		Validation loss: 0.9319068
		Validation accuracy: 0.9037698412698413

	Epoch 78
		Training loss: 0.12864107
		Training accuracy: 0.9762376237623762
		Validation loss: 1.4407104
		Validation accuracy: 0.8705357142857143

	Epoch 79
		Training loss: 0.1655649
		Training accuracy: 0.9715346534653465
		Validation loss: 0.7335575
		Validation accuracy: 0.9122023809523809

	Epoch 80
		Training loss: 0.11005369
		Training accuracy: 0.9806930693069307
		Validation loss: 0.88169307
		Validation accuracy: 0.9047619047619048

	Epoch 81
		Training loss: 0.042271983
		Training accuracy: 0.9907178217821783
		Validation loss: 0.6171372
		Validation accuracy: 0.9330357142857143

	Epoch 82
		Training loss: 0.37036029
		Training accuracy: 0.9544554455445544
		Validation loss: 1.8753291
		Validation accuracy: 0.8516865079365079

	Epoch 83
		Training loss: 0.23200421
		Training accuracy: 0.9695544554455445
		Validation loss: 0.898964
		Validation accuracy: 0.9057539682539683

	Epoch 84
		Training loss: 0.08419395
		Training accuracy: 0.9825495049504951
		Validation loss: 0.76691353
		Validation accuracy: 0.9241071428571429

	Epoch 85
		Training loss: 0.11527921
		Training accuracy: 0.9806930693069307
		Validation loss: 0.8704349
		Validation accuracy: 0.910218253968254

	Epoch 86
		Training loss: 0.15783583
		Training accuracy: 0.9754950495049505
		Validation loss: 0.7588245
		Validation accuracy: 0.9136904761904762

	Epoch 87
		Training loss: 0.0497652
		Training accuracy: 0.9899752475247525
		Validation loss: 0.60973704
		Validation accuracy: 0.9315476190476191

	Epoch 88
		Training loss: 0.1391452
		Training accuracy: 0.9766089108910891
		Validation loss: 1.2110021
		Validation accuracy: 0.8958333333333334

	Epoch 89
		Training loss: 0.39714736
		Training accuracy: 0.9564356435643564
		Validation loss: 0.93218297
		Validation accuracy: 0.9122023809523809

	Epoch 90
		Training loss: 0.09832268
		Training accuracy: 0.9834158415841584
		Validation loss: 0.9500154
		Validation accuracy: 0.9191468253968254

	Epoch 91
		Training loss: 0.07080349
		Training accuracy: 0.9873762376237624
		Validation loss: 0.665814
		Validation accuracy: 0.9295634920634921

	Epoch 92
		Training loss: 0.09944831
		Training accuracy: 0.9840346534653466
		Validation loss: 0.6250873
		Validation accuracy: 0.933531746031746

	Epoch 93
		Training loss: 0.15155177
		Training accuracy: 0.9756188118811882
		Validation loss: 1.1589446
		Validation accuracy: 0.8973214285714286

	Epoch 94
		Training loss: 0.19491193
		Training accuracy: 0.9705445544554455
		Validation loss: 0.79449195
		Validation accuracy: 0.9310515873015873

	Epoch 95
		Training loss: 0.12336656
		Training accuracy: 0.9799504950495049
		Validation loss: 1.3406829
		Validation accuracy: 0.8878968253968254

	Epoch 96
		Training loss: 0.098376274
		Training accuracy: 0.9858910891089109
		Validation loss: 1.0335541
		Validation accuracy: 0.9146825396825397

	Epoch 97
		Training loss: 0.35700974
		Training accuracy: 0.9584158415841584
		Validation loss: 1.1672889
		Validation accuracy: 0.9161706349206349

	Epoch 98
		Training loss: 0.15477362
		Training accuracy: 0.9751237623762377
		Validation loss: 1.1659424
		Validation accuracy: 0.9151785714285714

	Epoch 99
		Training loss: 0.047176998
		Training accuracy: 0.9915841584158416
		Validation loss: 1.1933266
		Validation accuracy: 0.9047619047619048

	Epoch 100
		Training loss: 0.16632709
		Training accuracy: 0.9763613861386139
		Validation loss: 1.5218375
		Validation accuracy: 0.8943452380952381


Pretraining generator.
	Initial performance
		Training loss: 0.0032966794
		Training accuracy: nan
		Validation loss: 0.0033041944
		Validation accuracy: nan


Training discriminator and generator simultaneously.

Training new discriminator on static trained discriminator.
	Initial performance
		Training loss: 5.532158
		Training accuracy: 0.0
		Validation loss: 5.5349627
		Validation accuracy: 0.0


{'generator': {'pretrain_train_loss': [0.0032966794], 'pretrain_val_loss': [0.0033041944], 'train_loss': [-5.537492], 'val_loss': [-5.540614], 'train_acc': [0.0], 'val_acc': [0.0]}, 'discriminator': {'pretrain_train_loss': [5.5362, 1.9551861, 0.8937912, 0.6180468, 0.52876264, 0.451164, 0.47618854, 0.3916991, 0.40357697, 0.35080853, 0.42040563, 0.3041774, 0.31238115, 0.30311486, 0.30223942, 0.3171809, 0.2687996, 0.2960265, 0.26238418, 0.22401553, 0.24700171, 0.24553652, 0.24517128, 0.22517304, 0.20532085, 0.22454968, 0.18105665, 0.18671073, 0.3352135, 0.14890353, 0.13458182, 0.15687202, 0.22677459, 0.2860637, 0.16091125, 0.16315472, 0.14884885, 0.21021184, 0.21370442, 0.09330529, 0.11814044, 0.27748328, 0.23685777, 0.06559255, 0.08458829, 0.20774598, 0.16773205, 0.16085114, 0.15248503, 0.16394319, 0.10738687, 0.22586048, 0.11551033, 0.112841256, 0.34850052, 0.10412424, 0.11641511, 0.117876016, 0.15249099, 0.1729725, 0.08795001, 0.29257086, 0.12101086, 0.08269211, 0.19188415, 0.18583083, 0.1534959, 0.06758957, 0.10388059, 0.2504645, 0.1300551, 0.07789944, 0.17419231, 0.13736728, 0.13382342, 0.2715474, 0.11341143, 0.100389905, 0.12864107, 0.1655649, 0.11005369, 0.042271983, 0.37036029, 0.23200421, 0.08419395, 0.11527921, 0.15783583, 0.0497652, 0.1391452, 0.39714736, 0.09832268, 0.07080349, 0.09944831, 0.15155177, 0.19491193, 0.12336656, 0.098376274, 0.35700974, 0.15477362, 0.047176998, 0.16632709], 'pretrain_val_loss': [5.539698, 1.1100335, 0.71270865, 0.7317673, 0.6089036, 0.5109805, 0.3954366, 0.4409231, 0.8159845, 0.33663684, 0.5322492, 0.40340644, 0.40953597, 0.48003298, 0.712098, 0.63006383, 0.41711396, 0.30556533, 0.6303184, 0.331423, 0.31869608, 0.4805476, 0.32944962, 0.40294033, 0.3549554, 0.59147954, 0.53620166, 1.2374884, 0.37954283, 0.5318085, 0.38435477, 0.26139972, 0.72396296, 0.2347456, 0.43764433, 0.40961537, 0.6060451, 0.5024065, 0.3785317, 0.32356924, 0.40533143, 0.89514184, 0.40006176, 0.28263748, 0.49330088, 0.9205786, 0.80235565, 0.5684568, 0.54571176, 0.55201787, 0.68975484, 0.6660173, 0.38432422, 0.7030331, 0.5469415, 0.51651305, 0.5144538, 0.90929836, 0.77386546, 0.6654994, 0.61367613, 0.49161527, 0.41230237, 0.539369, 0.664961, 0.7876478, 0.63948417, 0.41339675, 0.7081269, 1.1608115, 0.64517266, 0.98657525, 0.59059566, 1.0648102, 0.7237612, 0.82777065, 0.6022717, 0.9319068, 1.4407104, 0.7335575, 0.88169307, 0.6171372, 1.8753291, 0.898964, 0.76691353, 0.8704349, 0.7588245, 0.60973704, 1.2110021, 0.93218297, 0.9500154, 0.665814, 0.6250873, 1.1589446, 0.79449195, 1.3406829, 1.0335541, 1.1672889, 1.1659424, 1.1933266, 1.5218375], 'pretrain_train_acc': [0.0, 0.40470297029702973, 0.6523514851485148, 0.7629950495049505, 0.8023514851485148, 0.8383663366336633, 0.8325495049504951, 0.8591584158415841, 0.8595297029702971, 0.8767326732673267, 0.8612623762376238, 0.8949257425742574, 0.8918316831683168, 0.8974009900990099, 0.9016089108910891, 0.8959158415841584, 0.908539603960396, 0.906559405940594, 0.9201732673267327, 0.9230198019801981, 0.924009900990099, 0.9287128712871288, 0.9254950495049505, 0.9316831683168317, 0.938490099009901, 0.9332920792079208, 0.9445544554455445, 0.9454207920792079, 0.9136138613861386, 0.9542079207920792, 0.959529702970297, 0.9519801980198019, 0.9392326732673267, 0.9303217821782178, 0.9565594059405941, 0.9521039603960396, 0.9550742574257426, 0.9422029702970297, 0.9445544554455445, 0.9707920792079208, 0.9663366336633663, 0.940470297029703, 0.9438118811881188, 0.9808168316831684, 0.9785891089108911, 0.9506188118811881, 0.9579207920792079, 0.9618811881188118, 0.9617574257425743, 0.9618811881188118, 0.9728960396039604, 0.9534653465346534, 0.9699257425742575, 0.9726485148514852, 0.9444306930693069, 0.9775990099009901, 0.9733910891089109, 0.9717821782178218, 0.968069306930693, 0.9648514851485148, 0.9778465346534654, 0.9492574257425742, 0.9715346534653465, 0.9810643564356436, 0.9627475247524753, 0.9662128712871287, 0.9701732673267327, 0.9842821782178218, 0.9764851485148515, 0.9564356435643564, 0.9748762376237624, 0.9821782178217822, 0.9679455445544555, 0.9732673267326732, 0.9725247524752475, 0.9535891089108911, 0.9790841584158416, 0.9810643564356436, 0.9762376237623762, 0.9715346534653465, 0.9806930693069307, 0.9907178217821783, 0.9544554455445544, 0.9695544554455445, 0.9825495049504951, 0.9806930693069307, 0.9754950495049505, 0.9899752475247525, 0.9766089108910891, 0.9564356435643564, 0.9834158415841584, 0.9873762376237624, 0.9840346534653466, 0.9756188118811882, 0.9705445544554455, 0.9799504950495049, 0.9858910891089109, 0.9584158415841584, 0.9751237623762377, 0.9915841584158416, 0.9763613861386139], 'pretrain_val_acc': [0.0, 0.566468253968254, 0.7366071428571429, 0.7276785714285714, 0.7624007936507936, 0.8253968253968254, 0.8432539682539683, 0.8392857142857143, 0.7579365079365079, 0.871031746031746, 0.8115079365079365, 0.8655753968253969, 0.8625992063492064, 0.8591269841269841, 0.7986111111111112, 0.847718253968254, 0.8779761904761905, 0.9007936507936508, 0.8536706349206349, 0.908234126984127, 0.8993055555555556, 0.8650793650793651, 0.90625, 0.8864087301587301, 0.8928571428571429, 0.8606150793650794, 0.8606150793650794, 0.7653769841269841, 0.8978174603174603, 0.875, 0.908234126984127, 0.9196428571428571, 0.8606150793650794, 0.9350198412698413, 0.9002976190476191, 0.9007936507936508, 0.871031746031746, 0.9087301587301587, 0.9136904761904762, 0.9226190476190477, 0.9171626984126984, 0.871031746031746, 0.9151785714285714, 0.9330357142857143, 0.9027777777777778, 0.8665674603174603, 0.8814484126984127, 0.9087301587301587, 0.9067460317460317, 0.8978174603174603, 0.8839285714285714, 0.8913690476190477, 0.9290674603174603, 0.8893849206349206, 0.9181547619047619, 0.9186507936507936, 0.9191468253968254, 0.8804563492063492, 0.8913690476190477, 0.9107142857142857, 0.9176587301587301, 0.9231150793650794, 0.9310515873015873, 0.9166666666666666, 0.9117063492063492, 0.8948412698412699, 0.9146825396825397, 0.9320436507936508, 0.9087301587301587, 0.8725198412698413, 0.9265873015873016, 0.8809523809523809, 0.9270833333333334, 0.8893849206349206, 0.9131944444444444, 0.8968253968253969, 0.9280753968253969, 0.9037698412698413, 0.8705357142857143, 0.9122023809523809, 0.9047619047619048, 0.9330357142857143, 0.8516865079365079, 0.9057539682539683, 0.9241071428571429, 0.910218253968254, 0.9136904761904762, 0.9315476190476191, 0.8958333333333334, 0.9122023809523809, 0.9191468253968254, 0.9295634920634921, 0.933531746031746, 0.8973214285714286, 0.9310515873015873, 0.8878968253968254, 0.9146825396825397, 0.9161706349206349, 0.9151785714285714, 0.9047619047619048, 0.8943452380952381], 'train_loss': [5.5376015], 'val_loss': [5.54065], 'train_acc': [0.0], 'val_acc': [0.0], 'posttrain_train_loss': [5.532158], 'posttrain_val_loss': [5.5349627], 'posttrain_train_acc': [0.0], 'posttrain_val_acc': [0.0]}}
