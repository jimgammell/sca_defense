Beginning trial described in ./config/multigen_trial.json.
Experiment type: multiple generators each corresponding to 1 key.
Experiment settings:
	byte: 0
	keys: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15]
	key_dataset_kwargs:
		keep_data_in_memory: True
		data_path: ./data
		download: True
		extract: True
		preprocess: True
		delete_download_after_extraction: False
		delete_extracted_after_preprocess: False
	dataloader_kwargs:
		batch_size: 16
		shuffle: True
	dataset_prop_for_validation: 0.2
	trace_map_constructor: None
	trace_map_kwargs:
		layers: [256]
		hidden_activation: <class 'torch.nn.modules.activation.ReLU'>
	plaintext_map_constructor: None
	plaintext_map_kwargs:
		layers: [64]
		hidden_activation: <class 'torch.nn.modules.activation.ReLU'>
	key_map_constructor: <function get_mlp_map at 0x7f92a8846670>
	key_map_kwargs:
		layers: [64, 256]
		hidden_activation: <class 'torch.nn.modules.activation.ReLU'>
	cumulative_map_constructor: None
	cumulative_map_kwargs:
		layers: [256]
		hidden_activation: <class 'torch.nn.modules.activation.ReLU'>
	discriminator_constructor: <function get_google_style_resnet_discriminator at 0x7f92a8846820>
	discriminator_kwargs:
	discriminator_loss_constructor: <class 'torch.nn.modules.loss.CrossEntropyLoss'>
	discriminator_loss_kwargs:
	discriminator_optimizer_constructor: <class 'torch.optim.adam.Adam'>
	discriminator_optimizer_kwargs:
	generator_loss_constructor: <class 'loss_functions.BatchStdLoss'>
	generator_loss_kwargs:
	generator_optimizer_constructor: <class 'torch.optim.adam.Adam'>
	generator_optimizer_kwargs:
	device: cuda
	discriminator_pretraining_epochs: 0
	generator_pretraining_epochs: 0
	gan_training_epochs: 100
	discriminator_posttraining_epochs: 100
	seed: 0
Loading datasets.
AesKeyGroupDataset:
	Available keys: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15]
	Key transform: Compose(
    IntToBinary()
    ToTensor1D()
)
	Byte: 0
	Number of samples available: 10112
	Trace size: torch.Size([1, 3000])
	Key size: torch.Size([1, 8])
	Plaintext size: torch.Size([1, 8])
	Key index size: ()
Constructing generator.
KeyOnlyGenerator(
  (key_trace_map): Sequential(
    (0): Flatten(start_dim=1, end_dim=-1)
    (1): Linear(in_features=8, out_features=64, bias=True)
    (2): ReLU()
    (3): Linear(in_features=64, out_features=256, bias=True)
    (4): ReLU()
    (5): Linear(in_features=256, out_features=3000, bias=True)
    (6): Unflatten(dim=-1, unflattened_size=torch.Size([1, 3000]))
  )
)

Constructing discriminator.
Discriminator(
  (model): Sequential(
    (0): MaxPool1d(kernel_size=4, stride=4, padding=0, dilation=1, ceil_mode=False)
    (1): Stack(
      (F): Sequential(
        (0): Block(
          (F): Sequential(
            (0): BatchNorm1d(1, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (1): ReLU()
            (2): Conv1d(1, 16, kernel_size=(1,), stride=(1,), bias=False)
            (3): BatchNorm1d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (4): ReLU()
            (5): Conv1d(16, 16, kernel_size=(3,), stride=(1,), padding=(1,), bias=False)
            (6): BatchNorm1d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (7): ReLU()
            (8): Conv1d(16, 64, kernel_size=(1,), stride=(1,))
          )
          (Id): Conv1d(1, 64, kernel_size=(1,), stride=(1,))
        )
        (1): Block(
          (F): Sequential(
            (0): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (1): ReLU()
            (2): Conv1d(64, 16, kernel_size=(1,), stride=(1,), bias=False)
            (3): BatchNorm1d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (4): ReLU()
            (5): Conv1d(16, 16, kernel_size=(3,), stride=(1,), padding=(1,), bias=False)
            (6): BatchNorm1d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (7): ReLU()
            (8): Conv1d(16, 64, kernel_size=(1,), stride=(1,))
          )
          (Id): Identity()
        )
        (2): Block(
          (F): Sequential(
            (0): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (1): ReLU()
            (2): Conv1d(64, 16, kernel_size=(1,), stride=(1,), bias=False)
            (3): BatchNorm1d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (4): ReLU()
            (5): Conv1d(16, 16, kernel_size=(3,), stride=(2,), padding=(1,), bias=False)
            (6): BatchNorm1d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (7): ReLU()
            (8): Conv1d(16, 64, kernel_size=(1,), stride=(1,))
          )
          (Id): MaxPool1d(kernel_size=1, stride=2, padding=0, dilation=1, ceil_mode=False)
        )
      )
    )
    (2): Stack(
      (F): Sequential(
        (0): Block(
          (F): Sequential(
            (0): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (1): ReLU()
            (2): Conv1d(64, 32, kernel_size=(1,), stride=(1,), bias=False)
            (3): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (4): ReLU()
            (5): Conv1d(32, 32, kernel_size=(3,), stride=(1,), padding=(1,), bias=False)
            (6): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (7): ReLU()
            (8): Conv1d(32, 128, kernel_size=(1,), stride=(1,))
          )
          (Id): Conv1d(64, 128, kernel_size=(1,), stride=(1,))
        )
        (1): Block(
          (F): Sequential(
            (0): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (1): ReLU()
            (2): Conv1d(128, 32, kernel_size=(1,), stride=(1,), bias=False)
            (3): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (4): ReLU()
            (5): Conv1d(32, 32, kernel_size=(3,), stride=(1,), padding=(1,), bias=False)
            (6): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (7): ReLU()
            (8): Conv1d(32, 128, kernel_size=(1,), stride=(1,))
          )
          (Id): Identity()
        )
        (2): Block(
          (F): Sequential(
            (0): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (1): ReLU()
            (2): Conv1d(128, 32, kernel_size=(1,), stride=(1,), bias=False)
            (3): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (4): ReLU()
            (5): Conv1d(32, 32, kernel_size=(3,), stride=(1,), padding=(1,), bias=False)
            (6): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (7): ReLU()
            (8): Conv1d(32, 128, kernel_size=(1,), stride=(1,))
          )
          (Id): Identity()
        )
        (3): Block(
          (F): Sequential(
            (0): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (1): ReLU()
            (2): Conv1d(128, 32, kernel_size=(1,), stride=(1,), bias=False)
            (3): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (4): ReLU()
            (5): Conv1d(32, 32, kernel_size=(3,), stride=(2,), padding=(1,), bias=False)
            (6): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (7): ReLU()
            (8): Conv1d(32, 128, kernel_size=(1,), stride=(1,))
          )
          (Id): MaxPool1d(kernel_size=1, stride=2, padding=0, dilation=1, ceil_mode=False)
        )
      )
    )
    (3): Stack(
      (F): Sequential(
        (0): Block(
          (F): Sequential(
            (0): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (1): ReLU()
            (2): Conv1d(128, 64, kernel_size=(1,), stride=(1,), bias=False)
            (3): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (4): ReLU()
            (5): Conv1d(64, 64, kernel_size=(3,), stride=(1,), padding=(1,), bias=False)
            (6): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (7): ReLU()
            (8): Conv1d(64, 256, kernel_size=(1,), stride=(1,))
          )
          (Id): Conv1d(128, 256, kernel_size=(1,), stride=(1,))
        )
        (1): Block(
          (F): Sequential(
            (0): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (1): ReLU()
            (2): Conv1d(256, 64, kernel_size=(1,), stride=(1,), bias=False)
            (3): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (4): ReLU()
            (5): Conv1d(64, 64, kernel_size=(3,), stride=(1,), padding=(1,), bias=False)
            (6): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (7): ReLU()
            (8): Conv1d(64, 256, kernel_size=(1,), stride=(1,))
          )
          (Id): Identity()
        )
        (2): Block(
          (F): Sequential(
            (0): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (1): ReLU()
            (2): Conv1d(256, 64, kernel_size=(1,), stride=(1,), bias=False)
            (3): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (4): ReLU()
            (5): Conv1d(64, 64, kernel_size=(3,), stride=(1,), padding=(1,), bias=False)
            (6): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (7): ReLU()
            (8): Conv1d(64, 256, kernel_size=(1,), stride=(1,))
          )
          (Id): Identity()
        )
        (3): Block(
          (F): Sequential(
            (0): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (1): ReLU()
            (2): Conv1d(256, 64, kernel_size=(1,), stride=(1,), bias=False)
            (3): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (4): ReLU()
            (5): Conv1d(64, 64, kernel_size=(3,), stride=(2,), padding=(1,), bias=False)
            (6): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (7): ReLU()
            (8): Conv1d(64, 256, kernel_size=(1,), stride=(1,))
          )
          (Id): MaxPool1d(kernel_size=1, stride=2, padding=0, dilation=1, ceil_mode=False)
        )
      )
    )
    (4): Stack(
      (F): Sequential(
        (0): Block(
          (F): Sequential(
            (0): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (1): ReLU()
            (2): Conv1d(256, 128, kernel_size=(1,), stride=(1,), bias=False)
            (3): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (4): ReLU()
            (5): Conv1d(128, 128, kernel_size=(3,), stride=(1,), padding=(1,), bias=False)
            (6): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (7): ReLU()
            (8): Conv1d(128, 512, kernel_size=(1,), stride=(1,))
          )
          (Id): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
        )
        (1): Block(
          (F): Sequential(
            (0): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (1): ReLU()
            (2): Conv1d(512, 128, kernel_size=(1,), stride=(1,), bias=False)
            (3): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (4): ReLU()
            (5): Conv1d(128, 128, kernel_size=(3,), stride=(1,), padding=(1,), bias=False)
            (6): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (7): ReLU()
            (8): Conv1d(128, 512, kernel_size=(1,), stride=(1,))
          )
          (Id): Identity()
        )
        (2): Block(
          (F): Sequential(
            (0): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (1): ReLU()
            (2): Conv1d(512, 128, kernel_size=(1,), stride=(1,), bias=False)
            (3): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (4): ReLU()
            (5): Conv1d(128, 128, kernel_size=(3,), stride=(2,), padding=(1,), bias=False)
            (6): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (7): ReLU()
            (8): Conv1d(128, 512, kernel_size=(1,), stride=(1,))
          )
          (Id): MaxPool1d(kernel_size=1, stride=2, padding=0, dilation=1, ceil_mode=False)
        )
      )
    )
    (5): Flatten(start_dim=1, end_dim=-1)
    (6): LazyLinear(in_features=0, out_features=256, bias=True)
  )
)

Calculating initial results.
Training results:
gen_loss: 0.009248744
disc_loss: 5.579209
disc_acc: 0.0

Validation results:
gen_loss: 0.0092163645
disc_loss: 5.587276
disc_acc: 0.0


Training discriminator and generator simultaneously.
	Epoch 1
Training results:
gen_loss: 2.2425835
disc_loss: 1.9122689
disc_acc: 0.6111386138613861

Validation results:
gen_loss: 0.3896614
disc_loss: 3.6711767
disc_acc: 0.059027777777777776


	Epoch 2
Training results:
gen_loss: 0.35459483
disc_loss: 2.5778868
disc_acc: 0.25495049504950495

Validation results:
gen_loss: 0.22114153
disc_loss: 2.467896
disc_acc: 0.2976190476190476


	Epoch 3
Training results:
gen_loss: 0.19974166
disc_loss: 2.7039266
disc_acc: 0.17289603960396038

Validation results:
gen_loss: 0.15424216
disc_loss: 2.768767
disc_acc: 0.1597222222222222


	Epoch 4
Training results:
gen_loss: 0.12544593
disc_loss: 2.7901683
disc_acc: 0.1275990099009901

Validation results:
gen_loss: 0.114550725
disc_loss: 2.7523253
disc_acc: 0.0798611111111111


	Epoch 5
Training results:
gen_loss: 0.11189163
disc_loss: 2.787774
disc_acc: 0.11163366336633664

Validation results:
gen_loss: 0.08683461
disc_loss: 2.833389
disc_acc: 0.11706349206349206


	Epoch 6
Training results:
gen_loss: 0.08410294
disc_loss: 2.8164098
disc_acc: 0.08490099009900991

Validation results:
gen_loss: 0.07274034
disc_loss: 2.7435088
disc_acc: 0.25049603174603174


	Epoch 7
Training results:
gen_loss: 0.07219532
disc_loss: 2.7977104
disc_acc: 0.07821782178217822

Validation results:
gen_loss: 0.07487428
disc_loss: 2.8123589
disc_acc: 0.17261904761904762


	Epoch 8
Training results:
gen_loss: 0.073812455
disc_loss: 2.813191
disc_acc: 0.07549504950495049

Validation results:
gen_loss: 0.07350679
disc_loss: 2.7835767
disc_acc: 0.06398809523809523


	Epoch 9
Training results:
gen_loss: 0.07164022
disc_loss: 2.831098
disc_acc: 0.06683168316831684

Validation results:
gen_loss: 0.07931442
disc_loss: 2.8216686
disc_acc: 0.054563492063492064


	Epoch 10
Training results:
gen_loss: 0.06344058
disc_loss: 2.8138087
disc_acc: 0.07017326732673268

Validation results:
gen_loss: 0.048431594
disc_loss: 2.8119202
disc_acc: 0.05853174603174603


	Epoch 11
Training results:
gen_loss: 0.052891545
disc_loss: 2.8026328
disc_acc: 0.06782178217821783

Validation results:
gen_loss: 0.05861601
disc_loss: 2.794742
disc_acc: 0.06349206349206349


	Epoch 12
Training results:
gen_loss: 0.04750169
disc_loss: 2.8071358
disc_acc: 0.06027227722772277

Validation results:
gen_loss: 0.06104495
disc_loss: 2.8411021
disc_acc: 0.09821428571428571


	Epoch 13
Training results:
gen_loss: 0.040901285
disc_loss: 2.803759
disc_acc: 0.0599009900990099

Validation results:
gen_loss: 0.030725723
disc_loss: 2.7889247
disc_acc: 0.06349206349206349


	Epoch 14
Training results:
gen_loss: 0.04402227
disc_loss: 2.805147
disc_acc: 0.06089108910891089

Validation results:
gen_loss: 0.06060456
disc_loss: 2.7975445
disc_acc: 0.06101190476190476


	Epoch 15
Training results:
gen_loss: 0.041119628
disc_loss: 2.8069324
disc_acc: 0.06670792079207921

Validation results:
gen_loss: 0.05617028
disc_loss: 2.8041556
disc_acc: 0.06349206349206349


	Epoch 16
Training results:
gen_loss: 0.04433486
disc_loss: 2.8002858
disc_acc: 0.06262376237623762

Validation results:
gen_loss: 0.056774363
disc_loss: 2.7941215
disc_acc: 0.057539682539682536


	Epoch 17
Training results:
gen_loss: 0.042210296
disc_loss: 2.8055115
disc_acc: 0.06262376237623762

Validation results:
gen_loss: 0.03137178
disc_loss: 2.7999861
disc_acc: 0.06101190476190476


	Epoch 18
Training results:
gen_loss: 0.045170873
disc_loss: 2.8087263
disc_acc: 0.06386138613861386

Validation results:
gen_loss: 0.034715615
disc_loss: 2.8040457
disc_acc: 0.05505952380952381


	Epoch 19
Training results:
gen_loss: 0.03941899
disc_loss: 2.8158374
disc_acc: 0.05804455445544555

Validation results:
gen_loss: 0.03608156
disc_loss: 2.8292572
disc_acc: 0.05853174603174603


	Epoch 20
Training results:
gen_loss: 0.05104087
disc_loss: 2.8408952
disc_acc: 0.06695544554455446

Validation results:
gen_loss: 0.062720805
disc_loss: 2.7944047
disc_acc: 0.05406746031746032


	Epoch 21
Training results:
gen_loss: 0.04710485
disc_loss: 2.8055344
disc_acc: 0.059777227722772275

Validation results:
gen_loss: 0.030580187
disc_loss: 2.8065562
disc_acc: 0.07093253968253968


	Epoch 22
Training results:
gen_loss: 0.04038176
disc_loss: 2.7963612
disc_acc: 0.062376237623762376

Validation results:
gen_loss: 0.043789636
disc_loss: 2.8333116
disc_acc: 0.0625


	Epoch 23
Training results:
gen_loss: 0.04978251
disc_loss: 2.8174083
disc_acc: 0.06064356435643564

Validation results:
gen_loss: 0.048681762
disc_loss: 2.7998097
disc_acc: 0.05853174603174603


	Epoch 24
Training results:
gen_loss: 0.042490438
disc_loss: 2.8103297
disc_acc: 0.06150990099009901

Validation results:
gen_loss: 0.06739676
disc_loss: 2.80855
disc_acc: 0.05704365079365079


	Epoch 25
Training results:
gen_loss: 0.046041135
disc_loss: 2.8132613
disc_acc: 0.06435643564356436

Validation results:
gen_loss: 0.03679729
disc_loss: 2.834397
disc_acc: 0.061507936507936505


	Epoch 26
Training results:
gen_loss: 0.050109584
disc_loss: 2.827089
disc_acc: 0.06621287128712872

Validation results:
gen_loss: 0.047057837
disc_loss: 2.7934208
disc_acc: 0.05704365079365079


	Epoch 27
Training results:
gen_loss: 0.03399691
disc_loss: 2.8018591
disc_acc: 0.06547029702970297

Validation results:
gen_loss: 0.03592056
disc_loss: 2.800112
disc_acc: 0.05257936507936508


	Epoch 28
Training results:
gen_loss: 0.037785407
disc_loss: 2.8174288
disc_acc: 0.05891089108910891

Validation results:
gen_loss: 0.03611055
disc_loss: 2.8676891
disc_acc: 0.053075396825396824


	Epoch 29
Training results:
gen_loss: 0.03651297
disc_loss: 2.817975
disc_acc: 0.06596534653465347

Validation results:
gen_loss: 0.022303212
disc_loss: 2.7864618
disc_acc: 0.06349206349206349


	Epoch 30
Training results:
gen_loss: 0.023227949
disc_loss: 2.7988825
disc_acc: 0.06262376237623762

Validation results:
gen_loss: 0.014292554
disc_loss: 2.787936
disc_acc: 0.05505952380952381


	Epoch 31
Training results:
gen_loss: 0.023074323
disc_loss: 2.8195117
disc_acc: 0.05903465346534653

Validation results:
gen_loss: 0.028833048
disc_loss: 2.827436
disc_acc: 0.07390873015873016


	Epoch 32
Training results:
gen_loss: 0.024852185
disc_loss: 2.8514173
disc_acc: 0.061386138613861385

Validation results:
gen_loss: 0.027878612
disc_loss: 2.7876215
disc_acc: 0.053075396825396824


	Epoch 33
Training results:
gen_loss: 0.021519335
disc_loss: 2.799622
disc_acc: 0.06027227722772277

Validation results:
gen_loss: 0.016798055
disc_loss: 2.8180096
disc_acc: 0.06299603174603174


	Epoch 34
Training results:
gen_loss: 0.016943421
disc_loss: 2.7974231
disc_acc: 0.0625

Validation results:
gen_loss: 0.012818543
disc_loss: 2.8877716
disc_acc: 0.053075396825396824


	Epoch 35
Training results:
gen_loss: 0.015672741
disc_loss: 2.8091855
disc_acc: 0.062376237623762376

Validation results:
gen_loss: 0.008661125
disc_loss: 2.7956455
disc_acc: 0.060515873015873016


	Epoch 36
Training results:
gen_loss: 0.017284036
disc_loss: 2.8149447
disc_acc: 0.06113861386138614

Validation results:
gen_loss: 0.014951781
disc_loss: 2.814525
disc_acc: 0.06398809523809523


	Epoch 37
Training results:
gen_loss: 0.015455573
disc_loss: 2.8131237
disc_acc: 0.06175742574257426

Validation results:
gen_loss: 0.016458882
disc_loss: 2.785554
disc_acc: 0.0689484126984127


	Epoch 38
Training results:
gen_loss: 0.014100353
disc_loss: 2.8032
disc_acc: 0.06497524752475248

Validation results:
gen_loss: 0.011946362
disc_loss: 2.8155968
disc_acc: 0.05853174603174603


	Epoch 39
Training results:
gen_loss: 0.013962696
disc_loss: 2.8092158
disc_acc: 0.06262376237623762

Validation results:
gen_loss: 0.008710353
disc_loss: 2.8012962
disc_acc: 0.05853174603174603


	Epoch 40
Training results:
gen_loss: 0.016687687
disc_loss: 2.8171349
disc_acc: 0.0594059405940594

Validation results:
gen_loss: 0.011069138
disc_loss: 2.869416
disc_acc: 0.060515873015873016


	Epoch 41
Training results:
gen_loss: 0.016692935
disc_loss: 2.8254166
disc_acc: 0.06324257425742574

Validation results:
gen_loss: 0.021257935
disc_loss: 2.8356774
disc_acc: 0.05853174603174603


	Epoch 42
Training results:
gen_loss: 0.039203584
disc_loss: 4.4409604
disc_acc: 0.06188118811881188

Validation results:
gen_loss: 0.031014387
disc_loss: 3.2269838
disc_acc: 0.0689484126984127


	Epoch 43
Training results:
gen_loss: 0.033714645
disc_loss: 2.8889499
disc_acc: 0.059777227722772275

Validation results:
gen_loss: 0.013363912
disc_loss: 2.8343947
disc_acc: 0.05853174603174603


	Epoch 44
Training results:
gen_loss: 0.016632292
disc_loss: 2.8174179
disc_acc: 0.057301980198019804

Validation results:
gen_loss: 0.011744313
disc_loss: 2.7901726
disc_acc: 0.05704365079365079


	Epoch 45
Training results:
gen_loss: 0.009955165
disc_loss: 2.7975068
disc_acc: 0.06287128712871287

Validation results:
gen_loss: 0.0047041248
disc_loss: 2.7926948
disc_acc: 0.057539682539682536


	Epoch 46
Training results:
gen_loss: 0.007848268
disc_loss: 2.7879622
disc_acc: 0.06349009900990099

Validation results:
gen_loss: 0.009269008
disc_loss: 2.7751825
disc_acc: 0.06349206349206349


	Epoch 47
Training results:
gen_loss: 0.008379213
disc_loss: 2.7870634
disc_acc: 0.06287128712871287

Validation results:
gen_loss: 0.0072302166
disc_loss: 2.7936518
disc_acc: 0.05853174603174603


	Epoch 48
Training results:
gen_loss: 0.009523375
disc_loss: 2.7859778
disc_acc: 0.06126237623762376

Validation results:
gen_loss: 0.017173609
disc_loss: 2.7744312
disc_acc: 0.0679563492063492


	Epoch 49
Training results:
gen_loss: 0.009590175
disc_loss: 2.7862058
disc_acc: 0.06126237623762376

Validation results:
gen_loss: 0.006159216
disc_loss: 2.7967184
disc_acc: 0.05853174603174603


	Epoch 50
Training results:
gen_loss: 0.008167741
disc_loss: 2.7931893
disc_acc: 0.05891089108910891

Validation results:
gen_loss: 0.010304404
disc_loss: 2.7911105
disc_acc: 0.0625


	Epoch 51
Training results:
gen_loss: 0.008193065
disc_loss: 2.7975655
disc_acc: 0.062376237623762376

Validation results:
gen_loss: 0.006090503
disc_loss: 2.7819717
disc_acc: 0.05853174603174603


	Epoch 52
Training results:
gen_loss: 0.00808584
disc_loss: 2.8033986
disc_acc: 0.06089108910891089

Validation results:
gen_loss: 0.005635741
disc_loss: 2.7885067
disc_acc: 0.07093253968253968


	Epoch 53
Training results:
gen_loss: 0.008223118
disc_loss: 2.8075154
disc_acc: 0.059777227722772275

Validation results:
gen_loss: 0.0059172227
disc_loss: 2.8009522
disc_acc: 0.05853174603174603


	Epoch 54
Training results:
gen_loss: 0.011031963
disc_loss: 2.8121727
disc_acc: 0.06571782178217822

Validation results:
gen_loss: 0.007094216
disc_loss: 2.8222055
disc_acc: 0.062003968253968256


	Epoch 55
Training results:
gen_loss: 0.00723785
disc_loss: 2.8103485
disc_acc: 0.057301980198019804

Validation results:
gen_loss: 0.0048055225
disc_loss: 2.7835457
disc_acc: 0.07093253968253968


	Epoch 56
Training results:
gen_loss: 0.009751549
disc_loss: 2.8229308
disc_acc: 0.06002475247524752

Validation results:
gen_loss: 0.0060451385
disc_loss: 2.812001
disc_acc: 0.07093253968253968


	Epoch 57
Training results:
gen_loss: 0.00721051
disc_loss: 2.8141282
disc_acc: 0.057920792079207924

Validation results:
gen_loss: 0.00026214795
disc_loss: 2.852008
disc_acc: 0.05853174603174603


	Epoch 58
Training results:
gen_loss: 0.008872291
disc_loss: 2.8396926
disc_acc: 0.06336633663366337

Validation results:
gen_loss: 0.0063414183
disc_loss: 2.7952144
disc_acc: 0.05853174603174603


	Epoch 59
Training results:
gen_loss: 0.00828784
disc_loss: 2.8035326
disc_acc: 0.06175742574257426

Validation results:
gen_loss: 0.0060780933
disc_loss: 2.8392005
disc_acc: 0.06299603174603174


	Epoch 60
Training results:
gen_loss: 0.008762008
disc_loss: 2.8115523
disc_acc: 0.06274752475247525

Validation results:
gen_loss: 0.00402507
disc_loss: 2.8806663
disc_acc: 0.06001984126984127


	Epoch 61
Training results:
gen_loss: 0.005859589
disc_loss: 2.8041158
disc_acc: 0.061386138613861385

Validation results:
gen_loss: 0.007598688
disc_loss: 2.7945616
disc_acc: 0.06398809523809523


	Epoch 62
Training results:
gen_loss: 0.0068527497
disc_loss: 2.9935527
disc_acc: 0.06386138613861386

Validation results:
gen_loss: 0.003957878
disc_loss: 2.8014743
disc_acc: 0.05803571428571429


	Epoch 63
Training results:
gen_loss: 0.005069174
disc_loss: 2.7966468
disc_acc: 0.0646039603960396

Validation results:
gen_loss: 0.0037362091
disc_loss: 2.8007627
disc_acc: 0.05257936507936508


	Epoch 64
Training results:
gen_loss: 0.005718644
disc_loss: 2.7885182
disc_acc: 0.05903465346534653

Validation results:
gen_loss: 0.0118434355
disc_loss: 2.778179
disc_acc: 0.06398809523809523


	Epoch 65
Training results:
gen_loss: 0.0059725223
disc_loss: 2.7875338
disc_acc: 0.0625

Validation results:
gen_loss: 0.0044185515
disc_loss: 2.7853692
disc_acc: 0.05853174603174603


	Epoch 66
Training results:
gen_loss: 0.005574739
disc_loss: 2.7974691
disc_acc: 0.06324257425742574

Validation results:
gen_loss: 0.008189818
disc_loss: 2.8559787
disc_acc: 0.062003968253968256


	Epoch 67
Training results:
gen_loss: 0.0062633776
disc_loss: 2.813528
disc_acc: 0.05618811881188119

Validation results:
gen_loss: 0.008418563
disc_loss: 2.817802
disc_acc: 0.06398809523809523


	Epoch 68
Training results:
gen_loss: 0.00519576
disc_loss: 2.8092904
disc_acc: 0.06287128712871287

Validation results:
gen_loss: 0.0066973916
disc_loss: 2.7837672
disc_acc: 0.062003968253968256


	Epoch 69
Training results:
gen_loss: 0.003570032
disc_loss: 2.816674
disc_acc: 0.0655940594059406

Validation results:
gen_loss: 0.0015949487
disc_loss: 2.8004591
disc_acc: 0.0689484126984127


	Epoch 70
Training results:
gen_loss: 0.00216356
disc_loss: 2.8100553
disc_acc: 0.06089108910891089

Validation results:
gen_loss: 0.0038702593
disc_loss: 2.9000065
disc_acc: 0.05704365079365079


	Epoch 71
Training results:
gen_loss: 0.0021735013
disc_loss: 2.8056784
disc_acc: 0.06485148514851485

Validation results:
gen_loss: 0.001964653
disc_loss: 2.9167547
disc_acc: 0.05704365079365079


	Epoch 72
Training results:
gen_loss: 0.0021952328
disc_loss: 2.8749135
disc_acc: 0.057920792079207924

Validation results:
gen_loss: 0.0021622968
disc_loss: 2.780253
disc_acc: 0.05853174603174603


	Epoch 73
Training results:
gen_loss: 0.0021919813
disc_loss: 2.7966487
disc_acc: 0.062004950495049505

Validation results:
gen_loss: 0.0014568954
disc_loss: 2.7941034
disc_acc: 0.06349206349206349


	Epoch 74
Training results:
gen_loss: 0.0020675873
disc_loss: 2.795686
disc_acc: 0.06175742574257426

Validation results:
gen_loss: 0.0029622384
disc_loss: 2.8447382
disc_acc: 0.05853174603174603


	Epoch 75
Training results:
gen_loss: 0.0020865283
disc_loss: 2.8056867
disc_acc: 0.06212871287128713

Validation results:
gen_loss: 0.0010098303
disc_loss: 2.8361464
disc_acc: 0.053075396825396824


	Epoch 76
Training results:
gen_loss: 0.0018462762
disc_loss: 2.821972
disc_acc: 0.06336633663366337

Validation results:
gen_loss: 0.0014121586
disc_loss: 2.8677795
disc_acc: 0.05803571428571429


	Epoch 77
Training results:
gen_loss: 0.0026794039
disc_loss: 2.8355904
disc_acc: 0.062376237623762376

Validation results:
gen_loss: 0.0053133834
disc_loss: 2.802871
disc_acc: 0.05853174603174603


	Epoch 78
Training results:
gen_loss: 0.0008929193
disc_loss: 2.7979913
disc_acc: 0.057920792079207924

Validation results:
gen_loss: 2.2913331e-05
disc_loss: 2.8607645
disc_acc: 0.05853174603174603


	Epoch 79
Training results:
gen_loss: 3.4419736e-05
disc_loss: 2.7987928
disc_acc: 0.06324257425742574

Validation results:
gen_loss: 4.20151e-05
disc_loss: 2.8188791
disc_acc: 0.07341269841269842


	Epoch 80
Training results:
gen_loss: 6.75893e-05
disc_loss: 2.8144996
disc_acc: 0.06311881188118812

Validation results:
gen_loss: 0.00028316843
disc_loss: 2.8601043
disc_acc: 0.062003968253968256


	Epoch 81
Training results:
gen_loss: 0.00017132646
disc_loss: 2.8218586
disc_acc: 0.06064356435643564

Validation results:
gen_loss: 1.7048045e-05
disc_loss: 2.7883322
disc_acc: 0.07291666666666667


	Epoch 82
Training results:
gen_loss: 3.0959327e-05
disc_loss: 2.8011987
disc_acc: 0.0599009900990099

Validation results:
gen_loss: 1.3031164e-05
disc_loss: 2.7857194
disc_acc: 0.06398809523809523


	Epoch 83
Training results:
gen_loss: 0.00013928038
disc_loss: 2.8248374
disc_acc: 0.05928217821782178

Validation results:
gen_loss: 9.7000906e-05
disc_loss: 2.8131409
disc_acc: 0.062003968253968256


	Epoch 84
Training results:
gen_loss: 4.3471075e-05
disc_loss: 2.8064933
disc_acc: 0.058292079207920795

Validation results:
gen_loss: 5.8729697e-06
disc_loss: 2.8462055
disc_acc: 0.062003968253968256


	Epoch 85
Training results:
gen_loss: 8.965149e-05
disc_loss: 2.8189042
disc_acc: 0.05928217821782178

Validation results:
gen_loss: 0.0017560913
disc_loss: 2.8817766
disc_acc: 0.061507936507936505


	Epoch 86
Training results:
gen_loss: 0.00010899581
disc_loss: 2.8359184
disc_acc: 0.06571782178217822

Validation results:
gen_loss: 1.6310041e-05
disc_loss: 2.7924783
disc_acc: 0.062003968253968256


	Epoch 87
Training results:
gen_loss: 8.138226e-06
disc_loss: 2.7983568
disc_acc: 0.06287128712871287

Validation results:
gen_loss: 5.6921604e-06
disc_loss: 2.7870476
disc_acc: 0.07341269841269842


	Epoch 88
Training results:
gen_loss: 5.1471056e-06
disc_loss: 2.801988
disc_acc: 0.0625

Validation results:
gen_loss: 6.8530503e-06
disc_loss: 2.810628
disc_acc: 0.07390873015873016


	Epoch 89
Training results:
gen_loss: 0.00010804667
disc_loss: 2.8298938
disc_acc: 0.06188118811881188

Validation results:
gen_loss: 1.6579288e-05
disc_loss: 2.779863
disc_acc: 0.062003968253968256


	Epoch 90
Training results:
gen_loss: 1.0809635e-05
disc_loss: 2.8020809
disc_acc: 0.06448019801980198

Validation results:
gen_loss: 7.803263e-06
disc_loss: 2.8202112
disc_acc: 0.06398809523809523


	Epoch 91
Training results:
gen_loss: 9.63937e-06
disc_loss: 2.8072836
disc_acc: 0.06410891089108911

Validation results:
gen_loss: 3.0194476e-06
disc_loss: 2.7934551
disc_acc: 0.057539682539682536


	Epoch 92
Training results:
gen_loss: 9.271966e-05
disc_loss: 2.8268633
disc_acc: 0.06262376237623762

Validation results:
gen_loss: 0.00017416793
disc_loss: 2.8201458
disc_acc: 0.061507936507936505


	Epoch 93
Training results:
gen_loss: 9.138851e-05
disc_loss: 2.8120408
disc_acc: 0.06089108910891089

Validation results:
gen_loss: 0.00020635236
disc_loss: 2.817635
disc_acc: 0.06398809523809523


	Epoch 94
Training results:
gen_loss: 4.6700865e-05
disc_loss: 2.8132725
disc_acc: 0.06472772277227723

Validation results:
gen_loss: 0.00013258323
disc_loss: 2.8019066
disc_acc: 0.060515873015873016


	Epoch 95
Training results:
gen_loss: 4.2816246e-05
disc_loss: 2.8037782
disc_acc: 0.0625

Validation results:
gen_loss: 6.418766e-06
disc_loss: 2.7870631
disc_acc: 0.061507936507936505


	Epoch 96
Training results:
gen_loss: 5.1043908e-06
disc_loss: 2.8006408
disc_acc: 0.061014851485148514

Validation results:
gen_loss: 5.0235667e-06
disc_loss: 2.7808497
disc_acc: 0.05853174603174603


	Epoch 97
Training results:
gen_loss: 3.204947e-05
disc_loss: 2.80902
disc_acc: 0.0646039603960396

Validation results:
gen_loss: 0.00030128943
disc_loss: 2.8093781
disc_acc: 0.06349206349206349


	Epoch 98
Training results:
gen_loss: 0.0002291931
disc_loss: 2.8218775
disc_acc: 0.06373762376237624

Validation results:
gen_loss: 5.0638922e-05
disc_loss: 2.7897274
disc_acc: 0.06746031746031746


	Epoch 99
Training results:
gen_loss: 1.6214515e-05
disc_loss: 2.8009539
disc_acc: 0.0625

Validation results:
gen_loss: 5.2376317e-06
disc_loss: 2.7871637
disc_acc: 0.07390873015873016


	Epoch 100
Training results:
gen_loss: 0.00030895197
disc_loss: 2.8249047
disc_acc: 0.06002475247524752

Validation results:
gen_loss: 0.037377257
disc_loss: 2.7979276
disc_acc: 0.05505952380952381



Training new discriminator on static trained discriminator.
	Initial performance
Training results:
gen_loss: 0.010746637
disc_loss: 14.817089
disc_acc: 0.0

Validation results:
gen_loss: 0.010737705
disc_loss: 14.95784
disc_acc: 0.0


	Epoch 1
Training results:
gen_loss: 0.054508075
disc_loss: 9.325473
disc_acc: 0.06324257425742574

Validation results:
gen_loss: 0.0472757
disc_loss: 3.0624688
disc_acc: 0.05257936507936508


	Epoch 2
Training results:
gen_loss: 0.047700148
disc_loss: 3.0854073
disc_acc: 0.059158415841584155

Validation results:
gen_loss: 0.047988575
disc_loss: 2.9152462
disc_acc: 0.0679563492063492


	Epoch 3
Training results:
gen_loss: 0.04558691
disc_loss: 3.1135714
disc_acc: 0.058292079207920795

Validation results:
gen_loss: 0.04760931
disc_loss: 3.0103312
disc_acc: 0.0625


	Epoch 4
Training results:
gen_loss: 0.044237804
disc_loss: 3.0384722
disc_acc: 0.059158415841584155

Validation results:
gen_loss: 0.047991514
disc_loss: 3.506817
disc_acc: 0.05257936507936508


	Epoch 5
Training results:
gen_loss: 0.04354302
disc_loss: 3.0213675
disc_acc: 0.05891089108910891

Validation results:
gen_loss: 0.039362878
disc_loss: 2.988149
disc_acc: 0.07390873015873016


	Epoch 6
Training results:
gen_loss: 0.04135521
disc_loss: 2.9769132
disc_acc: 0.06522277227722773

Validation results:
gen_loss: 0.04377867
disc_loss: 3.0994825
disc_acc: 0.07291666666666667


	Epoch 7
Training results:
gen_loss: 0.03846307
disc_loss: 2.9400585
disc_acc: 0.06212871287128713

Validation results:
gen_loss: 0.04317741
disc_loss: 2.8702135
disc_acc: 0.07093253968253968


	Epoch 8
Training results:
gen_loss: 0.03667456
disc_loss: 2.90459
disc_acc: 0.062004950495049505

Validation results:
gen_loss: 0.03752191
disc_loss: 2.834206
disc_acc: 0.07093253968253968


	Epoch 9
Training results:
gen_loss: 0.038033552
disc_loss: 2.902551
disc_acc: 0.06188118811881188

Validation results:
gen_loss: 0.035613146
disc_loss: 3.0137668
disc_acc: 0.05952380952380952


	Epoch 10
Training results:
gen_loss: 0.039240856
disc_loss: 2.9524193
disc_acc: 0.060396039603960394

Validation results:
gen_loss: 0.044181895
disc_loss: 2.9814749
disc_acc: 0.06349206349206349


	Epoch 11
Training results:
gen_loss: 0.042776782
disc_loss: 2.9312055
disc_acc: 0.061386138613861385

Validation results:
gen_loss: 0.05072066
disc_loss: 3.0097888
disc_acc: 0.06349206349206349


	Epoch 12
Training results:
gen_loss: 0.04862053
disc_loss: 2.920789
disc_acc: 0.06423267326732673

Validation results:
gen_loss: 0.04232222
disc_loss: 2.9208944
disc_acc: 0.062003968253968256


	Epoch 13
Training results:
gen_loss: 0.05037918
disc_loss: 2.9544806
disc_acc: 0.0599009900990099

Validation results:
gen_loss: 0.046178408
disc_loss: 3.4097168
disc_acc: 0.07093253968253968


	Epoch 14
Training results:
gen_loss: 0.047534164
disc_loss: 2.8822963
disc_acc: 0.06596534653465347

Validation results:
gen_loss: 0.046253566
disc_loss: 2.9301162
disc_acc: 0.057539682539682536


	Epoch 15
Training results:
gen_loss: 0.049575288
disc_loss: 2.8717732
disc_acc: 0.061014851485148514

Validation results:
gen_loss: 0.039542984
disc_loss: 2.8343449
disc_acc: 0.060515873015873016


	Epoch 16
Training results:
gen_loss: 0.051403653
disc_loss: 2.823984
disc_acc: 0.061014851485148514

Validation results:
gen_loss: 0.028204167
disc_loss: 2.8455336
disc_acc: 0.06001984126984127


	Epoch 17
Training results:
gen_loss: 0.050651543
disc_loss: 2.8315487
disc_acc: 0.05804455445544555

Validation results:
gen_loss: 0.04135457
disc_loss: 2.8184578
disc_acc: 0.057539682539682536


	Epoch 18
Training results:
gen_loss: 0.05332021
disc_loss: 2.8252034
disc_acc: 0.060396039603960394

Validation results:
gen_loss: 0.036694832
disc_loss: 2.8863964
disc_acc: 0.0679563492063492


	Epoch 19
Training results:
gen_loss: 0.05538275
disc_loss: 2.8162966
disc_acc: 0.06175742574257426

Validation results:
gen_loss: 0.05311293
disc_loss: 2.7972405
disc_acc: 0.05853174603174603


	Epoch 20
Training results:
gen_loss: 0.058836304
disc_loss: 2.801205
disc_acc: 0.06349009900990099

Validation results:
gen_loss: 0.05989036
disc_loss: 2.7916877
disc_acc: 0.06845238095238096


	Epoch 21
Training results:
gen_loss: 0.065137066
disc_loss: 2.8119566
disc_acc: 0.06027227722772277

Validation results:
gen_loss: 0.05493831
disc_loss: 2.7988617
disc_acc: 0.0689484126984127


	Epoch 22
Training results:
gen_loss: 0.07542318
disc_loss: 2.8516123
disc_acc: 0.0676980198019802

Validation results:
gen_loss: 0.08890067
disc_loss: 2.7813706
disc_acc: 0.0625


	Epoch 23
Training results:
gen_loss: 0.10136743
disc_loss: 2.804896
disc_acc: 0.06089108910891089

Validation results:
gen_loss: 0.10346566
disc_loss: 2.8063495
disc_acc: 0.05803571428571429


	Epoch 24
Training results:
gen_loss: 0.120270416
disc_loss: 2.8191073
disc_acc: 0.06720297029702971

Validation results:
gen_loss: 0.04452513
disc_loss: 2.9383664
disc_acc: 0.05853174603174603


	Epoch 25
Training results:
gen_loss: 0.1427661
disc_loss: 2.8578906
disc_acc: 0.06212871287128713

Validation results:
gen_loss: 0.13960586
disc_loss: 2.795234
disc_acc: 0.0679563492063492


	Epoch 26
Training results:
gen_loss: 0.16795123
disc_loss: 2.7967136
disc_acc: 0.06163366336633663

Validation results:
gen_loss: 0.1774175
disc_loss: 2.8129194
disc_acc: 0.05257936507936508


	Epoch 27
Training results:
gen_loss: 0.20055576
disc_loss: 2.8301024
disc_acc: 0.06448019801980198

Validation results:
gen_loss: 0.020175526
disc_loss: 2.8344357
disc_acc: 0.06299603174603174


	Epoch 28
Training results:
gen_loss: 0.2096354
disc_loss: 2.7977235
disc_acc: 0.06113861386138614

Validation results:
gen_loss: 0.22465664
disc_loss: 2.8010094
disc_acc: 0.05505952380952381


	Epoch 29
Training results:
gen_loss: 0.2514568
disc_loss: 2.8316624
disc_acc: 0.062376237623762376

Validation results:
gen_loss: 0.13379152
disc_loss: 2.8047109
disc_acc: 0.053075396825396824


	Epoch 30
Training results:
gen_loss: 0.28086266
disc_loss: 2.8003943
disc_acc: 0.06225247524752475

Validation results:
gen_loss: 0.16689764
disc_loss: 2.7951705
disc_acc: 0.062003968253968256


	Epoch 31
Training results:
gen_loss: 0.28318134
disc_loss: 2.8361764
disc_acc: 0.06423267326732673

Validation results:
gen_loss: 0.03712689
disc_loss: 2.8664281
disc_acc: 0.062003968253968256


	Epoch 32
Training results:
gen_loss: 0.32491636
disc_loss: 2.8082254
disc_acc: 0.057920792079207924

Validation results:
gen_loss: 0.0010127347
disc_loss: 2.8062062
disc_acc: 0.053075396825396824


	Epoch 33
Training results:
gen_loss: 0.5884488
disc_loss: 18.420275
disc_acc: 0.060148514851485146

Validation results:
gen_loss: 0.453398
disc_loss: 4.204851
disc_acc: 0.0689484126984127


	Epoch 34
Training results:
gen_loss: 0.40487304
disc_loss: 3.1400099
disc_acc: 0.06596534653465347

Validation results:
gen_loss: 0.3823108
disc_loss: 2.9179282
disc_acc: 0.06398809523809523


	Epoch 35
Training results:
gen_loss: 0.3520378
disc_loss: 2.952671
disc_acc: 0.059777227722772275

Validation results:
gen_loss: 0.3455026
disc_loss: 2.8467095
disc_acc: 0.052083333333333336


	Epoch 36
Training results:
gen_loss: 0.3226901
disc_loss: 2.9130962
disc_acc: 0.06695544554455446

Validation results:
gen_loss: 0.31698886
disc_loss: 2.848135
disc_acc: 0.06349206349206349


	Epoch 37
Training results:
gen_loss: 0.2793325
disc_loss: 2.8819084
disc_acc: 0.061014851485148514

Validation results:
gen_loss: 0.24455504
disc_loss: 2.8454316
disc_acc: 0.05853174603174603


	Epoch 38
Training results:
gen_loss: 0.22794503
disc_loss: 2.8649943
disc_acc: 0.06621287128712872

Validation results:
gen_loss: 0.22928025
disc_loss: 2.9323888
disc_acc: 0.06349206349206349


	Epoch 39
Training results:
gen_loss: 0.22569273
disc_loss: 2.8378289
disc_acc: 0.05866336633663366

Validation results:
gen_loss: 0.20541835
disc_loss: 2.8480337
disc_acc: 0.05505952380952381


	Epoch 40
Training results:
gen_loss: 0.18191162
disc_loss: 2.8307903
disc_acc: 0.05952970297029703

Validation results:
gen_loss: 0.142739
disc_loss: 2.8290706
disc_acc: 0.06398809523809523


	Epoch 41
Training results:
gen_loss: 0.12029926
disc_loss: 2.8163457
disc_acc: 0.06175742574257426

Validation results:
gen_loss: 0.1280574
disc_loss: 2.838114
disc_acc: 0.054563492063492064


	Epoch 42
Training results:
gen_loss: 0.12744649
disc_loss: 2.8160157
disc_acc: 0.0629950495049505

Validation results:
gen_loss: 0.13054667
disc_loss: 2.8131104
disc_acc: 0.05803571428571429


	Epoch 43
Training results:
gen_loss: 0.13144435
disc_loss: 2.809246
disc_acc: 0.05804455445544555

Validation results:
gen_loss: 0.22981855
disc_loss: 2.8080428
disc_acc: 0.05704365079365079


	Epoch 44
Training results:
gen_loss: 0.14000928
disc_loss: 2.807167
disc_acc: 0.06089108910891089

Validation results:
gen_loss: 0.1442479
disc_loss: 2.8046088
disc_acc: 0.06349206349206349


	Epoch 45
Training results:
gen_loss: 0.14497517
disc_loss: 2.798963
disc_acc: 0.06262376237623762

Validation results:
gen_loss: 0.15694879
disc_loss: 2.7833495
disc_acc: 0.0679563492063492


	Epoch 46
Training results:
gen_loss: 0.15050773
disc_loss: 2.8002446
disc_acc: 0.060396039603960394

Validation results:
gen_loss: 0.137473
disc_loss: 2.8303185
disc_acc: 0.06349206349206349


	Epoch 47
Training results:
gen_loss: 0.14670897
disc_loss: 2.798562
disc_acc: 0.0625

Validation results:
gen_loss: 0.15721042
disc_loss: 2.8253486
disc_acc: 0.0625


	Epoch 48
Training results:
gen_loss: 0.16211258
disc_loss: 2.7981772
disc_acc: 0.06175742574257426

Validation results:
gen_loss: 0.1536292
disc_loss: 2.8000224
disc_acc: 0.0625


	Epoch 49
Training results:
gen_loss: 0.1689597
disc_loss: 2.805358
disc_acc: 0.060148514851485146

Validation results:
gen_loss: 0.39851025
disc_loss: 2.7867332
disc_acc: 0.06349206349206349


	Epoch 50
Training results:
gen_loss: 0.17448023
disc_loss: 2.7956333
disc_acc: 0.06126237623762376

Validation results:
gen_loss: 0.00910818
disc_loss: 2.808855
disc_acc: 0.05853174603174603


	Epoch 51
Training results:
gen_loss: 0.19238365
disc_loss: 2.7989326
disc_acc: 0.0625

Validation results:
gen_loss: 0.20642021
disc_loss: 2.7936738
disc_acc: 0.07390873015873016


	Epoch 52
Training results:
gen_loss: 0.21449047
disc_loss: 2.8190036
disc_acc: 0.0650990099009901

Validation results:
gen_loss: 0.5914414
disc_loss: 2.8003702
disc_acc: 0.06349206349206349


	Epoch 53
Training results:
gen_loss: 0.21632662
disc_loss: 2.818142
disc_acc: 0.06311881188118812

Validation results:
gen_loss: 0.2262732
disc_loss: 2.853516
disc_acc: 0.05406746031746032


	Epoch 54
Training results:
gen_loss: 0.22207259
disc_loss: 2.8187873
disc_acc: 0.06967821782178218

Validation results:
gen_loss: 0.37979108
disc_loss: 2.8033078
disc_acc: 0.05853174603174603


	Epoch 55
Training results:
gen_loss: 0.24994119
disc_loss: 2.8139498
disc_acc: 0.057673267326732676

Validation results:
gen_loss: 0.001721308
disc_loss: 2.7976437
disc_acc: 0.061507936507936505


	Epoch 56
Training results:
gen_loss: 0.2666189
disc_loss: 2.8137844
disc_acc: 0.06150990099009901

Validation results:
gen_loss: 0.0018058376
disc_loss: 2.8841608
disc_acc: 0.053075396825396824


	Epoch 57
Training results:
gen_loss: 0.29704067
disc_loss: 2.8247395
disc_acc: 0.06596534653465347

Validation results:
gen_loss: 0.002222106
disc_loss: 2.8125713
disc_acc: 0.06845238095238096


	Epoch 58
Training results:
gen_loss: 0.34672824
disc_loss: 2.8251426
disc_acc: 0.060148514851485146

Validation results:
gen_loss: 0.4071134
disc_loss: 2.7991896
disc_acc: 0.06845238095238096


	Epoch 59
Training results:
gen_loss: 0.4948781
disc_loss: 2.993102
disc_acc: 0.06287128712871287

Validation results:
gen_loss: 0.0039420477
disc_loss: 2.9138694
disc_acc: 0.06349206349206349


	Epoch 60
Training results:
gen_loss: 0.40436026
disc_loss: 2.815782
disc_acc: 0.06398514851485149

Validation results:
gen_loss: 0.36859947
disc_loss: 2.7981012
disc_acc: 0.057539682539682536


	Epoch 61
Training results:
gen_loss: 0.34134826
disc_loss: 2.789554
disc_acc: 0.0655940594059406

Validation results:
gen_loss: 0.0013322717
disc_loss: 2.8092709
disc_acc: 0.05505952380952381


	Epoch 62
Training results:
gen_loss: 1.3624984
disc_loss: 13.2327
disc_acc: 0.06571782178217822

Validation results:
gen_loss: 1.2695599
disc_loss: 2.973141
disc_acc: 0.06398809523809523


	Epoch 63
Training results:
gen_loss: 1.1767987
disc_loss: 3.0388927
disc_acc: 0.05754950495049505

Validation results:
gen_loss: 1.1310086
disc_loss: 3.0592515
disc_acc: 0.05853174603174603


	Epoch 64
Training results:
gen_loss: 1.0657629
disc_loss: 2.9339592
disc_acc: 0.06064356435643564

Validation results:
gen_loss: 0.9836366
disc_loss: 2.883973
disc_acc: 0.06349206349206349


	Epoch 65
Training results:
gen_loss: 0.9101532
disc_loss: 2.9054751
disc_acc: 0.062004950495049505

Validation results:
gen_loss: 0.8814198
disc_loss: 2.8213353
disc_acc: 0.05853174603174603


	Epoch 66
Training results:
gen_loss: 0.8174163
disc_loss: 2.8631473
disc_acc: 0.06398514851485149

Validation results:
gen_loss: 0.7817363
disc_loss: 2.8306692
disc_acc: 0.054563492063492064


	Epoch 67
Training results:
gen_loss: 0.7509232
disc_loss: 2.8411043
disc_acc: 0.06386138613861386

Validation results:
gen_loss: 0.7151384
disc_loss: 2.9041436
disc_acc: 0.05505952380952381


	Epoch 68
Training results:
gen_loss: 0.7309934
disc_loss: 2.9193826
disc_acc: 0.062004950495049505

Validation results:
gen_loss: 0.8820575
disc_loss: 2.9050758
disc_acc: 0.05853174603174603


	Epoch 69
Training results:
gen_loss: 1.5434914
disc_loss: 11.887538
disc_acc: 0.06794554455445545

Validation results:
gen_loss: 1.5568666
disc_loss: 3.0794291
disc_acc: 0.07390873015873016


	Epoch 70
Training results:
gen_loss: 1.5317256
disc_loss: 3.0691593
disc_acc: 0.05952970297029703

Validation results:
gen_loss: 1.4673673
disc_loss: 3.0051095
disc_acc: 0.061507936507936505


	Epoch 71
Training results:
gen_loss: 1.2390966
disc_loss: 2.9353304
disc_acc: 0.059158415841584155

Validation results:
gen_loss: 1.1644578
disc_loss: 2.948839
disc_acc: 0.054563492063492064


	Epoch 72
Training results:
gen_loss: 1.1368033
disc_loss: 2.9002964
disc_acc: 0.06126237623762376

Validation results:
gen_loss: 1.089458
disc_loss: 2.8503706
disc_acc: 0.05853174603174603


	Epoch 73
Training results:
gen_loss: 0.95604306
disc_loss: 2.862624
disc_acc: 0.06027227722772277

Validation results:
gen_loss: 0.8429464
disc_loss: 3.1601892
disc_acc: 0.05505952380952381


	Epoch 74
Training results:
gen_loss: 0.7629704
disc_loss: 2.8539405
disc_acc: 0.05891089108910891

Validation results:
gen_loss: 0.7218386
disc_loss: 2.8232028
disc_acc: 0.0689484126984127


	Epoch 75
Training results:
gen_loss: 0.66344684
disc_loss: 2.8287961
disc_acc: 0.06472772277227723

Validation results:
gen_loss: 0.6358933
disc_loss: 2.788087
disc_acc: 0.057539682539682536


	Epoch 76
Training results:
gen_loss: 0.6636919
disc_loss: 2.815128
disc_acc: 0.06002475247524752

Validation results:
gen_loss: 0.7130174
disc_loss: 2.8017766
disc_acc: 0.057539682539682536


	Epoch 77
Training results:
gen_loss: 0.7674243
disc_loss: 2.8074102
disc_acc: 0.06386138613861386

Validation results:
gen_loss: 0.7252654
disc_loss: 2.7977514
disc_acc: 0.05257936507936508


	Epoch 78
Training results:
gen_loss: 0.737999
disc_loss: 2.8051178
disc_acc: 0.0625

Validation results:
gen_loss: 0.7074315
disc_loss: 2.8282006
disc_acc: 0.05257936507936508


	Epoch 79
Training results:
gen_loss: 0.73159385
disc_loss: 2.813287
disc_acc: 0.06646039603960396

Validation results:
gen_loss: 0.81106734
disc_loss: 2.794541
disc_acc: 0.07093253968253968


	Epoch 80
Training results:
gen_loss: 0.78544515
disc_loss: 2.8093364
disc_acc: 0.06126237623762376

Validation results:
gen_loss: 0.82791656
disc_loss: 2.85862
disc_acc: 0.060515873015873016


	Epoch 81
Training results:
gen_loss: 1.017139
disc_loss: 2.809977
disc_acc: 0.06423267326732673

Validation results:
gen_loss: 0.8119878
disc_loss: 3.199717
disc_acc: 0.0


	Epoch 82
Training results:
gen_loss: 1.111312
disc_loss: 2.8076618
disc_acc: 0.06423267326732673

Validation results:
gen_loss: 2.0704482
disc_loss: 2.7948596
disc_acc: 0.060515873015873016


	Epoch 83
Training results:
gen_loss: 0.9437933
disc_loss: 8.265615
disc_acc: 0.062376237623762376

Validation results:
gen_loss: 0.6061444
disc_loss: 2.9493997
disc_acc: 0.06398809523809523


	Epoch 84
Training results:
gen_loss: 0.5957036
disc_loss: 2.9257965
disc_acc: 0.06410891089108911

Validation results:
gen_loss: 0.55978554
disc_loss: 2.8331869
disc_acc: 0.060515873015873016


	Epoch 85
Training results:
gen_loss: 0.59205925
disc_loss: 2.8539639
disc_acc: 0.06547029702970297

Validation results:
gen_loss: 0.5797694
disc_loss: 2.806271
disc_acc: 0.07093253968253968


	Epoch 86
Training results:
gen_loss: 0.50990385
disc_loss: 2.8427474
disc_acc: 0.062376237623762376

Validation results:
gen_loss: 0.5651048
disc_loss: 2.787884
disc_acc: 0.07093253968253968


	Epoch 87
Training results:
gen_loss: 0.47089013
disc_loss: 2.8163874
disc_acc: 0.06163366336633663

Validation results:
gen_loss: 0.31301394
disc_loss: 2.8209307
disc_acc: 0.06398809523809523


	Epoch 88
Training results:
gen_loss: 0.32331985
disc_loss: 2.8123064
disc_acc: 0.06311881188118812

Validation results:
gen_loss: 0.31691465
disc_loss: 2.781371
disc_acc: 0.0689484126984127


	Epoch 89
Training results:
gen_loss: 0.19992083
disc_loss: 2.8028448
disc_acc: 0.0594059405940594

Validation results:
gen_loss: 0.007094949
disc_loss: 2.8143141
disc_acc: 0.06845238095238096


	Epoch 90
Training results:
gen_loss: 0.007170656
disc_loss: 2.7983437
disc_acc: 0.06027227722772277

Validation results:
gen_loss: 0.006575424
disc_loss: 2.7824755
disc_acc: 0.06349206349206349


	Epoch 91
Training results:
gen_loss: 0.006311321
disc_loss: 2.8011298
disc_acc: 0.05928217821782178

Validation results:
gen_loss: 0.0048278095
disc_loss: 2.7992568
disc_acc: 0.06101190476190476


	Epoch 92
Training results:
gen_loss: 0.006456698
disc_loss: 2.7949507
disc_acc: 0.06002475247524752

Validation results:
gen_loss: 0.0046051494
disc_loss: 2.7793634
disc_acc: 0.06349206349206349


	Epoch 93
Training results:
gen_loss: 0.26703075
disc_loss: 2.7995598
disc_acc: 0.06150990099009901

Validation results:
gen_loss: 0.32034877
disc_loss: 2.8083916
disc_acc: 0.05505952380952381


	Epoch 94
Training results:
gen_loss: 0.33682686
disc_loss: 2.795736
disc_acc: 0.061014851485148514

Validation results:
gen_loss: 0.57209677
disc_loss: 2.7809432
disc_acc: 0.06845238095238096


	Epoch 95
Training results:
gen_loss: 0.36447686
disc_loss: 2.8195724
disc_acc: 0.06448019801980198

Validation results:
gen_loss: 0.0032473062
disc_loss: 2.8820796
disc_acc: 0.061507936507936505


	Epoch 96
Training results:
gen_loss: 0.3446549
disc_loss: 2.8223195
disc_acc: 0.06683168316831684

Validation results:
gen_loss: 0.0014802122
disc_loss: 2.8211508
disc_acc: 0.07390873015873016


	Epoch 97
Training results:
gen_loss: 0.38160044
disc_loss: 2.8124468
disc_acc: 0.060148514851485146

Validation results:
gen_loss: 0.0025155959
disc_loss: 2.8548312
disc_acc: 0.06349206349206349


	Epoch 98
Training results:
gen_loss: 1.882481
disc_loss: 31.885897
disc_acc: 0.06683168316831684

Validation results:
gen_loss: 1.6796895
disc_loss: 3.720313
disc_acc: 0.06349206349206349


	Epoch 99
Training results:
gen_loss: 1.6078393
disc_loss: 3.4840634
disc_acc: 0.06633663366336634

Validation results:
gen_loss: 1.5867939
disc_loss: 3.1476629
disc_acc: 0.0679563492063492


	Epoch 100
Training results:
gen_loss: 1.5415454
disc_loss: 3.1694505
disc_acc: 0.06324257425742574

Validation results:
gen_loss: 1.566169
disc_loss: 2.9830225
disc_acc: 0.0679563492063492



gen_train_loss: 0.009248744, 2.2425835, 0.35459483, 0.19974166, 0.12544593, 0.11189163, 0.08410294, 0.07219532, 0.073812455, 0.07164022, 0.06344058, 0.052891545, 0.04750169, 0.040901285, 0.04402227, 0.041119628, 0.04433486, 0.042210296, 0.045170873, 0.03941899, 0.05104087, 0.04710485, 0.04038176, 0.04978251, 0.042490438, 0.046041135, 0.050109584, 0.03399691, 0.037785407, 0.03651297, 0.023227949, 0.023074323, 0.024852185, 0.021519335, 0.016943421, 0.015672741, 0.017284036, 0.015455573, 0.014100353, 0.013962696, 0.016687687, 0.016692935, 0.039203584, 0.033714645, 0.016632292, 0.009955165, 0.007848268, 0.008379213, 0.009523375, 0.009590175, 0.008167741, 0.008193065, 0.00808584, 0.008223118, 0.011031963, 0.00723785, 0.009751549, 0.00721051, 0.008872291, 0.00828784, 0.008762008, 0.005859589, 0.0068527497, 0.005069174, 0.005718644, 0.0059725223, 0.005574739, 0.0062633776, 0.00519576, 0.003570032, 0.00216356, 0.0021735013, 0.0021952328, 0.0021919813, 0.0020675873, 0.0020865283, 0.0018462762, 0.0026794039, 0.0008929193, 3.4419736e-05, 6.75893e-05, 0.00017132646, 3.0959327e-05, 0.00013928038, 4.3471075e-05, 8.965149e-05, 0.00010899581, 8.138226e-06, 5.1471056e-06, 0.00010804667, 1.0809635e-05, 9.63937e-06, 9.271966e-05, 9.138851e-05, 4.6700865e-05, 4.2816246e-05, 5.1043908e-06, 3.204947e-05, 0.0002291931, 1.6214515e-05, 0.00030895197, 0.010746637, 0.054508075, 0.047700148, 0.04558691, 0.044237804, 0.04354302, 0.04135521, 0.03846307, 0.03667456, 0.038033552, 0.039240856, 0.042776782, 0.04862053, 0.05037918, 0.047534164, 0.049575288, 0.051403653, 0.050651543, 0.05332021, 0.05538275, 0.058836304, 0.065137066, 0.07542318, 0.10136743, 0.120270416, 0.1427661, 0.16795123, 0.20055576, 0.2096354, 0.2514568, 0.28086266, 0.28318134, 0.32491636, 0.5884488, 0.40487304, 0.3520378, 0.3226901, 0.2793325, 0.22794503, 0.22569273, 0.18191162, 0.12029926, 0.12744649, 0.13144435, 0.14000928, 0.14497517, 0.15050773, 0.14670897, 0.16211258, 0.1689597, 0.17448023, 0.19238365, 0.21449047, 0.21632662, 0.22207259, 0.24994119, 0.2666189, 0.29704067, 0.34672824, 0.4948781, 0.40436026, 0.34134826, 1.3624984, 1.1767987, 1.0657629, 0.9101532, 0.8174163, 0.7509232, 0.7309934, 1.5434914, 1.5317256, 1.2390966, 1.1368033, 0.95604306, 0.7629704, 0.66344684, 0.6636919, 0.7674243, 0.737999, 0.73159385, 0.78544515, 1.017139, 1.111312, 0.9437933, 0.5957036, 0.59205925, 0.50990385, 0.47089013, 0.32331985, 0.19992083, 0.007170656, 0.006311321, 0.006456698, 0.26703075, 0.33682686, 0.36447686, 0.3446549, 0.38160044, 1.882481, 1.6078393, 1.5415454
disc_train_loss: 5.579209, 1.9122689, 2.5778868, 2.7039266, 2.7901683, 2.787774, 2.8164098, 2.7977104, 2.813191, 2.831098, 2.8138087, 2.8026328, 2.8071358, 2.803759, 2.805147, 2.8069324, 2.8002858, 2.8055115, 2.8087263, 2.8158374, 2.8408952, 2.8055344, 2.7963612, 2.8174083, 2.8103297, 2.8132613, 2.827089, 2.8018591, 2.8174288, 2.817975, 2.7988825, 2.8195117, 2.8514173, 2.799622, 2.7974231, 2.8091855, 2.8149447, 2.8131237, 2.8032, 2.8092158, 2.8171349, 2.8254166, 4.4409604, 2.8889499, 2.8174179, 2.7975068, 2.7879622, 2.7870634, 2.7859778, 2.7862058, 2.7931893, 2.7975655, 2.8033986, 2.8075154, 2.8121727, 2.8103485, 2.8229308, 2.8141282, 2.8396926, 2.8035326, 2.8115523, 2.8041158, 2.9935527, 2.7966468, 2.7885182, 2.7875338, 2.7974691, 2.813528, 2.8092904, 2.816674, 2.8100553, 2.8056784, 2.8749135, 2.7966487, 2.795686, 2.8056867, 2.821972, 2.8355904, 2.7979913, 2.7987928, 2.8144996, 2.8218586, 2.8011987, 2.8248374, 2.8064933, 2.8189042, 2.8359184, 2.7983568, 2.801988, 2.8298938, 2.8020809, 2.8072836, 2.8268633, 2.8120408, 2.8132725, 2.8037782, 2.8006408, 2.80902, 2.8218775, 2.8009539, 2.8249047, 14.817089, 9.325473, 3.0854073, 3.1135714, 3.0384722, 3.0213675, 2.9769132, 2.9400585, 2.90459, 2.902551, 2.9524193, 2.9312055, 2.920789, 2.9544806, 2.8822963, 2.8717732, 2.823984, 2.8315487, 2.8252034, 2.8162966, 2.801205, 2.8119566, 2.8516123, 2.804896, 2.8191073, 2.8578906, 2.7967136, 2.8301024, 2.7977235, 2.8316624, 2.8003943, 2.8361764, 2.8082254, 18.420275, 3.1400099, 2.952671, 2.9130962, 2.8819084, 2.8649943, 2.8378289, 2.8307903, 2.8163457, 2.8160157, 2.809246, 2.807167, 2.798963, 2.8002446, 2.798562, 2.7981772, 2.805358, 2.7956333, 2.7989326, 2.8190036, 2.818142, 2.8187873, 2.8139498, 2.8137844, 2.8247395, 2.8251426, 2.993102, 2.815782, 2.789554, 13.2327, 3.0388927, 2.9339592, 2.9054751, 2.8631473, 2.8411043, 2.9193826, 11.887538, 3.0691593, 2.9353304, 2.9002964, 2.862624, 2.8539405, 2.8287961, 2.815128, 2.8074102, 2.8051178, 2.813287, 2.8093364, 2.809977, 2.8076618, 8.265615, 2.9257965, 2.8539639, 2.8427474, 2.8163874, 2.8123064, 2.8028448, 2.7983437, 2.8011298, 2.7949507, 2.7995598, 2.795736, 2.8195724, 2.8223195, 2.8124468, 31.885897, 3.4840634, 3.1694505
disc_train_acc: 0.0, 0.6111386138613861, 0.25495049504950495, 0.17289603960396038, 0.1275990099009901, 0.11163366336633664, 0.08490099009900991, 0.07821782178217822, 0.07549504950495049, 0.06683168316831684, 0.07017326732673268, 0.06782178217821783, 0.06027227722772277, 0.0599009900990099, 0.06089108910891089, 0.06670792079207921, 0.06262376237623762, 0.06262376237623762, 0.06386138613861386, 0.05804455445544555, 0.06695544554455446, 0.059777227722772275, 0.062376237623762376, 0.06064356435643564, 0.06150990099009901, 0.06435643564356436, 0.06621287128712872, 0.06547029702970297, 0.05891089108910891, 0.06596534653465347, 0.06262376237623762, 0.05903465346534653, 0.061386138613861385, 0.06027227722772277, 0.0625, 0.062376237623762376, 0.06113861386138614, 0.06175742574257426, 0.06497524752475248, 0.06262376237623762, 0.0594059405940594, 0.06324257425742574, 0.06188118811881188, 0.059777227722772275, 0.057301980198019804, 0.06287128712871287, 0.06349009900990099, 0.06287128712871287, 0.06126237623762376, 0.06126237623762376, 0.05891089108910891, 0.062376237623762376, 0.06089108910891089, 0.059777227722772275, 0.06571782178217822, 0.057301980198019804, 0.06002475247524752, 0.057920792079207924, 0.06336633663366337, 0.06175742574257426, 0.06274752475247525, 0.061386138613861385, 0.06386138613861386, 0.0646039603960396, 0.05903465346534653, 0.0625, 0.06324257425742574, 0.05618811881188119, 0.06287128712871287, 0.0655940594059406, 0.06089108910891089, 0.06485148514851485, 0.057920792079207924, 0.062004950495049505, 0.06175742574257426, 0.06212871287128713, 0.06336633663366337, 0.062376237623762376, 0.057920792079207924, 0.06324257425742574, 0.06311881188118812, 0.06064356435643564, 0.0599009900990099, 0.05928217821782178, 0.058292079207920795, 0.05928217821782178, 0.06571782178217822, 0.06287128712871287, 0.0625, 0.06188118811881188, 0.06448019801980198, 0.06410891089108911, 0.06262376237623762, 0.06089108910891089, 0.06472772277227723, 0.0625, 0.061014851485148514, 0.0646039603960396, 0.06373762376237624, 0.0625, 0.06002475247524752, 0.0, 0.06324257425742574, 0.059158415841584155, 0.058292079207920795, 0.059158415841584155, 0.05891089108910891, 0.06522277227722773, 0.06212871287128713, 0.062004950495049505, 0.06188118811881188, 0.060396039603960394, 0.061386138613861385, 0.06423267326732673, 0.0599009900990099, 0.06596534653465347, 0.061014851485148514, 0.061014851485148514, 0.05804455445544555, 0.060396039603960394, 0.06175742574257426, 0.06349009900990099, 0.06027227722772277, 0.0676980198019802, 0.06089108910891089, 0.06720297029702971, 0.06212871287128713, 0.06163366336633663, 0.06448019801980198, 0.06113861386138614, 0.062376237623762376, 0.06225247524752475, 0.06423267326732673, 0.057920792079207924, 0.060148514851485146, 0.06596534653465347, 0.059777227722772275, 0.06695544554455446, 0.061014851485148514, 0.06621287128712872, 0.05866336633663366, 0.05952970297029703, 0.06175742574257426, 0.0629950495049505, 0.05804455445544555, 0.06089108910891089, 0.06262376237623762, 0.060396039603960394, 0.0625, 0.06175742574257426, 0.060148514851485146, 0.06126237623762376, 0.0625, 0.0650990099009901, 0.06311881188118812, 0.06967821782178218, 0.057673267326732676, 0.06150990099009901, 0.06596534653465347, 0.060148514851485146, 0.06287128712871287, 0.06398514851485149, 0.0655940594059406, 0.06571782178217822, 0.05754950495049505, 0.06064356435643564, 0.062004950495049505, 0.06398514851485149, 0.06386138613861386, 0.062004950495049505, 0.06794554455445545, 0.05952970297029703, 0.059158415841584155, 0.06126237623762376, 0.06027227722772277, 0.05891089108910891, 0.06472772277227723, 0.06002475247524752, 0.06386138613861386, 0.0625, 0.06646039603960396, 0.06126237623762376, 0.06423267326732673, 0.06423267326732673, 0.062376237623762376, 0.06410891089108911, 0.06547029702970297, 0.062376237623762376, 0.06163366336633663, 0.06311881188118812, 0.0594059405940594, 0.06027227722772277, 0.05928217821782178, 0.06002475247524752, 0.06150990099009901, 0.061014851485148514, 0.06448019801980198, 0.06683168316831684, 0.060148514851485146, 0.06683168316831684, 0.06633663366336634, 0.06324257425742574
gen_val_loss: 0.0092163645, 0.3896614, 0.22114153, 0.15424216, 0.114550725, 0.08683461, 0.07274034, 0.07487428, 0.07350679, 0.07931442, 0.048431594, 0.05861601, 0.06104495, 0.030725723, 0.06060456, 0.05617028, 0.056774363, 0.03137178, 0.034715615, 0.03608156, 0.062720805, 0.030580187, 0.043789636, 0.048681762, 0.06739676, 0.03679729, 0.047057837, 0.03592056, 0.03611055, 0.022303212, 0.014292554, 0.028833048, 0.027878612, 0.016798055, 0.012818543, 0.008661125, 0.014951781, 0.016458882, 0.011946362, 0.008710353, 0.011069138, 0.021257935, 0.031014387, 0.013363912, 0.011744313, 0.0047041248, 0.009269008, 0.0072302166, 0.017173609, 0.006159216, 0.010304404, 0.006090503, 0.005635741, 0.0059172227, 0.007094216, 0.0048055225, 0.0060451385, 0.00026214795, 0.0063414183, 0.0060780933, 0.00402507, 0.007598688, 0.003957878, 0.0037362091, 0.0118434355, 0.0044185515, 0.008189818, 0.008418563, 0.0066973916, 0.0015949487, 0.0038702593, 0.001964653, 0.0021622968, 0.0014568954, 0.0029622384, 0.0010098303, 0.0014121586, 0.0053133834, 2.2913331e-05, 4.20151e-05, 0.00028316843, 1.7048045e-05, 1.3031164e-05, 9.7000906e-05, 5.8729697e-06, 0.0017560913, 1.6310041e-05, 5.6921604e-06, 6.8530503e-06, 1.6579288e-05, 7.803263e-06, 3.0194476e-06, 0.00017416793, 0.00020635236, 0.00013258323, 6.418766e-06, 5.0235667e-06, 0.00030128943, 5.0638922e-05, 5.2376317e-06, 0.037377257, 0.010737705, 0.0472757, 0.047988575, 0.04760931, 0.047991514, 0.039362878, 0.04377867, 0.04317741, 0.03752191, 0.035613146, 0.044181895, 0.05072066, 0.04232222, 0.046178408, 0.046253566, 0.039542984, 0.028204167, 0.04135457, 0.036694832, 0.05311293, 0.05989036, 0.05493831, 0.08890067, 0.10346566, 0.04452513, 0.13960586, 0.1774175, 0.020175526, 0.22465664, 0.13379152, 0.16689764, 0.03712689, 0.0010127347, 0.453398, 0.3823108, 0.3455026, 0.31698886, 0.24455504, 0.22928025, 0.20541835, 0.142739, 0.1280574, 0.13054667, 0.22981855, 0.1442479, 0.15694879, 0.137473, 0.15721042, 0.1536292, 0.39851025, 0.00910818, 0.20642021, 0.5914414, 0.2262732, 0.37979108, 0.001721308, 0.0018058376, 0.002222106, 0.4071134, 0.0039420477, 0.36859947, 0.0013322717, 1.2695599, 1.1310086, 0.9836366, 0.8814198, 0.7817363, 0.7151384, 0.8820575, 1.5568666, 1.4673673, 1.1644578, 1.089458, 0.8429464, 0.7218386, 0.6358933, 0.7130174, 0.7252654, 0.7074315, 0.81106734, 0.82791656, 0.8119878, 2.0704482, 0.6061444, 0.55978554, 0.5797694, 0.5651048, 0.31301394, 0.31691465, 0.007094949, 0.006575424, 0.0048278095, 0.0046051494, 0.32034877, 0.57209677, 0.0032473062, 0.0014802122, 0.0025155959, 1.6796895, 1.5867939, 1.566169
disc_val_loss: 5.587276, 3.6711767, 2.467896, 2.768767, 2.7523253, 2.833389, 2.7435088, 2.8123589, 2.7835767, 2.8216686, 2.8119202, 2.794742, 2.8411021, 2.7889247, 2.7975445, 2.8041556, 2.7941215, 2.7999861, 2.8040457, 2.8292572, 2.7944047, 2.8065562, 2.8333116, 2.7998097, 2.80855, 2.834397, 2.7934208, 2.800112, 2.8676891, 2.7864618, 2.787936, 2.827436, 2.7876215, 2.8180096, 2.8877716, 2.7956455, 2.814525, 2.785554, 2.8155968, 2.8012962, 2.869416, 2.8356774, 3.2269838, 2.8343947, 2.7901726, 2.7926948, 2.7751825, 2.7936518, 2.7744312, 2.7967184, 2.7911105, 2.7819717, 2.7885067, 2.8009522, 2.8222055, 2.7835457, 2.812001, 2.852008, 2.7952144, 2.8392005, 2.8806663, 2.7945616, 2.8014743, 2.8007627, 2.778179, 2.7853692, 2.8559787, 2.817802, 2.7837672, 2.8004591, 2.9000065, 2.9167547, 2.780253, 2.7941034, 2.8447382, 2.8361464, 2.8677795, 2.802871, 2.8607645, 2.8188791, 2.8601043, 2.7883322, 2.7857194, 2.8131409, 2.8462055, 2.8817766, 2.7924783, 2.7870476, 2.810628, 2.779863, 2.8202112, 2.7934551, 2.8201458, 2.817635, 2.8019066, 2.7870631, 2.7808497, 2.8093781, 2.7897274, 2.7871637, 2.7979276, 14.95784, 3.0624688, 2.9152462, 3.0103312, 3.506817, 2.988149, 3.0994825, 2.8702135, 2.834206, 3.0137668, 2.9814749, 3.0097888, 2.9208944, 3.4097168, 2.9301162, 2.8343449, 2.8455336, 2.8184578, 2.8863964, 2.7972405, 2.7916877, 2.7988617, 2.7813706, 2.8063495, 2.9383664, 2.795234, 2.8129194, 2.8344357, 2.8010094, 2.8047109, 2.7951705, 2.8664281, 2.8062062, 4.204851, 2.9179282, 2.8467095, 2.848135, 2.8454316, 2.9323888, 2.8480337, 2.8290706, 2.838114, 2.8131104, 2.8080428, 2.8046088, 2.7833495, 2.8303185, 2.8253486, 2.8000224, 2.7867332, 2.808855, 2.7936738, 2.8003702, 2.853516, 2.8033078, 2.7976437, 2.8841608, 2.8125713, 2.7991896, 2.9138694, 2.7981012, 2.8092709, 2.973141, 3.0592515, 2.883973, 2.8213353, 2.8306692, 2.9041436, 2.9050758, 3.0794291, 3.0051095, 2.948839, 2.8503706, 3.1601892, 2.8232028, 2.788087, 2.8017766, 2.7977514, 2.8282006, 2.794541, 2.85862, 3.199717, 2.7948596, 2.9493997, 2.8331869, 2.806271, 2.787884, 2.8209307, 2.781371, 2.8143141, 2.7824755, 2.7992568, 2.7793634, 2.8083916, 2.7809432, 2.8820796, 2.8211508, 2.8548312, 3.720313, 3.1476629, 2.9830225
disc_val_acc: 0.0, 0.059027777777777776, 0.2976190476190476, 0.1597222222222222, 0.0798611111111111, 0.11706349206349206, 0.25049603174603174, 0.17261904761904762, 0.06398809523809523, 0.054563492063492064, 0.05853174603174603, 0.06349206349206349, 0.09821428571428571, 0.06349206349206349, 0.06101190476190476, 0.06349206349206349, 0.057539682539682536, 0.06101190476190476, 0.05505952380952381, 0.05853174603174603, 0.05406746031746032, 0.07093253968253968, 0.0625, 0.05853174603174603, 0.05704365079365079, 0.061507936507936505, 0.05704365079365079, 0.05257936507936508, 0.053075396825396824, 0.06349206349206349, 0.05505952380952381, 0.07390873015873016, 0.053075396825396824, 0.06299603174603174, 0.053075396825396824, 0.060515873015873016, 0.06398809523809523, 0.0689484126984127, 0.05853174603174603, 0.05853174603174603, 0.060515873015873016, 0.05853174603174603, 0.0689484126984127, 0.05853174603174603, 0.05704365079365079, 0.057539682539682536, 0.06349206349206349, 0.05853174603174603, 0.0679563492063492, 0.05853174603174603, 0.0625, 0.05853174603174603, 0.07093253968253968, 0.05853174603174603, 0.062003968253968256, 0.07093253968253968, 0.07093253968253968, 0.05853174603174603, 0.05853174603174603, 0.06299603174603174, 0.06001984126984127, 0.06398809523809523, 0.05803571428571429, 0.05257936507936508, 0.06398809523809523, 0.05853174603174603, 0.062003968253968256, 0.06398809523809523, 0.062003968253968256, 0.0689484126984127, 0.05704365079365079, 0.05704365079365079, 0.05853174603174603, 0.06349206349206349, 0.05853174603174603, 0.053075396825396824, 0.05803571428571429, 0.05853174603174603, 0.05853174603174603, 0.07341269841269842, 0.062003968253968256, 0.07291666666666667, 0.06398809523809523, 0.062003968253968256, 0.062003968253968256, 0.061507936507936505, 0.062003968253968256, 0.07341269841269842, 0.07390873015873016, 0.062003968253968256, 0.06398809523809523, 0.057539682539682536, 0.061507936507936505, 0.06398809523809523, 0.060515873015873016, 0.061507936507936505, 0.05853174603174603, 0.06349206349206349, 0.06746031746031746, 0.07390873015873016, 0.05505952380952381, 0.0, 0.05257936507936508, 0.0679563492063492, 0.0625, 0.05257936507936508, 0.07390873015873016, 0.07291666666666667, 0.07093253968253968, 0.07093253968253968, 0.05952380952380952, 0.06349206349206349, 0.06349206349206349, 0.062003968253968256, 0.07093253968253968, 0.057539682539682536, 0.060515873015873016, 0.06001984126984127, 0.057539682539682536, 0.0679563492063492, 0.05853174603174603, 0.06845238095238096, 0.0689484126984127, 0.0625, 0.05803571428571429, 0.05853174603174603, 0.0679563492063492, 0.05257936507936508, 0.06299603174603174, 0.05505952380952381, 0.053075396825396824, 0.062003968253968256, 0.062003968253968256, 0.053075396825396824, 0.0689484126984127, 0.06398809523809523, 0.052083333333333336, 0.06349206349206349, 0.05853174603174603, 0.06349206349206349, 0.05505952380952381, 0.06398809523809523, 0.054563492063492064, 0.05803571428571429, 0.05704365079365079, 0.06349206349206349, 0.0679563492063492, 0.06349206349206349, 0.0625, 0.0625, 0.06349206349206349, 0.05853174603174603, 0.07390873015873016, 0.06349206349206349, 0.05406746031746032, 0.05853174603174603, 0.061507936507936505, 0.053075396825396824, 0.06845238095238096, 0.06845238095238096, 0.06349206349206349, 0.057539682539682536, 0.05505952380952381, 0.06398809523809523, 0.05853174603174603, 0.06349206349206349, 0.05853174603174603, 0.054563492063492064, 0.05505952380952381, 0.05853174603174603, 0.07390873015873016, 0.061507936507936505, 0.054563492063492064, 0.05853174603174603, 0.05505952380952381, 0.0689484126984127, 0.057539682539682536, 0.057539682539682536, 0.05257936507936508, 0.05257936507936508, 0.07093253968253968, 0.060515873015873016, 0.0, 0.060515873015873016, 0.06398809523809523, 0.060515873015873016, 0.07093253968253968, 0.07093253968253968, 0.06398809523809523, 0.0689484126984127, 0.06845238095238096, 0.06349206349206349, 0.06101190476190476, 0.06349206349206349, 0.05505952380952381, 0.06845238095238096, 0.061507936507936505, 0.07390873015873016, 0.06349206349206349, 0.06349206349206349, 0.0679563492063492, 0.0679563492063492

