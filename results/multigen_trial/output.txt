Beginning trial described in ./config/multigen_trial.json.
Experiment type: multiple generators each corresponding to 1 key.
Experiment settings:
	byte: 0
	keys: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15]
	key_dataset_kwargs:
		keep_data_in_memory: True
		data_path: ./data
		download: True
		extract: True
		preprocess: True
		delete_download_after_extraction: False
		delete_extracted_after_preprocess: False
	dataloader_kwargs:
		batch_size: 16
		shuffle: True
	dataset_prop_for_validation: 0.2
	trace_map_constructor: None
	trace_map_kwargs:
		layers: [256]
		hidden_activation: <class 'torch.nn.modules.activation.ReLU'>
	plaintext_map_constructor: None
	plaintext_map_kwargs:
		layers: [64]
		hidden_activation: <class 'torch.nn.modules.activation.ReLU'>
	key_map_constructor: <function get_zero_map at 0x7fece56b03a0>
	key_map_kwargs:
	cumulative_map_constructor: None
	cumulative_map_kwargs:
		layers: [256]
		hidden_activation: <class 'torch.nn.modules.activation.ReLU'>
	discriminator_constructor: <function get_google_style_resnet_discriminator at 0x7fece2204790>
	discriminator_kwargs:
	discriminator_loss_constructor: <class 'torch.nn.modules.loss.CrossEntropyLoss'>
	discriminator_loss_kwargs:
	discriminator_optimizer_constructor: <class 'torch.optim.adam.Adam'>
	discriminator_optimizer_kwargs:
	generator_loss_constructor: <class 'loss_functions.BatchStdLoss'>
	generator_loss_kwargs:
	generator_optimizer_constructor: <class 'torch.optim.adam.Adam'>
	generator_optimizer_kwargs:
	device: cuda
	discriminator_pretraining_epochs: 0
	generator_pretraining_epochs: 0
	gan_training_epochs: 100
	discriminator_posttraining_epochs: 100
	seed: 0
Loading datasets.
AesKeyGroupDataset:
	Available keys: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15]
	Key transform: Compose(
    IntToBinary()
    ToTensor1D()
)
	Byte: 0
	Number of samples available: 10112
	Trace size: torch.Size([1, 3000])
	Key size: torch.Size([1, 8])
	Plaintext size: torch.Size([1, 8])
	Key index size: ()
Constructing generator.
KeyOnlyGenerator(
  (key_trace_map): Sequential(
    (0): Flatten(start_dim=1, end_dim=-1)
    (1): Linear(in_features=8, out_features=3000, bias=False)
    (2): Unflatten(dim=-1, unflattened_size=torch.Size([1, 3000]))
  )
)

Constructing discriminator.
Discriminator(
  (model): Sequential(
    (0): MaxPool1d(kernel_size=4, stride=4, padding=0, dilation=1, ceil_mode=False)
    (1): Stack(
      (F): Sequential(
        (0): Block(
          (F): Sequential(
            (0): BatchNorm1d(1, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (1): ReLU()
            (2): Conv1d(1, 16, kernel_size=(1,), stride=(1,), bias=False)
            (3): BatchNorm1d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (4): ReLU()
            (5): Conv1d(16, 16, kernel_size=(3,), stride=(1,), padding=(1,), bias=False)
            (6): BatchNorm1d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (7): ReLU()
            (8): Conv1d(16, 64, kernel_size=(1,), stride=(1,))
          )
          (Id): Conv1d(1, 64, kernel_size=(1,), stride=(1,))
        )
        (1): Block(
          (F): Sequential(
            (0): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (1): ReLU()
            (2): Conv1d(64, 16, kernel_size=(1,), stride=(1,), bias=False)
            (3): BatchNorm1d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (4): ReLU()
            (5): Conv1d(16, 16, kernel_size=(3,), stride=(1,), padding=(1,), bias=False)
            (6): BatchNorm1d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (7): ReLU()
            (8): Conv1d(16, 64, kernel_size=(1,), stride=(1,))
          )
          (Id): Identity()
        )
        (2): Block(
          (F): Sequential(
            (0): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (1): ReLU()
            (2): Conv1d(64, 16, kernel_size=(1,), stride=(1,), bias=False)
            (3): BatchNorm1d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (4): ReLU()
            (5): Conv1d(16, 16, kernel_size=(3,), stride=(2,), padding=(1,), bias=False)
            (6): BatchNorm1d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (7): ReLU()
            (8): Conv1d(16, 64, kernel_size=(1,), stride=(1,))
          )
          (Id): MaxPool1d(kernel_size=1, stride=2, padding=0, dilation=1, ceil_mode=False)
        )
      )
    )
    (2): Stack(
      (F): Sequential(
        (0): Block(
          (F): Sequential(
            (0): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (1): ReLU()
            (2): Conv1d(64, 32, kernel_size=(1,), stride=(1,), bias=False)
            (3): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (4): ReLU()
            (5): Conv1d(32, 32, kernel_size=(3,), stride=(1,), padding=(1,), bias=False)
            (6): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (7): ReLU()
            (8): Conv1d(32, 128, kernel_size=(1,), stride=(1,))
          )
          (Id): Conv1d(64, 128, kernel_size=(1,), stride=(1,))
        )
        (1): Block(
          (F): Sequential(
            (0): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (1): ReLU()
            (2): Conv1d(128, 32, kernel_size=(1,), stride=(1,), bias=False)
            (3): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (4): ReLU()
            (5): Conv1d(32, 32, kernel_size=(3,), stride=(1,), padding=(1,), bias=False)
            (6): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (7): ReLU()
            (8): Conv1d(32, 128, kernel_size=(1,), stride=(1,))
          )
          (Id): Identity()
        )
        (2): Block(
          (F): Sequential(
            (0): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (1): ReLU()
            (2): Conv1d(128, 32, kernel_size=(1,), stride=(1,), bias=False)
            (3): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (4): ReLU()
            (5): Conv1d(32, 32, kernel_size=(3,), stride=(1,), padding=(1,), bias=False)
            (6): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (7): ReLU()
            (8): Conv1d(32, 128, kernel_size=(1,), stride=(1,))
          )
          (Id): Identity()
        )
        (3): Block(
          (F): Sequential(
            (0): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (1): ReLU()
            (2): Conv1d(128, 32, kernel_size=(1,), stride=(1,), bias=False)
            (3): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (4): ReLU()
            (5): Conv1d(32, 32, kernel_size=(3,), stride=(2,), padding=(1,), bias=False)
            (6): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (7): ReLU()
            (8): Conv1d(32, 128, kernel_size=(1,), stride=(1,))
          )
          (Id): MaxPool1d(kernel_size=1, stride=2, padding=0, dilation=1, ceil_mode=False)
        )
      )
    )
    (3): Stack(
      (F): Sequential(
        (0): Block(
          (F): Sequential(
            (0): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (1): ReLU()
            (2): Conv1d(128, 64, kernel_size=(1,), stride=(1,), bias=False)
            (3): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (4): ReLU()
            (5): Conv1d(64, 64, kernel_size=(3,), stride=(1,), padding=(1,), bias=False)
            (6): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (7): ReLU()
            (8): Conv1d(64, 256, kernel_size=(1,), stride=(1,))
          )
          (Id): Conv1d(128, 256, kernel_size=(1,), stride=(1,))
        )
        (1): Block(
          (F): Sequential(
            (0): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (1): ReLU()
            (2): Conv1d(256, 64, kernel_size=(1,), stride=(1,), bias=False)
            (3): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (4): ReLU()
            (5): Conv1d(64, 64, kernel_size=(3,), stride=(1,), padding=(1,), bias=False)
            (6): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (7): ReLU()
            (8): Conv1d(64, 256, kernel_size=(1,), stride=(1,))
          )
          (Id): Identity()
        )
        (2): Block(
          (F): Sequential(
            (0): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (1): ReLU()
            (2): Conv1d(256, 64, kernel_size=(1,), stride=(1,), bias=False)
            (3): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (4): ReLU()
            (5): Conv1d(64, 64, kernel_size=(3,), stride=(1,), padding=(1,), bias=False)
            (6): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (7): ReLU()
            (8): Conv1d(64, 256, kernel_size=(1,), stride=(1,))
          )
          (Id): Identity()
        )
        (3): Block(
          (F): Sequential(
            (0): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (1): ReLU()
            (2): Conv1d(256, 64, kernel_size=(1,), stride=(1,), bias=False)
            (3): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (4): ReLU()
            (5): Conv1d(64, 64, kernel_size=(3,), stride=(2,), padding=(1,), bias=False)
            (6): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (7): ReLU()
            (8): Conv1d(64, 256, kernel_size=(1,), stride=(1,))
          )
          (Id): MaxPool1d(kernel_size=1, stride=2, padding=0, dilation=1, ceil_mode=False)
        )
      )
    )
    (4): Stack(
      (F): Sequential(
        (0): Block(
          (F): Sequential(
            (0): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (1): ReLU()
            (2): Conv1d(256, 128, kernel_size=(1,), stride=(1,), bias=False)
            (3): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (4): ReLU()
            (5): Conv1d(128, 128, kernel_size=(3,), stride=(1,), padding=(1,), bias=False)
            (6): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (7): ReLU()
            (8): Conv1d(128, 512, kernel_size=(1,), stride=(1,))
          )
          (Id): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
        )
        (1): Block(
          (F): Sequential(
            (0): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (1): ReLU()
            (2): Conv1d(512, 128, kernel_size=(1,), stride=(1,), bias=False)
            (3): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (4): ReLU()
            (5): Conv1d(128, 128, kernel_size=(3,), stride=(1,), padding=(1,), bias=False)
            (6): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (7): ReLU()
            (8): Conv1d(128, 512, kernel_size=(1,), stride=(1,))
          )
          (Id): Identity()
        )
        (2): Block(
          (F): Sequential(
            (0): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (1): ReLU()
            (2): Conv1d(512, 128, kernel_size=(1,), stride=(1,), bias=False)
            (3): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (4): ReLU()
            (5): Conv1d(128, 128, kernel_size=(3,), stride=(2,), padding=(1,), bias=False)
            (6): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (7): ReLU()
            (8): Conv1d(128, 512, kernel_size=(1,), stride=(1,))
          )
          (Id): MaxPool1d(kernel_size=1, stride=2, padding=0, dilation=1, ceil_mode=False)
        )
      )
    )
    (5): Flatten(start_dim=1, end_dim=-1)
    (6): LazyLinear(in_features=0, out_features=256, bias=True)
  )
)

Calculating initial results.
Training results:
gen_loss: 1.285839
disc_loss: 5.5640526
disc_acc: 0.021905940594059405

Validation results:
gen_loss: 1.2870709
disc_loss: 5.5621567
disc_acc: 0.020833333333333332


Training discriminator and generator simultaneously.
	Epoch 1
Training results:
gen_loss: 5.056077
disc_loss: 0.87767285
disc_acc: 0.8409653465346535

Validation results:
gen_loss: 8.240648
disc_loss: 0.262876
disc_acc: 0.9345238095238095


	Epoch 2
Training results:
gen_loss: 5.019606
disc_loss: 0.53974223
disc_acc: 0.9034653465346535

Validation results:
gen_loss: 4.372683
disc_loss: 1.1997291
disc_acc: 0.8501984126984127


	Epoch 3
Training results:
gen_loss: 4.3744416
disc_loss: 0.45640528
disc_acc: 0.9153465346534654

Validation results:
gen_loss: 4.0846605
disc_loss: 0.104325615
disc_acc: 0.9627976190476191


	Epoch 4
Training results:
gen_loss: 4.162913
disc_loss: 0.5823086
disc_acc: 0.8915841584158416

Validation results:
gen_loss: 3.8122888
disc_loss: 2.1802294
disc_acc: 0.7053571428571429


	Epoch 5
Training results:
gen_loss: 3.8578732
disc_loss: 0.610605
disc_acc: 0.8746287128712872

Validation results:
gen_loss: 3.5253198
disc_loss: 0.24002603
disc_acc: 0.9151785714285714


	Epoch 6
Training results:
gen_loss: 4.063985
disc_loss: 0.5690732
disc_acc: 0.8696782178217822

Validation results:
gen_loss: 3.220265
disc_loss: 0.22026117
disc_acc: 0.9231150793650794


	Epoch 7
Training results:
gen_loss: 3.5620925
disc_loss: 0.5243742
disc_acc: 0.8771039603960396

Validation results:
gen_loss: 2.7333207
disc_loss: 0.2928956
disc_acc: 0.8874007936507936


	Epoch 8
Training results:
gen_loss: 4.6883516
disc_loss: 0.7508123
disc_acc: 0.8393564356435643

Validation results:
gen_loss: 4.2532816
disc_loss: 1.4817857
disc_acc: 0.7906746031746031


	Epoch 9
Training results:
gen_loss: 4.1417603
disc_loss: 0.5893844
disc_acc: 0.866460396039604

Validation results:
gen_loss: 4.0895853
disc_loss: 1.1738718
disc_acc: 0.7390873015873016


	Epoch 10
Training results:
gen_loss: 3.409003
disc_loss: 0.606887
disc_acc: 0.843440594059406

Validation results:
gen_loss: 3.0910194
disc_loss: 0.22754957
disc_acc: 0.9211309523809523


	Epoch 11
Training results:
gen_loss: 3.9408467
disc_loss: 0.57117814
disc_acc: 0.8560643564356436

Validation results:
gen_loss: 2.8330603
disc_loss: 0.40466973
disc_acc: 0.9057539682539683


	Epoch 12
Training results:
gen_loss: 3.4184995
disc_loss: 0.42487243
disc_acc: 0.8872524752475247

Validation results:
gen_loss: 3.0990872
disc_loss: 0.4108682
disc_acc: 0.8973214285714286


	Epoch 13
Training results:
gen_loss: 3.2485023
disc_loss: 0.29704112
disc_acc: 0.9172029702970297

Validation results:
gen_loss: 4.70735
disc_loss: 0.2523271
disc_acc: 0.9131944444444444


	Epoch 14
Training results:
gen_loss: 4.33203
disc_loss: 0.3870471
disc_acc: 0.9094059405940594

Validation results:
gen_loss: 3.838601
disc_loss: 0.060757622
disc_acc: 0.9900793650793651


	Epoch 15
Training results:
gen_loss: 5.9272046
disc_loss: 0.36720243
disc_acc: 0.9256188118811881

Validation results:
gen_loss: 12.3511715
disc_loss: 0.13584292
disc_acc: 0.9637896825396826


	Epoch 16
Training results:
gen_loss: 6.1752253
disc_loss: 0.41219807
disc_acc: 0.9090346534653465

Validation results:
gen_loss: 7.1332016
disc_loss: 0.6454617
disc_acc: 0.8358134920634921


	Epoch 17
Training results:
gen_loss: 6.2944336
disc_loss: 0.51442873
disc_acc: 0.8998762376237623

Validation results:
gen_loss: 11.849588
disc_loss: 0.38771847
disc_acc: 0.8556547619047619


	Epoch 18
Training results:
gen_loss: 6.00792
disc_loss: 0.5811018
disc_acc: 0.8662128712871288

Validation results:
gen_loss: 7.2739353
disc_loss: 0.1382884
disc_acc: 0.9345238095238095


	Epoch 19
Training results:
gen_loss: 6.6669283
disc_loss: 0.42441475
disc_acc: 0.9064356435643565

Validation results:
gen_loss: 9.210997
disc_loss: 1.1860869
disc_acc: 0.7638888888888888


	Epoch 20
Training results:
gen_loss: 9.531699
disc_loss: 1.0791914
disc_acc: 0.8482673267326732

Validation results:
gen_loss: 9.502517
disc_loss: 3.9267485
disc_acc: 0.6899801587301587


	Epoch 21
Training results:
gen_loss: 4.877143
disc_loss: 0.24552903
disc_acc: 0.9469059405940594

Validation results:
gen_loss: 4.1291823
disc_loss: 0.16070932
disc_acc: 0.9469246031746031


	Epoch 22
Training results:
gen_loss: 7.6661825
disc_loss: 0.4073865
disc_acc: 0.9172029702970297

Validation results:
gen_loss: 6.601918
disc_loss: 0.66876405
disc_acc: 0.7688492063492064


	Epoch 23
Training results:
gen_loss: 6.6606717
disc_loss: 0.382742
disc_acc: 0.9142326732673267

Validation results:
gen_loss: 10.551357
disc_loss: 0.04778046
disc_acc: 0.9880952380952381


	Epoch 24
Training results:
gen_loss: 8.531816
disc_loss: 0.40033013
disc_acc: 0.8988861386138614

Validation results:
gen_loss: 7.8031573
disc_loss: 0.27451995
disc_acc: 0.9047619047619048


	Epoch 25
Training results:
gen_loss: 6.877006
disc_loss: 0.34748682
disc_acc: 0.9106435643564357

Validation results:
gen_loss: 5.514217
disc_loss: 0.04712656
disc_acc: 0.9935515873015873


	Epoch 26
Training results:
gen_loss: 7.2837744
disc_loss: 0.4198062
disc_acc: 0.9087871287128713

Validation results:
gen_loss: 10.197987
disc_loss: 0.05041362
disc_acc: 0.9915674603174603


	Epoch 27
Training results:
gen_loss: 7.7173934
disc_loss: 0.28695548
disc_acc: 0.9308168316831683

Validation results:
gen_loss: 5.4255366
disc_loss: 0.093293
disc_acc: 0.9781746031746031


	Epoch 28
Training results:
gen_loss: 8.796293
disc_loss: 0.41218683
disc_acc: 0.8925742574257426

Validation results:
gen_loss: 8.640185
disc_loss: 0.16496669
disc_acc: 0.9608134920634921


	Epoch 29
Training results:
gen_loss: 8.099369
disc_loss: 0.35919395
disc_acc: 0.9153465346534654

Validation results:
gen_loss: 13.376267
disc_loss: 0.15400472
disc_acc: 0.9489087301587301


	Epoch 30
Training results:
gen_loss: 8.287974
disc_loss: 0.42635885
disc_acc: 0.8888613861386139

Validation results:
gen_loss: 6.905038
disc_loss: 0.39986762
disc_acc: 0.8566468253968254


	Epoch 31
Training results:
gen_loss: 5.5704904
disc_loss: 0.47957417
disc_acc: 0.8726485148514852

Validation results:
gen_loss: 4.2749166
disc_loss: 0.15844542
disc_acc: 0.9404761904761905


	Epoch 32
Training results:
gen_loss: 4.3622766
disc_loss: 0.21090302
disc_acc: 0.9372524752475248

Validation results:
gen_loss: 4.025615
disc_loss: 0.0095009245
disc_acc: 0.9970238095238095


	Epoch 33
Training results:
gen_loss: 6.1660113
disc_loss: 0.30417645
disc_acc: 0.9243811881188119

Validation results:
gen_loss: 10.05686
disc_loss: 0.3359079
disc_acc: 0.8893849206349206


	Epoch 34
Training results:
gen_loss: 6.203637
disc_loss: 0.31615058
disc_acc: 0.9216584158415841

Validation results:
gen_loss: 4.720476
disc_loss: 0.6368527
disc_acc: 0.9007936507936508


	Epoch 35
Training results:
gen_loss: 4.5658154
disc_loss: 0.23483625
disc_acc: 0.9444306930693069

Validation results:
gen_loss: 3.4380727
disc_loss: 0.033761237
disc_acc: 0.9910714285714286


	Epoch 36
Training results:
gen_loss: 6.356356
disc_loss: 0.38485596
disc_acc: 0.9103960396039604

Validation results:
gen_loss: 5.540535
disc_loss: 0.03719086
disc_acc: 0.9910714285714286


	Epoch 37
Training results:
gen_loss: 5.5776696
disc_loss: 0.1446078
disc_acc: 0.9648514851485148

Validation results:
gen_loss: 4.790269
disc_loss: 0.028303662
disc_acc: 0.9940476190476191


	Epoch 38
Training results:
gen_loss: 5.3928156
disc_loss: 0.2061916
disc_acc: 0.9574257425742574

Validation results:
gen_loss: 8.100107
disc_loss: 2.4000707
disc_acc: 0.8323412698412699


	Epoch 39
Training results:
gen_loss: 7.512904
disc_loss: 0.25453836
disc_acc: 0.9449257425742574

Validation results:
gen_loss: 6.515589
disc_loss: 0.07690119
disc_acc: 0.9846230158730159


	Epoch 40
Training results:
gen_loss: 4.8056517
disc_loss: 0.17931317
disc_acc: 0.9622524752475248

Validation results:
gen_loss: 4.285686
disc_loss: 0.13646156
disc_acc: 0.9543650793650794


	Epoch 41
Training results:
gen_loss: 4.83422
disc_loss: 0.11164917
disc_acc: 0.9717821782178218

Validation results:
gen_loss: 6.0246854
disc_loss: 0.052970577
disc_acc: 0.9866071428571429


	Epoch 42
Training results:
gen_loss: 6.47407
disc_loss: 0.22811793
disc_acc: 0.9558168316831683

Validation results:
gen_loss: 5.1717715
disc_loss: 0.006441355
disc_acc: 0.9975198412698413


	Epoch 43
Training results:
gen_loss: 11.000604
disc_loss: 0.49205175
disc_acc: 0.9341584158415842

Validation results:
gen_loss: 12.3024645
disc_loss: 0.027389038
disc_acc: 0.9950396825396826


	Epoch 44
Training results:
gen_loss: 10.4456835
disc_loss: 0.3163415
disc_acc: 0.9371287128712872

Validation results:
gen_loss: 7.375338
disc_loss: 0.09665168
disc_acc: 0.96875


	Epoch 45
Training results:
gen_loss: 5.989551
disc_loss: 0.16763611
disc_acc: 0.9618811881188118

Validation results:
gen_loss: 6.2105637
disc_loss: 0.1852465
disc_acc: 0.9360119047619048


	Epoch 46
Training results:
gen_loss: 6.4070225
disc_loss: 0.18139835
disc_acc: 0.9561881188118811

Validation results:
gen_loss: 5.598792
disc_loss: 0.32435486
disc_acc: 0.8983134920634921


	Epoch 47
Training results:
gen_loss: 6.645083
disc_loss: 0.26885036
disc_acc: 0.9518564356435644

Validation results:
gen_loss: 13.226751
disc_loss: 0.13000277
disc_acc: 0.9697420634920635


	Epoch 48
Training results:
gen_loss: 10.304243
disc_loss: 0.222277
disc_acc: 0.9544554455445544

Validation results:
gen_loss: 8.965342
disc_loss: 0.67939603
disc_acc: 0.8209325396825397


	Epoch 49
Training results:
gen_loss: 9.756299
disc_loss: 0.31645963
disc_acc: 0.9471534653465347

Validation results:
gen_loss: 6.140306
disc_loss: 0.051622126
disc_acc: 0.9821428571428571


	Epoch 50
Training results:
gen_loss: 8.156911
disc_loss: 0.1574588
disc_acc: 0.9678217821782178

Validation results:
gen_loss: 9.054442
disc_loss: 0.0003354703
disc_acc: 1.0


	Epoch 51
Training results:
gen_loss: 9.044376
disc_loss: 0.1742876
disc_acc: 0.9724009900990099

Validation results:
gen_loss: 9.256833
disc_loss: 0.25587693
disc_acc: 0.9573412698412699


	Epoch 52
Training results:
gen_loss: 14.997554
disc_loss: 0.35927767
disc_acc: 0.9545792079207921

Validation results:
gen_loss: 18.492136
disc_loss: 0.99478656
disc_acc: 0.8363095238095238


	Epoch 53
Training results:
gen_loss: 15.970669
disc_loss: 0.29647163
disc_acc: 0.9590346534653466

Validation results:
gen_loss: 15.075006
disc_loss: 0.043551065
disc_acc: 0.9890873015873016


	Epoch 54
Training results:
gen_loss: 12.635454
disc_loss: 0.23982018
disc_acc: 0.9643564356435643

Validation results:
gen_loss: 13.89381
disc_loss: 0.14756598
disc_acc: 0.9836309523809523


	Epoch 55
Training results:
gen_loss: 17.440363
disc_loss: 0.1666713
disc_acc: 0.9754950495049505

Validation results:
gen_loss: 13.819395
disc_loss: 0.04027236
disc_acc: 0.9945436507936508


	Epoch 56
Training results:
gen_loss: 18.226967
disc_loss: 0.26476023
disc_acc: 0.9698019801980198

Validation results:
gen_loss: 15.369975
disc_loss: 0.06112576
disc_acc: 0.9836309523809523


	Epoch 57
Training results:
gen_loss: 14.378752
disc_loss: 0.20193955
disc_acc: 0.9706683168316832

Validation results:
gen_loss: 21.574066
disc_loss: 0.053434532
disc_acc: 0.9930555555555556


	Epoch 58
Training results:
gen_loss: 17.018015
disc_loss: 0.38410118
disc_acc: 0.9537128712871287

Validation results:
gen_loss: 14.417939
disc_loss: 0.072660245
disc_acc: 0.9821428571428571


	Epoch 59
Training results:
gen_loss: 12.87872
disc_loss: 0.24318464
disc_acc: 0.9696782178217822

Validation results:
gen_loss: 14.670578
disc_loss: 0.026648983
disc_acc: 0.9920634920634921


	Epoch 60
Training results:
gen_loss: 12.644156
disc_loss: 0.10424487
disc_acc: 0.9801980198019802

Validation results:
gen_loss: 10.605987
disc_loss: 0.00898385
disc_acc: 0.9965277777777778


	Epoch 61
Training results:
gen_loss: 13.140215
disc_loss: 0.15452906
disc_acc: 0.9725247524752475

Validation results:
gen_loss: 13.749527
disc_loss: 0.08530891
disc_acc: 0.9846230158730159


	Epoch 62
Training results:
gen_loss: 17.583115
disc_loss: 0.3137935
disc_acc: 0.9653465346534653

Validation results:
gen_loss: 18.470451
disc_loss: 0.19722234
disc_acc: 0.9791666666666666


	Epoch 63
Training results:
gen_loss: 18.37967
disc_loss: 0.2697049
disc_acc: 0.9711633663366337

Validation results:
gen_loss: 15.468356
disc_loss: 1.7440108
disc_acc: 0.8601190476190477


	Epoch 64
Training results:
gen_loss: 23.659454
disc_loss: 0.35147592
disc_acc: 0.9633663366336633

Validation results:
gen_loss: 20.586031
disc_loss: 0.04983022
disc_acc: 0.9885912698412699


	Epoch 65
Training results:
gen_loss: 23.96459
disc_loss: 0.43324262
disc_acc: 0.9622524752475248

Validation results:
gen_loss: 22.24628
disc_loss: 0.032438204
disc_acc: 0.9970238095238095


	Epoch 66
Training results:
gen_loss: 20.956429
disc_loss: 0.17361365
disc_acc: 0.9810643564356436

Validation results:
gen_loss: 20.965181
disc_loss: 0.32406968
disc_acc: 0.9399801587301587


	Epoch 67
Training results:
gen_loss: 20.307343
disc_loss: 0.16318698
disc_acc: 0.9766089108910891

Validation results:
gen_loss: 22.660625
disc_loss: 0.31707126
disc_acc: 0.9672619047619048


	Epoch 68
Training results:
gen_loss: 23.571684
disc_loss: 0.30639026
disc_acc: 0.9715346534653465

Validation results:
gen_loss: 27.583904
disc_loss: 0.21629347
disc_acc: 0.9846230158730159


	Epoch 69
Training results:
gen_loss: 26.114153
disc_loss: 0.36720598
disc_acc: 0.968440594059406

Validation results:
gen_loss: 28.652206
disc_loss: 0.3765773
disc_acc: 0.9484126984126984


	Epoch 70
Training results:
gen_loss: 25.635288
disc_loss: 0.3249038
disc_acc: 0.9736386138613862

Validation results:
gen_loss: 26.333437
disc_loss: 1.2514927
disc_acc: 0.9370039682539683


	Epoch 71
Training results:
gen_loss: 26.48095
disc_loss: 0.3777134
disc_acc: 0.9716584158415842

Validation results:
gen_loss: 32.47713
disc_loss: 0.13416582
disc_acc: 0.9871031746031746


	Epoch 72
Training results:
gen_loss: 28.21355
disc_loss: 0.53410214
disc_acc: 0.9681930693069307

Validation results:
gen_loss: 28.764174
disc_loss: 1.1334208
disc_acc: 0.8898809523809523


	Epoch 73
Training results:
gen_loss: 26.050877
disc_loss: 0.29446462
disc_acc: 0.9768564356435644

Validation results:
gen_loss: 27.513126
disc_loss: 0.234897
disc_acc: 0.9776785714285714


	Epoch 74
Training results:
gen_loss: 29.682106
disc_loss: 0.5297153
disc_acc: 0.9655940594059406

Validation results:
gen_loss: 30.14444
disc_loss: 0.07003876
disc_acc: 0.9880952380952381


	Epoch 75
Training results:
gen_loss: 26.173248
disc_loss: 0.26430428
disc_acc: 0.9771039603960396

Validation results:
gen_loss: 24.295834
disc_loss: 0.8906894
disc_acc: 0.9191468253968254


	Epoch 76
Training results:
gen_loss: 30.11719
disc_loss: 0.44096407
disc_acc: 0.9675742574257425

Validation results:
gen_loss: 33.402042
disc_loss: 0.024411066
disc_acc: 0.9985119047619048


	Epoch 77
Training results:
gen_loss: 29.910336
disc_loss: 0.27340087
disc_acc: 0.9790841584158416

Validation results:
gen_loss: 27.980621
disc_loss: 0.047255352
disc_acc: 0.9945436507936508


	Epoch 78
Training results:
gen_loss: 34.285725
disc_loss: 0.8082173
disc_acc: 0.9639851485148515

Validation results:
gen_loss: 39.251286
disc_loss: 0.13689627
disc_acc: 0.9856150793650794


	Epoch 79
Training results:
gen_loss: 32.698864
disc_loss: 0.24313578
disc_acc: 0.9811881188118812

Validation results:
gen_loss: 33.96579
disc_loss: 0.11450162
disc_acc: 0.9930555555555556


	Epoch 80
Training results:
gen_loss: 30.532549
disc_loss: 0.43306303
disc_acc: 0.9773514851485149

Validation results:
gen_loss: 36.107887
disc_loss: 2.7867215
disc_acc: 0.9231150793650794


	Epoch 81
Training results:
gen_loss: 37.206562
disc_loss: 0.4646273
disc_acc: 0.9773514851485149

Validation results:
gen_loss: 41.014416
disc_loss: 0.24038066
disc_acc: 0.9712301587301587


	Epoch 82
Training results:
gen_loss: 39.865585
disc_loss: 0.49518016
disc_acc: 0.9748762376237624

Validation results:
gen_loss: 57.100384
disc_loss: 0.20139642
disc_acc: 0.9895833333333334


	Epoch 83
Training results:
gen_loss: 51.26599
disc_loss: 1.0661196
disc_acc: 0.9617574257425743

Validation results:
gen_loss: 43.384487
disc_loss: 0.0
disc_acc: 1.0


	Epoch 84
Training results:
gen_loss: 38.966095
disc_loss: 0.40972924
disc_acc: 0.9789603960396039

Validation results:
gen_loss: 38.921864
disc_loss: 9.025896
disc_acc: 0.9196428571428571


	Epoch 85
Training results:
gen_loss: 35.820343
disc_loss: 0.6200315
disc_acc: 0.9701732673267327

Validation results:
gen_loss: 37.724575
disc_loss: 0.8236335
disc_acc: 0.9489087301587301


	Epoch 86
Training results:
gen_loss: 35.409214
disc_loss: 0.5813249
disc_acc: 0.9731435643564357

Validation results:
gen_loss: 31.011396
disc_loss: 0.14491574
disc_acc: 0.9970238095238095


	Epoch 87
Training results:
gen_loss: 34.995728
disc_loss: 0.42737973
disc_acc: 0.9756188118811882

Validation results:
gen_loss: 36.863987
disc_loss: 1.2653328
disc_acc: 0.9548611111111112


	Epoch 88
Training results:
gen_loss: 46.504898
disc_loss: 0.90782523
disc_acc: 0.9646039603960396

Validation results:
gen_loss: 43.61134
disc_loss: 0.16415735
disc_acc: 0.9875992063492064


	Epoch 89
Training results:
gen_loss: 38.032253
disc_loss: 0.34962568
disc_acc: 0.9793316831683169

Validation results:
gen_loss: 42.154957
disc_loss: 0.37098405
disc_acc: 0.972718253968254


	Epoch 90
Training results:
gen_loss: 41.245358
disc_loss: 0.36381117
disc_acc: 0.9799504950495049

Validation results:
gen_loss: 44.03222
disc_loss: 0.21966533
disc_acc: 0.9776785714285714


	Epoch 91
Training results:
gen_loss: 45.42756
disc_loss: 0.5082735
disc_acc: 0.9752475247524752

Validation results:
gen_loss: 52.500336
disc_loss: 0.14709532
disc_acc: 0.9950396825396826


	Epoch 92
Training results:
gen_loss: 49.72987
disc_loss: 0.84964204
disc_acc: 0.9691831683168317

Validation results:
gen_loss: 54.532425
disc_loss: 0.021872144
disc_acc: 0.998015873015873


	Epoch 93
Training results:
gen_loss: 46.750095
disc_loss: 0.6714183
disc_acc: 0.9745049504950495

Validation results:
gen_loss: 47.703644
disc_loss: 0.1452909
disc_acc: 0.9890873015873016


	Epoch 94
Training results:
gen_loss: 54.072758
disc_loss: 0.67365927
disc_acc: 0.9714108910891089

Validation results:
gen_loss: 52.564117
disc_loss: 0.16931535
disc_acc: 0.9930555555555556


	Epoch 95
Training results:
gen_loss: 48.525566
disc_loss: 0.67272043
disc_acc: 0.9712871287128713

Validation results:
gen_loss: 48.84207
disc_loss: 0.43622196
disc_acc: 0.9920634920634921


	Epoch 96
Training results:
gen_loss: 49.5863
disc_loss: 0.7527211
disc_acc: 0.974009900990099

Validation results:
gen_loss: 50.900146
disc_loss: 0.13793321
disc_acc: 0.9871031746031746


	Epoch 97
Training results:
gen_loss: 55.206463
disc_loss: 0.8543534
disc_acc: 0.9681930693069307

Validation results:
gen_loss: 56.051224
disc_loss: 0.04821656
disc_acc: 0.9965277777777778


	Epoch 98
Training results:
gen_loss: 47.019337
disc_loss: 0.76070917
disc_acc: 0.9691831683168317

Validation results:
gen_loss: 50.276608
disc_loss: 0.30822974
disc_acc: 0.9771825396825397


	Epoch 99
Training results:
gen_loss: 44.489666
disc_loss: 0.7830089
disc_acc: 0.9701732673267327

Validation results:
gen_loss: 44.72526
disc_loss: 0.06639581
disc_acc: 0.9940476190476191


	Epoch 100
Training results:
gen_loss: 39.727917
disc_loss: 0.6473791
disc_acc: 0.9728960396039604

Validation results:
gen_loss: 37.498344
disc_loss: 0.3099421
disc_acc: 0.9756944444444444



Training new discriminator on static trained discriminator.
	Initial performance
Training results:
gen_loss: 1.1286682
disc_loss: 5.6058893
disc_acc: 0.0

Validation results:
gen_loss: 1.1419588
disc_loss: 5.611858
disc_acc: 0.0


	Epoch 1
Training results:
gen_loss: 9.862069
disc_loss: 0.2938418
disc_acc: 0.9506188118811881

Validation results:
gen_loss: 13.098334
disc_loss: 0.14236474
disc_acc: 0.9737103174603174


	Epoch 2
Training results:
gen_loss: 18.755692
disc_loss: 0.2549272
disc_acc: 0.9853960396039604

Validation results:
gen_loss: 25.617031
disc_loss: 0.0012615721
disc_acc: 0.9995039682539683


	Epoch 3
Training results:
gen_loss: 21.504982
disc_loss: 0.0043687345
disc_acc: 0.9993811881188119

Validation results:
gen_loss: 34.76393
disc_loss: 5.3218154e-09
disc_acc: 1.0


	Epoch 4
Training results:
gen_loss: 82.22736
disc_loss: 1.1901512
disc_acc: 0.9842821782178218

Validation results:
gen_loss: 95.81329
disc_loss: 0.40260553
disc_acc: 0.9975198412698413


	Epoch 5
Training results:
gen_loss: 90.36512
disc_loss: 0.03536109
disc_acc: 0.9991336633663367

Validation results:
gen_loss: 96.01664
disc_loss: 0.0
disc_acc: 1.0


	Epoch 6
Training results:
gen_loss: 88.45823
disc_loss: 0.3219612
disc_acc: 0.9965346534653465

Validation results:
gen_loss: 96.39886
disc_loss: 0.0
disc_acc: 1.0


	Epoch 7
Training results:
gen_loss: 124.44633
disc_loss: 0.049420454
disc_acc: 0.999009900990099

Validation results:
gen_loss: 129.56516
disc_loss: 0.0
disc_acc: 1.0


	Epoch 8
Training results:
gen_loss: 136.74638
disc_loss: 0.6070976
disc_acc: 0.9955445544554455

Validation results:
gen_loss: 236.15463
disc_loss: 2.4212584
disc_acc: 0.9965277777777778


	Epoch 9
Training results:
gen_loss: 199.64174
disc_loss: 1.0753752
disc_acc: 0.9951732673267327

Validation results:
gen_loss: 169.35922
disc_loss: 0.027068093
disc_acc: 0.9990079365079365


	Epoch 10
Training results:
gen_loss: 156.82318
disc_loss: 0.054646395
disc_acc: 0.9996287128712872

Validation results:
gen_loss: 177.26242
disc_loss: 1.0075e-07
disc_acc: 1.0


	Epoch 11
Training results:
gen_loss: 185.3971
disc_loss: 0.19406651
disc_acc: 0.9987623762376238

Validation results:
gen_loss: 192.94997
disc_loss: 0.0
disc_acc: 1.0


	Epoch 12
Training results:
gen_loss: 227.77504
disc_loss: 0.23918101
disc_acc: 0.9982673267326733

Validation results:
gen_loss: 263.0724
disc_loss: 0.0
disc_acc: 1.0


	Epoch 13
Training results:
gen_loss: 230.817
disc_loss: 0.48190737
disc_acc: 0.9982673267326733

Validation results:
gen_loss: 164.35358
disc_loss: 0.0
disc_acc: 1.0


	Epoch 14
Training results:
gen_loss: 164.48227
disc_loss: 0.0
disc_acc: 1.0

Validation results:
gen_loss: 164.92012
disc_loss: 0.0
disc_acc: 1.0


	Epoch 15
Training results:
gen_loss: 164.138
disc_loss: 0.0
disc_acc: 1.0

Validation results:
gen_loss: 164.50285
disc_loss: 0.016177827
disc_acc: 0.9995039682539683


	Epoch 16
Training results:
gen_loss: 164.22668
disc_loss: 0.0
disc_acc: 1.0

Validation results:
gen_loss: 163.37454
disc_loss: 0.0
disc_acc: 1.0


	Epoch 17
Training results:
gen_loss: 164.2385
disc_loss: 0.0
disc_acc: 1.0

Validation results:
gen_loss: 165.68695
disc_loss: 8.687326e-06
disc_acc: 1.0


	Epoch 18
Training results:
gen_loss: 163.95789
disc_loss: 0.0
disc_acc: 1.0

Validation results:
gen_loss: 164.68512
disc_loss: 0.0
disc_acc: 1.0


	Epoch 19
Training results:
gen_loss: 164.22842
disc_loss: 0.0
disc_acc: 1.0

Validation results:
gen_loss: 163.78395
disc_loss: 0.0
disc_acc: 1.0


	Epoch 20
Training results:
gen_loss: 531.32983
disc_loss: 2.3336806
disc_acc: 0.9938118811881188

Validation results:
gen_loss: 597.20465
disc_loss: 0.46026102
disc_acc: 0.9990079365079365


	Epoch 21
Training results:
gen_loss: 633.42004
disc_loss: 0.082165524
disc_acc: 0.9997524752475248

Validation results:
gen_loss: 648.3237
disc_loss: 0.0
disc_acc: 1.0


	Epoch 22
Training results:
gen_loss: 642.615
disc_loss: 0.0
disc_acc: 1.0

Validation results:
gen_loss: 647.5231
disc_loss: 0.0
disc_acc: 1.0


	Epoch 23
Training results:
gen_loss: 644.00104
disc_loss: 0.0
disc_acc: 1.0

Validation results:
gen_loss: 650.54803
disc_loss: 0.0
disc_acc: 1.0


	Epoch 24
Training results:
gen_loss: 644.4999
disc_loss: 0.0
disc_acc: 1.0

Validation results:
gen_loss: 647.25604
disc_loss: 0.0
disc_acc: 1.0


	Epoch 25
Training results:
gen_loss: 644.31793
disc_loss: 0.0
disc_acc: 1.0

Validation results:
gen_loss: 647.2708
disc_loss: 0.0
disc_acc: 1.0


	Epoch 26
Training results:
gen_loss: 643.22003
disc_loss: 2.279307
disc_acc: 0.9977722772277228

Validation results:
gen_loss: 595.65497
disc_loss: 0.37744224
disc_acc: 0.9990079365079365


	Epoch 27
Training results:
gen_loss: 643.97504
disc_loss: 0.581483
disc_acc: 0.9986386138613862

Validation results:
gen_loss: 688.33325
disc_loss: 0.0
disc_acc: 1.0


	Epoch 28
Training results:
gen_loss: 666.3031
disc_loss: 1.0997238
disc_acc: 0.999009900990099

Validation results:
gen_loss: 638.1257
disc_loss: 0.0
disc_acc: 1.0


	Epoch 29
Training results:
gen_loss: 595.24506
disc_loss: 0.0
disc_acc: 1.0

Validation results:
gen_loss: 594.56995
disc_loss: 0.0
disc_acc: 1.0


	Epoch 30
Training results:
gen_loss: 594.177
disc_loss: 0.0
disc_acc: 1.0

Validation results:
gen_loss: 592.4311
disc_loss: 0.0
disc_acc: 1.0


	Epoch 31
Training results:
gen_loss: 594.2047
disc_loss: 0.0
disc_acc: 1.0

Validation results:
gen_loss: 598.43243
disc_loss: 0.0
disc_acc: 1.0


	Epoch 32
Training results:
gen_loss: 593.5084
disc_loss: 0.0
disc_acc: 1.0

Validation results:
gen_loss: 595.8691
disc_loss: 0.0
disc_acc: 1.0


	Epoch 33
Training results:
gen_loss: 594.06415
disc_loss: 0.0
disc_acc: 1.0

Validation results:
gen_loss: 594.2461
disc_loss: 0.0
disc_acc: 1.0


	Epoch 34
Training results:
gen_loss: 593.7381
disc_loss: 0.0
disc_acc: 1.0

Validation results:
gen_loss: 595.0824
disc_loss: 0.0
disc_acc: 1.0


	Epoch 35
Training results:
gen_loss: 595.02136
disc_loss: 0.0
disc_acc: 1.0

Validation results:
gen_loss: 592.4054
disc_loss: 0.0
disc_acc: 1.0


	Epoch 36
Training results:
gen_loss: 594.48566
disc_loss: 0.0
disc_acc: 1.0

Validation results:
gen_loss: 596.607
disc_loss: 0.0
disc_acc: 1.0


	Epoch 37
Training results:
gen_loss: 592.5634
disc_loss: 0.0
disc_acc: 1.0

Validation results:
gen_loss: 595.6352
disc_loss: 0.0
disc_acc: 1.0


	Epoch 38
Training results:
gen_loss: 594.41266
disc_loss: 0.0
disc_acc: 1.0

Validation results:
gen_loss: 593.4031
disc_loss: 0.0
disc_acc: 1.0


	Epoch 39
Training results:
gen_loss: 595.60455
disc_loss: 0.0
disc_acc: 1.0

Validation results:
gen_loss: 593.5822
disc_loss: 0.0
disc_acc: 1.0


	Epoch 40
Training results:
gen_loss: 594.73566
disc_loss: 0.0
disc_acc: 1.0

Validation results:
gen_loss: 594.5499
disc_loss: 0.0
disc_acc: 1.0


	Epoch 41
Training results:
gen_loss: 592.511
disc_loss: 0.0
disc_acc: 1.0

Validation results:
gen_loss: 592.71
disc_loss: 0.0
disc_acc: 1.0


	Epoch 42
Training results:
gen_loss: 594.16534
disc_loss: 0.0
disc_acc: 1.0

Validation results:
gen_loss: 593.85736
disc_loss: 0.0
disc_acc: 1.0


	Epoch 43
Training results:
gen_loss: 595.15045
disc_loss: 0.0
disc_acc: 1.0

Validation results:
gen_loss: 594.1627
disc_loss: 0.0
disc_acc: 1.0


	Epoch 44
Training results:
gen_loss: 592.8052
disc_loss: 0.0
disc_acc: 1.0

Validation results:
gen_loss: 594.3123
disc_loss: 0.0
disc_acc: 1.0


	Epoch 45
Training results:
gen_loss: 594.32196
disc_loss: 0.0
disc_acc: 1.0

Validation results:
gen_loss: 596.38403
disc_loss: 0.0
disc_acc: 1.0


	Epoch 46
Training results:
gen_loss: 594.47845
disc_loss: 0.0
disc_acc: 1.0

Validation results:
gen_loss: 594.58295
disc_loss: 0.0
disc_acc: 1.0


	Epoch 47
Training results:
gen_loss: 594.7347
disc_loss: 0.0
disc_acc: 1.0

Validation results:
gen_loss: 594.5042
disc_loss: 0.0
disc_acc: 1.0


	Epoch 48
Training results:
gen_loss: 593.3855
disc_loss: 0.0
disc_acc: 1.0

Validation results:
gen_loss: 594.8832
disc_loss: 0.0
disc_acc: 1.0


	Epoch 49
Training results:
gen_loss: 592.81586
disc_loss: 0.0
disc_acc: 1.0

Validation results:
gen_loss: 597.6952
disc_loss: 0.0
disc_acc: 1.0


	Epoch 50
Training results:
gen_loss: 595.39764
disc_loss: 0.0
disc_acc: 1.0

Validation results:
gen_loss: 593.3065
disc_loss: 0.0
disc_acc: 1.0


	Epoch 51
Training results:
gen_loss: 593.8278
disc_loss: 0.0
disc_acc: 1.0

Validation results:
gen_loss: 596.24194
disc_loss: 0.0
disc_acc: 1.0


	Epoch 52
Training results:
gen_loss: 594.96484
disc_loss: 0.0
disc_acc: 1.0

Validation results:
gen_loss: 595.4998
disc_loss: 0.0
disc_acc: 1.0


	Epoch 53
Training results:
gen_loss: 594.79486
disc_loss: 0.0
disc_acc: 1.0

Validation results:
gen_loss: 595.5157
disc_loss: 0.0
disc_acc: 1.0


	Epoch 54
Training results:
gen_loss: 593.5615
disc_loss: 0.0
disc_acc: 1.0

Validation results:
gen_loss: 593.6323
disc_loss: 0.09972709
disc_acc: 0.9995039682539683


	Epoch 55
Training results:
gen_loss: 594.5507
disc_loss: 0.0
disc_acc: 1.0

Validation results:
gen_loss: 593.27954
disc_loss: 0.0
disc_acc: 1.0


	Epoch 56
Training results:
gen_loss: 594.8015
disc_loss: 0.0
disc_acc: 1.0

Validation results:
gen_loss: 596.0154
disc_loss: 0.0
disc_acc: 1.0


	Epoch 57
Training results:
gen_loss: 594.1751
disc_loss: 0.0
disc_acc: 1.0

Validation results:
gen_loss: 595.3594
disc_loss: 0.0
disc_acc: 1.0


	Epoch 58
Training results:
gen_loss: 594.28253
disc_loss: 0.0
disc_acc: 1.0

Validation results:
gen_loss: 594.4667
disc_loss: 0.0
disc_acc: 1.0


	Epoch 59
Training results:
gen_loss: 594.32874
disc_loss: 0.0
disc_acc: 1.0

Validation results:
gen_loss: 595.6213
disc_loss: 0.0
disc_acc: 1.0


	Epoch 60
Training results:
gen_loss: 595.1126
disc_loss: 0.0
disc_acc: 1.0

Validation results:
gen_loss: 594.1675
disc_loss: 0.0
disc_acc: 1.0


	Epoch 61
Training results:
gen_loss: 593.54346
disc_loss: 0.0
disc_acc: 1.0

Validation results:
gen_loss: 597.6961
disc_loss: 0.0
disc_acc: 1.0


	Epoch 62
Training results:
gen_loss: 593.15485
disc_loss: 0.0
disc_acc: 1.0

Validation results:
gen_loss: 596.6774
disc_loss: 0.0
disc_acc: 1.0


	Epoch 63
Training results:
gen_loss: 592.6331
disc_loss: 0.0
disc_acc: 1.0

Validation results:
gen_loss: 594.81885
disc_loss: 0.0
disc_acc: 1.0


	Epoch 64
Training results:
gen_loss: 594.31854
disc_loss: 0.0
disc_acc: 1.0

Validation results:
gen_loss: 595.59595
disc_loss: 0.0
disc_acc: 1.0


	Epoch 65
Training results:
gen_loss: 592.5213
disc_loss: 0.0
disc_acc: 1.0

Validation results:
gen_loss: 592.1337
disc_loss: 0.0
disc_acc: 1.0


	Epoch 66
Training results:
gen_loss: 593.7506
disc_loss: 0.0
disc_acc: 1.0

Validation results:
gen_loss: 592.20276
disc_loss: 0.0
disc_acc: 1.0


	Epoch 67
Training results:
gen_loss: 594.4969
disc_loss: 0.0
disc_acc: 1.0

Validation results:
gen_loss: 593.3867
disc_loss: 0.0
disc_acc: 1.0


	Epoch 68
Training results:
gen_loss: 593.41943
disc_loss: 0.0
disc_acc: 1.0

Validation results:
gen_loss: 596.6043
disc_loss: 0.0
disc_acc: 1.0


	Epoch 69
Training results:
gen_loss: 594.2534
disc_loss: 0.0
disc_acc: 1.0

Validation results:
gen_loss: 596.41455
disc_loss: 0.0
disc_acc: 1.0


	Epoch 70
Training results:
gen_loss: 593.08514
disc_loss: 0.0
disc_acc: 1.0

Validation results:
gen_loss: 594.82855
disc_loss: 0.0
disc_acc: 1.0


	Epoch 71
Training results:
gen_loss: 595.2605
disc_loss: 0.0
disc_acc: 1.0

Validation results:
gen_loss: 593.3879
disc_loss: 0.0
disc_acc: 1.0


	Epoch 72
Training results:
gen_loss: 592.65247
disc_loss: 0.0
disc_acc: 1.0

Validation results:
gen_loss: 594.5406
disc_loss: 0.0
disc_acc: 1.0


	Epoch 73
Training results:
gen_loss: 593.98834
disc_loss: 0.0
disc_acc: 1.0

Validation results:
gen_loss: 595.56476
disc_loss: 0.0
disc_acc: 1.0


	Epoch 74
Training results:
gen_loss: 593.3278
disc_loss: 0.0
disc_acc: 1.0

Validation results:
gen_loss: 595.99854
disc_loss: 0.0
disc_acc: 1.0


	Epoch 75
Training results:
gen_loss: 593.8213
disc_loss: 0.0
disc_acc: 1.0

Validation results:
gen_loss: 594.863
disc_loss: 0.0
disc_acc: 1.0


	Epoch 76
Training results:
gen_loss: 593.32324
disc_loss: 0.0
disc_acc: 1.0

Validation results:
gen_loss: 594.48224
disc_loss: 0.0
disc_acc: 1.0


	Epoch 77
Training results:
gen_loss: 593.888
disc_loss: 0.0
disc_acc: 1.0

Validation results:
gen_loss: 595.3862
disc_loss: 0.0
disc_acc: 1.0


	Epoch 78
Training results:
gen_loss: 593.85345
disc_loss: 0.0
disc_acc: 1.0

Validation results:
gen_loss: 593.62646
disc_loss: 0.0
disc_acc: 1.0


	Epoch 79
Training results:
gen_loss: 593.80414
disc_loss: 0.0
disc_acc: 1.0

Validation results:
gen_loss: 594.9509
disc_loss: 0.0
disc_acc: 1.0


	Epoch 80
Training results:
gen_loss: 594.1001
disc_loss: 0.0
disc_acc: 1.0

Validation results:
gen_loss: 594.3421
disc_loss: 0.0
disc_acc: 1.0


	Epoch 81
Training results:
gen_loss: 594.475
disc_loss: 0.0
disc_acc: 1.0

Validation results:
gen_loss: 595.28735
disc_loss: 0.0
disc_acc: 1.0


	Epoch 82
Training results:
gen_loss: 594.5511
disc_loss: 0.0
disc_acc: 1.0

Validation results:
gen_loss: 598.4543
disc_loss: 0.0
disc_acc: 1.0


	Epoch 83
Training results:
gen_loss: 593.76025
disc_loss: 0.0
disc_acc: 1.0

Validation results:
gen_loss: 596.52264
disc_loss: 0.0
disc_acc: 1.0


	Epoch 84
Training results:
gen_loss: 592.8735
disc_loss: 0.0
disc_acc: 1.0

Validation results:
gen_loss: 598.1979
disc_loss: 0.0
disc_acc: 1.0


	Epoch 85
Training results:
gen_loss: 593.322
disc_loss: 0.0
disc_acc: 1.0

Validation results:
gen_loss: 594.89026
disc_loss: 0.0
disc_acc: 1.0


	Epoch 86
Training results:
gen_loss: 594.3279
disc_loss: 0.0
disc_acc: 1.0

Validation results:
gen_loss: 594.3459
disc_loss: 0.0
disc_acc: 1.0


	Epoch 87
Training results:
gen_loss: 593.4674
disc_loss: 0.0
disc_acc: 1.0

Validation results:
gen_loss: 595.3592
disc_loss: 0.0
disc_acc: 1.0


	Epoch 88
Training results:
gen_loss: 592.55743
disc_loss: 0.0
disc_acc: 1.0

Validation results:
gen_loss: 593.1997
disc_loss: 0.0
disc_acc: 1.0


	Epoch 89
Training results:
gen_loss: 593.73016
disc_loss: 0.0
disc_acc: 1.0

Validation results:
gen_loss: 596.32184
disc_loss: 0.0
disc_acc: 1.0


	Epoch 90
Training results:
gen_loss: 594.6422
disc_loss: 0.0
disc_acc: 1.0

Validation results:
gen_loss: 595.1565
disc_loss: 0.0
disc_acc: 1.0


	Epoch 91
Training results:
gen_loss: 595.4709
disc_loss: 0.0
disc_acc: 1.0

Validation results:
gen_loss: 595.3284
disc_loss: 0.0
disc_acc: 1.0


	Epoch 92
Training results:
gen_loss: 593.67206
disc_loss: 0.0
disc_acc: 1.0

Validation results:
gen_loss: 594.4558
disc_loss: 0.0
disc_acc: 1.0


	Epoch 93
Training results:
gen_loss: 594.4153
disc_loss: 0.0
disc_acc: 1.0

Validation results:
gen_loss: 592.0933
disc_loss: 0.0
disc_acc: 1.0


	Epoch 94
Training results:
gen_loss: 594.4054
disc_loss: 0.0
disc_acc: 1.0

Validation results:
gen_loss: 592.80457
disc_loss: 0.0
disc_acc: 1.0


	Epoch 95
Training results:
gen_loss: 593.37946
disc_loss: 0.0
disc_acc: 1.0

Validation results:
gen_loss: 593.1589
disc_loss: 0.0
disc_acc: 1.0


	Epoch 96
Training results:
gen_loss: 593.42
disc_loss: 0.0
disc_acc: 1.0

Validation results:
gen_loss: 594.5541
disc_loss: 0.0
disc_acc: 1.0


	Epoch 97
Training results:
gen_loss: 594.09283
disc_loss: 0.0
disc_acc: 1.0

Validation results:
gen_loss: 595.302
disc_loss: 0.0
disc_acc: 1.0


	Epoch 98
Training results:
gen_loss: 594.0305
disc_loss: 0.0
disc_acc: 1.0

Validation results:
gen_loss: 598.14343
disc_loss: 0.0
disc_acc: 1.0


	Epoch 99
Training results:
gen_loss: 594.25287
disc_loss: 0.0
disc_acc: 1.0

Validation results:
gen_loss: 596.5932
disc_loss: 0.0
disc_acc: 1.0


	Epoch 100
Training results:
gen_loss: 593.40155
disc_loss: 0.0
disc_acc: 1.0

Validation results:
gen_loss: 597.2147
disc_loss: 0.0
disc_acc: 1.0



gen_train_loss: 1.285839, 5.056077, 5.019606, 4.3744416, 4.162913, 3.8578732, 4.063985, 3.5620925, 4.6883516, 4.1417603, 3.409003, 3.9408467, 3.4184995, 3.2485023, 4.33203, 5.9272046, 6.1752253, 6.2944336, 6.00792, 6.6669283, 9.531699, 4.877143, 7.6661825, 6.6606717, 8.531816, 6.877006, 7.2837744, 7.7173934, 8.796293, 8.099369, 8.287974, 5.5704904, 4.3622766, 6.1660113, 6.203637, 4.5658154, 6.356356, 5.5776696, 5.3928156, 7.512904, 4.8056517, 4.83422, 6.47407, 11.000604, 10.4456835, 5.989551, 6.4070225, 6.645083, 10.304243, 9.756299, 8.156911, 9.044376, 14.997554, 15.970669, 12.635454, 17.440363, 18.226967, 14.378752, 17.018015, 12.87872, 12.644156, 13.140215, 17.583115, 18.37967, 23.659454, 23.96459, 20.956429, 20.307343, 23.571684, 26.114153, 25.635288, 26.48095, 28.21355, 26.050877, 29.682106, 26.173248, 30.11719, 29.910336, 34.285725, 32.698864, 30.532549, 37.206562, 39.865585, 51.26599, 38.966095, 35.820343, 35.409214, 34.995728, 46.504898, 38.032253, 41.245358, 45.42756, 49.72987, 46.750095, 54.072758, 48.525566, 49.5863, 55.206463, 47.019337, 44.489666, 39.727917, 1.1286682, 9.862069, 18.755692, 21.504982, 82.22736, 90.36512, 88.45823, 124.44633, 136.74638, 199.64174, 156.82318, 185.3971, 227.77504, 230.817, 164.48227, 164.138, 164.22668, 164.2385, 163.95789, 164.22842, 531.32983, 633.42004, 642.615, 644.00104, 644.4999, 644.31793, 643.22003, 643.97504, 666.3031, 595.24506, 594.177, 594.2047, 593.5084, 594.06415, 593.7381, 595.02136, 594.48566, 592.5634, 594.41266, 595.60455, 594.73566, 592.511, 594.16534, 595.15045, 592.8052, 594.32196, 594.47845, 594.7347, 593.3855, 592.81586, 595.39764, 593.8278, 594.96484, 594.79486, 593.5615, 594.5507, 594.8015, 594.1751, 594.28253, 594.32874, 595.1126, 593.54346, 593.15485, 592.6331, 594.31854, 592.5213, 593.7506, 594.4969, 593.41943, 594.2534, 593.08514, 595.2605, 592.65247, 593.98834, 593.3278, 593.8213, 593.32324, 593.888, 593.85345, 593.80414, 594.1001, 594.475, 594.5511, 593.76025, 592.8735, 593.322, 594.3279, 593.4674, 592.55743, 593.73016, 594.6422, 595.4709, 593.67206, 594.4153, 594.4054, 593.37946, 593.42, 594.09283, 594.0305, 594.25287, 593.40155
disc_train_loss: 5.5640526, 0.87767285, 0.53974223, 0.45640528, 0.5823086, 0.610605, 0.5690732, 0.5243742, 0.7508123, 0.5893844, 0.606887, 0.57117814, 0.42487243, 0.29704112, 0.3870471, 0.36720243, 0.41219807, 0.51442873, 0.5811018, 0.42441475, 1.0791914, 0.24552903, 0.4073865, 0.382742, 0.40033013, 0.34748682, 0.4198062, 0.28695548, 0.41218683, 0.35919395, 0.42635885, 0.47957417, 0.21090302, 0.30417645, 0.31615058, 0.23483625, 0.38485596, 0.1446078, 0.2061916, 0.25453836, 0.17931317, 0.11164917, 0.22811793, 0.49205175, 0.3163415, 0.16763611, 0.18139835, 0.26885036, 0.222277, 0.31645963, 0.1574588, 0.1742876, 0.35927767, 0.29647163, 0.23982018, 0.1666713, 0.26476023, 0.20193955, 0.38410118, 0.24318464, 0.10424487, 0.15452906, 0.3137935, 0.2697049, 0.35147592, 0.43324262, 0.17361365, 0.16318698, 0.30639026, 0.36720598, 0.3249038, 0.3777134, 0.53410214, 0.29446462, 0.5297153, 0.26430428, 0.44096407, 0.27340087, 0.8082173, 0.24313578, 0.43306303, 0.4646273, 0.49518016, 1.0661196, 0.40972924, 0.6200315, 0.5813249, 0.42737973, 0.90782523, 0.34962568, 0.36381117, 0.5082735, 0.84964204, 0.6714183, 0.67365927, 0.67272043, 0.7527211, 0.8543534, 0.76070917, 0.7830089, 0.6473791, 5.6058893, 0.2938418, 0.2549272, 0.0043687345, 1.1901512, 0.03536109, 0.3219612, 0.049420454, 0.6070976, 1.0753752, 0.054646395, 0.19406651, 0.23918101, 0.48190737, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 2.3336806, 0.082165524, 0.0, 0.0, 0.0, 0.0, 2.279307, 0.581483, 1.0997238, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0
disc_train_acc: 0.021905940594059405, 0.8409653465346535, 0.9034653465346535, 0.9153465346534654, 0.8915841584158416, 0.8746287128712872, 0.8696782178217822, 0.8771039603960396, 0.8393564356435643, 0.866460396039604, 0.843440594059406, 0.8560643564356436, 0.8872524752475247, 0.9172029702970297, 0.9094059405940594, 0.9256188118811881, 0.9090346534653465, 0.8998762376237623, 0.8662128712871288, 0.9064356435643565, 0.8482673267326732, 0.9469059405940594, 0.9172029702970297, 0.9142326732673267, 0.8988861386138614, 0.9106435643564357, 0.9087871287128713, 0.9308168316831683, 0.8925742574257426, 0.9153465346534654, 0.8888613861386139, 0.8726485148514852, 0.9372524752475248, 0.9243811881188119, 0.9216584158415841, 0.9444306930693069, 0.9103960396039604, 0.9648514851485148, 0.9574257425742574, 0.9449257425742574, 0.9622524752475248, 0.9717821782178218, 0.9558168316831683, 0.9341584158415842, 0.9371287128712872, 0.9618811881188118, 0.9561881188118811, 0.9518564356435644, 0.9544554455445544, 0.9471534653465347, 0.9678217821782178, 0.9724009900990099, 0.9545792079207921, 0.9590346534653466, 0.9643564356435643, 0.9754950495049505, 0.9698019801980198, 0.9706683168316832, 0.9537128712871287, 0.9696782178217822, 0.9801980198019802, 0.9725247524752475, 0.9653465346534653, 0.9711633663366337, 0.9633663366336633, 0.9622524752475248, 0.9810643564356436, 0.9766089108910891, 0.9715346534653465, 0.968440594059406, 0.9736386138613862, 0.9716584158415842, 0.9681930693069307, 0.9768564356435644, 0.9655940594059406, 0.9771039603960396, 0.9675742574257425, 0.9790841584158416, 0.9639851485148515, 0.9811881188118812, 0.9773514851485149, 0.9773514851485149, 0.9748762376237624, 0.9617574257425743, 0.9789603960396039, 0.9701732673267327, 0.9731435643564357, 0.9756188118811882, 0.9646039603960396, 0.9793316831683169, 0.9799504950495049, 0.9752475247524752, 0.9691831683168317, 0.9745049504950495, 0.9714108910891089, 0.9712871287128713, 0.974009900990099, 0.9681930693069307, 0.9691831683168317, 0.9701732673267327, 0.9728960396039604, 0.0, 0.9506188118811881, 0.9853960396039604, 0.9993811881188119, 0.9842821782178218, 0.9991336633663367, 0.9965346534653465, 0.999009900990099, 0.9955445544554455, 0.9951732673267327, 0.9996287128712872, 0.9987623762376238, 0.9982673267326733, 0.9982673267326733, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9938118811881188, 0.9997524752475248, 1.0, 1.0, 1.0, 1.0, 0.9977722772277228, 0.9986386138613862, 0.999009900990099, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0
gen_val_loss: 1.2870709, 8.240648, 4.372683, 4.0846605, 3.8122888, 3.5253198, 3.220265, 2.7333207, 4.2532816, 4.0895853, 3.0910194, 2.8330603, 3.0990872, 4.70735, 3.838601, 12.3511715, 7.1332016, 11.849588, 7.2739353, 9.210997, 9.502517, 4.1291823, 6.601918, 10.551357, 7.8031573, 5.514217, 10.197987, 5.4255366, 8.640185, 13.376267, 6.905038, 4.2749166, 4.025615, 10.05686, 4.720476, 3.4380727, 5.540535, 4.790269, 8.100107, 6.515589, 4.285686, 6.0246854, 5.1717715, 12.3024645, 7.375338, 6.2105637, 5.598792, 13.226751, 8.965342, 6.140306, 9.054442, 9.256833, 18.492136, 15.075006, 13.89381, 13.819395, 15.369975, 21.574066, 14.417939, 14.670578, 10.605987, 13.749527, 18.470451, 15.468356, 20.586031, 22.24628, 20.965181, 22.660625, 27.583904, 28.652206, 26.333437, 32.47713, 28.764174, 27.513126, 30.14444, 24.295834, 33.402042, 27.980621, 39.251286, 33.96579, 36.107887, 41.014416, 57.100384, 43.384487, 38.921864, 37.724575, 31.011396, 36.863987, 43.61134, 42.154957, 44.03222, 52.500336, 54.532425, 47.703644, 52.564117, 48.84207, 50.900146, 56.051224, 50.276608, 44.72526, 37.498344, 1.1419588, 13.098334, 25.617031, 34.76393, 95.81329, 96.01664, 96.39886, 129.56516, 236.15463, 169.35922, 177.26242, 192.94997, 263.0724, 164.35358, 164.92012, 164.50285, 163.37454, 165.68695, 164.68512, 163.78395, 597.20465, 648.3237, 647.5231, 650.54803, 647.25604, 647.2708, 595.65497, 688.33325, 638.1257, 594.56995, 592.4311, 598.43243, 595.8691, 594.2461, 595.0824, 592.4054, 596.607, 595.6352, 593.4031, 593.5822, 594.5499, 592.71, 593.85736, 594.1627, 594.3123, 596.38403, 594.58295, 594.5042, 594.8832, 597.6952, 593.3065, 596.24194, 595.4998, 595.5157, 593.6323, 593.27954, 596.0154, 595.3594, 594.4667, 595.6213, 594.1675, 597.6961, 596.6774, 594.81885, 595.59595, 592.1337, 592.20276, 593.3867, 596.6043, 596.41455, 594.82855, 593.3879, 594.5406, 595.56476, 595.99854, 594.863, 594.48224, 595.3862, 593.62646, 594.9509, 594.3421, 595.28735, 598.4543, 596.52264, 598.1979, 594.89026, 594.3459, 595.3592, 593.1997, 596.32184, 595.1565, 595.3284, 594.4558, 592.0933, 592.80457, 593.1589, 594.5541, 595.302, 598.14343, 596.5932, 597.2147
disc_val_loss: 5.5621567, 0.262876, 1.1997291, 0.104325615, 2.1802294, 0.24002603, 0.22026117, 0.2928956, 1.4817857, 1.1738718, 0.22754957, 0.40466973, 0.4108682, 0.2523271, 0.060757622, 0.13584292, 0.6454617, 0.38771847, 0.1382884, 1.1860869, 3.9267485, 0.16070932, 0.66876405, 0.04778046, 0.27451995, 0.04712656, 0.05041362, 0.093293, 0.16496669, 0.15400472, 0.39986762, 0.15844542, 0.0095009245, 0.3359079, 0.6368527, 0.033761237, 0.03719086, 0.028303662, 2.4000707, 0.07690119, 0.13646156, 0.052970577, 0.006441355, 0.027389038, 0.09665168, 0.1852465, 0.32435486, 0.13000277, 0.67939603, 0.051622126, 0.0003354703, 0.25587693, 0.99478656, 0.043551065, 0.14756598, 0.04027236, 0.06112576, 0.053434532, 0.072660245, 0.026648983, 0.00898385, 0.08530891, 0.19722234, 1.7440108, 0.04983022, 0.032438204, 0.32406968, 0.31707126, 0.21629347, 0.3765773, 1.2514927, 0.13416582, 1.1334208, 0.234897, 0.07003876, 0.8906894, 0.024411066, 0.047255352, 0.13689627, 0.11450162, 2.7867215, 0.24038066, 0.20139642, 0.0, 9.025896, 0.8236335, 0.14491574, 1.2653328, 0.16415735, 0.37098405, 0.21966533, 0.14709532, 0.021872144, 0.1452909, 0.16931535, 0.43622196, 0.13793321, 0.04821656, 0.30822974, 0.06639581, 0.3099421, 5.611858, 0.14236474, 0.0012615721, 5.3218154e-09, 0.40260553, 0.0, 0.0, 0.0, 2.4212584, 0.027068093, 1.0075e-07, 0.0, 0.0, 0.0, 0.0, 0.016177827, 0.0, 8.687326e-06, 0.0, 0.0, 0.46026102, 0.0, 0.0, 0.0, 0.0, 0.0, 0.37744224, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.09972709, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0
disc_val_acc: 0.020833333333333332, 0.9345238095238095, 0.8501984126984127, 0.9627976190476191, 0.7053571428571429, 0.9151785714285714, 0.9231150793650794, 0.8874007936507936, 0.7906746031746031, 0.7390873015873016, 0.9211309523809523, 0.9057539682539683, 0.8973214285714286, 0.9131944444444444, 0.9900793650793651, 0.9637896825396826, 0.8358134920634921, 0.8556547619047619, 0.9345238095238095, 0.7638888888888888, 0.6899801587301587, 0.9469246031746031, 0.7688492063492064, 0.9880952380952381, 0.9047619047619048, 0.9935515873015873, 0.9915674603174603, 0.9781746031746031, 0.9608134920634921, 0.9489087301587301, 0.8566468253968254, 0.9404761904761905, 0.9970238095238095, 0.8893849206349206, 0.9007936507936508, 0.9910714285714286, 0.9910714285714286, 0.9940476190476191, 0.8323412698412699, 0.9846230158730159, 0.9543650793650794, 0.9866071428571429, 0.9975198412698413, 0.9950396825396826, 0.96875, 0.9360119047619048, 0.8983134920634921, 0.9697420634920635, 0.8209325396825397, 0.9821428571428571, 1.0, 0.9573412698412699, 0.8363095238095238, 0.9890873015873016, 0.9836309523809523, 0.9945436507936508, 0.9836309523809523, 0.9930555555555556, 0.9821428571428571, 0.9920634920634921, 0.9965277777777778, 0.9846230158730159, 0.9791666666666666, 0.8601190476190477, 0.9885912698412699, 0.9970238095238095, 0.9399801587301587, 0.9672619047619048, 0.9846230158730159, 0.9484126984126984, 0.9370039682539683, 0.9871031746031746, 0.8898809523809523, 0.9776785714285714, 0.9880952380952381, 0.9191468253968254, 0.9985119047619048, 0.9945436507936508, 0.9856150793650794, 0.9930555555555556, 0.9231150793650794, 0.9712301587301587, 0.9895833333333334, 1.0, 0.9196428571428571, 0.9489087301587301, 0.9970238095238095, 0.9548611111111112, 0.9875992063492064, 0.972718253968254, 0.9776785714285714, 0.9950396825396826, 0.998015873015873, 0.9890873015873016, 0.9930555555555556, 0.9920634920634921, 0.9871031746031746, 0.9965277777777778, 0.9771825396825397, 0.9940476190476191, 0.9756944444444444, 0.0, 0.9737103174603174, 0.9995039682539683, 1.0, 0.9975198412698413, 1.0, 1.0, 1.0, 0.9965277777777778, 0.9990079365079365, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9995039682539683, 1.0, 1.0, 1.0, 1.0, 0.9990079365079365, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9990079365079365, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9995039682539683, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0

