Beginning trial described in ./config/multigen_trial.json.
Experiment type: multiple generators each corresponding to 1 key.
Experiment settings:
	byte: 0
	keys: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15]
	key_dataset_kwargs:
		keep_data_in_memory: True
		data_path: ./data
		download: True
		extract: True
		preprocess: True
		delete_download_after_extraction: False
		delete_extracted_after_preprocess: False
	dataloader_kwargs:
		batch_size: 16
		shuffle: True
	dataset_prop_for_validation: 0.2
	trace_map_constructor: None
	trace_map_kwargs:
		layers: [256]
		hidden_activation: <class 'torch.nn.modules.activation.ReLU'>
	plaintext_map_constructor: None
	plaintext_map_kwargs:
		layers: [64]
		hidden_activation: <class 'torch.nn.modules.activation.ReLU'>
	key_map_constructor: <function get_mlp_map at 0x7f03d37e65e0>
	key_map_kwargs:
		layers: [64, 256]
		hidden_activation: <class 'torch.nn.modules.activation.ReLU'>
	cumulative_map_constructor: None
	cumulative_map_kwargs:
		layers: [256]
		hidden_activation: <class 'torch.nn.modules.activation.ReLU'>
	discriminator_constructor: <function get_google_style_resnet_discriminator at 0x7f03d37e6790>
	discriminator_kwargs:
	discriminator_loss_constructor: <class 'torch.nn.modules.loss.CrossEntropyLoss'>
	discriminator_loss_kwargs:
	discriminator_optimizer_constructor: <class 'torch.optim.adam.Adam'>
	discriminator_optimizer_kwargs:
	generator_loss_constructor: <class 'loss_functions.BatchStdLoss'>
	generator_loss_kwargs:
	generator_optimizer_constructor: <class 'torch.optim.adam.Adam'>
	generator_optimizer_kwargs:
	device: cuda
	discriminator_pretraining_epochs: 0
	generator_pretraining_epochs: 0
	gan_training_epochs: 100
	discriminator_posttraining_epochs: 100
	seed: 0
Loading datasets.
AesKeyGroupDataset:
	Available keys: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15]
	Key transform: Compose(
    IntToBinary()
    ToTensor1D()
)
	Byte: 0
	Number of samples available: 10112
	Trace size: torch.Size([1, 3000])
	Key size: torch.Size([1, 8])
	Plaintext size: torch.Size([1, 8])
	Key index size: ()
Constructing generator.
KeyOnlyGenerator(
  (key_trace_map): Sequential(
    (0): Flatten(start_dim=1, end_dim=-1)
    (1): Linear(in_features=8, out_features=64, bias=True)
    (2): ReLU()
    (3): Linear(in_features=64, out_features=256, bias=True)
    (4): ReLU()
    (5): Linear(in_features=256, out_features=3000, bias=True)
    (6): Unflatten(dim=-1, unflattened_size=torch.Size([1, 3000]))
  )
)

Constructing discriminator.
Discriminator(
  (model): Sequential(
    (0): MaxPool1d(kernel_size=4, stride=4, padding=0, dilation=1, ceil_mode=False)
    (1): Stack(
      (F): Sequential(
        (0): Block(
          (F): Sequential(
            (0): BatchNorm1d(1, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (1): ReLU()
            (2): Conv1d(1, 16, kernel_size=(1,), stride=(1,), bias=False)
            (3): BatchNorm1d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (4): ReLU()
            (5): Conv1d(16, 16, kernel_size=(3,), stride=(1,), padding=(1,), bias=False)
            (6): BatchNorm1d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (7): ReLU()
            (8): Conv1d(16, 64, kernel_size=(1,), stride=(1,))
          )
          (Id): Conv1d(1, 64, kernel_size=(1,), stride=(1,))
        )
        (1): Block(
          (F): Sequential(
            (0): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (1): ReLU()
            (2): Conv1d(64, 16, kernel_size=(1,), stride=(1,), bias=False)
            (3): BatchNorm1d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (4): ReLU()
            (5): Conv1d(16, 16, kernel_size=(3,), stride=(1,), padding=(1,), bias=False)
            (6): BatchNorm1d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (7): ReLU()
            (8): Conv1d(16, 64, kernel_size=(1,), stride=(1,))
          )
          (Id): Identity()
        )
        (2): Block(
          (F): Sequential(
            (0): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (1): ReLU()
            (2): Conv1d(64, 16, kernel_size=(1,), stride=(1,), bias=False)
            (3): BatchNorm1d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (4): ReLU()
            (5): Conv1d(16, 16, kernel_size=(3,), stride=(2,), padding=(1,), bias=False)
            (6): BatchNorm1d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (7): ReLU()
            (8): Conv1d(16, 64, kernel_size=(1,), stride=(1,))
          )
          (Id): MaxPool1d(kernel_size=1, stride=2, padding=0, dilation=1, ceil_mode=False)
        )
      )
    )
    (2): Stack(
      (F): Sequential(
        (0): Block(
          (F): Sequential(
            (0): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (1): ReLU()
            (2): Conv1d(64, 32, kernel_size=(1,), stride=(1,), bias=False)
            (3): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (4): ReLU()
            (5): Conv1d(32, 32, kernel_size=(3,), stride=(1,), padding=(1,), bias=False)
            (6): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (7): ReLU()
            (8): Conv1d(32, 128, kernel_size=(1,), stride=(1,))
          )
          (Id): Conv1d(64, 128, kernel_size=(1,), stride=(1,))
        )
        (1): Block(
          (F): Sequential(
            (0): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (1): ReLU()
            (2): Conv1d(128, 32, kernel_size=(1,), stride=(1,), bias=False)
            (3): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (4): ReLU()
            (5): Conv1d(32, 32, kernel_size=(3,), stride=(1,), padding=(1,), bias=False)
            (6): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (7): ReLU()
            (8): Conv1d(32, 128, kernel_size=(1,), stride=(1,))
          )
          (Id): Identity()
        )
        (2): Block(
          (F): Sequential(
            (0): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (1): ReLU()
            (2): Conv1d(128, 32, kernel_size=(1,), stride=(1,), bias=False)
            (3): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (4): ReLU()
            (5): Conv1d(32, 32, kernel_size=(3,), stride=(1,), padding=(1,), bias=False)
            (6): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (7): ReLU()
            (8): Conv1d(32, 128, kernel_size=(1,), stride=(1,))
          )
          (Id): Identity()
        )
        (3): Block(
          (F): Sequential(
            (0): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (1): ReLU()
            (2): Conv1d(128, 32, kernel_size=(1,), stride=(1,), bias=False)
            (3): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (4): ReLU()
            (5): Conv1d(32, 32, kernel_size=(3,), stride=(2,), padding=(1,), bias=False)
            (6): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (7): ReLU()
            (8): Conv1d(32, 128, kernel_size=(1,), stride=(1,))
          )
          (Id): MaxPool1d(kernel_size=1, stride=2, padding=0, dilation=1, ceil_mode=False)
        )
      )
    )
    (3): Stack(
      (F): Sequential(
        (0): Block(
          (F): Sequential(
            (0): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (1): ReLU()
            (2): Conv1d(128, 64, kernel_size=(1,), stride=(1,), bias=False)
            (3): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (4): ReLU()
            (5): Conv1d(64, 64, kernel_size=(3,), stride=(1,), padding=(1,), bias=False)
            (6): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (7): ReLU()
            (8): Conv1d(64, 256, kernel_size=(1,), stride=(1,))
          )
          (Id): Conv1d(128, 256, kernel_size=(1,), stride=(1,))
        )
        (1): Block(
          (F): Sequential(
            (0): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (1): ReLU()
            (2): Conv1d(256, 64, kernel_size=(1,), stride=(1,), bias=False)
            (3): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (4): ReLU()
            (5): Conv1d(64, 64, kernel_size=(3,), stride=(1,), padding=(1,), bias=False)
            (6): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (7): ReLU()
            (8): Conv1d(64, 256, kernel_size=(1,), stride=(1,))
          )
          (Id): Identity()
        )
        (2): Block(
          (F): Sequential(
            (0): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (1): ReLU()
            (2): Conv1d(256, 64, kernel_size=(1,), stride=(1,), bias=False)
            (3): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (4): ReLU()
            (5): Conv1d(64, 64, kernel_size=(3,), stride=(1,), padding=(1,), bias=False)
            (6): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (7): ReLU()
            (8): Conv1d(64, 256, kernel_size=(1,), stride=(1,))
          )
          (Id): Identity()
        )
        (3): Block(
          (F): Sequential(
            (0): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (1): ReLU()
            (2): Conv1d(256, 64, kernel_size=(1,), stride=(1,), bias=False)
            (3): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (4): ReLU()
            (5): Conv1d(64, 64, kernel_size=(3,), stride=(2,), padding=(1,), bias=False)
            (6): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (7): ReLU()
            (8): Conv1d(64, 256, kernel_size=(1,), stride=(1,))
          )
          (Id): MaxPool1d(kernel_size=1, stride=2, padding=0, dilation=1, ceil_mode=False)
        )
      )
    )
    (4): Stack(
      (F): Sequential(
        (0): Block(
          (F): Sequential(
            (0): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (1): ReLU()
            (2): Conv1d(256, 128, kernel_size=(1,), stride=(1,), bias=False)
            (3): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (4): ReLU()
            (5): Conv1d(128, 128, kernel_size=(3,), stride=(1,), padding=(1,), bias=False)
            (6): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (7): ReLU()
            (8): Conv1d(128, 512, kernel_size=(1,), stride=(1,))
          )
          (Id): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
        )
        (1): Block(
          (F): Sequential(
            (0): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (1): ReLU()
            (2): Conv1d(512, 128, kernel_size=(1,), stride=(1,), bias=False)
            (3): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (4): ReLU()
            (5): Conv1d(128, 128, kernel_size=(3,), stride=(1,), padding=(1,), bias=False)
            (6): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (7): ReLU()
            (8): Conv1d(128, 512, kernel_size=(1,), stride=(1,))
          )
          (Id): Identity()
        )
        (2): Block(
          (F): Sequential(
            (0): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (1): ReLU()
            (2): Conv1d(512, 128, kernel_size=(1,), stride=(1,), bias=False)
            (3): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (4): ReLU()
            (5): Conv1d(128, 128, kernel_size=(3,), stride=(2,), padding=(1,), bias=False)
            (6): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (7): ReLU()
            (8): Conv1d(128, 512, kernel_size=(1,), stride=(1,))
          )
          (Id): MaxPool1d(kernel_size=1, stride=2, padding=0, dilation=1, ceil_mode=False)
        )
      )
    )
    (5): Flatten(start_dim=1, end_dim=-1)
    (6): LazyLinear(in_features=0, out_features=256, bias=True)
  )
)

Calculating initial results.
Training results:
gen_loss: 1.3862987
disc_loss: 5.53727
disc_acc: 0.002103960396039604

Validation results:
gen_loss: 1.3850006
disc_loss: 5.5387955
disc_acc: 0.004464285714285714


Training discriminator and generator simultaneously.
	Epoch 1
Training results:
gen_loss: 2.0140748
disc_loss: 1.7803814
disc_acc: 0.6193069306930693

Validation results:
gen_loss: 0.48245567
disc_loss: 1.7443458
disc_acc: 0.5243055555555556


	Epoch 2
Training results:
gen_loss: 0.29096654
disc_loss: 2.6011825
disc_acc: 0.25346534653465347

Validation results:
gen_loss: 0.19620453
disc_loss: 2.5961096
disc_acc: 0.2584325396825397


	Epoch 3
Training results:
gen_loss: 0.14945215
disc_loss: 2.7354913
disc_acc: 0.14764851485148514

Validation results:
gen_loss: 0.09023665
disc_loss: 2.834205
disc_acc: 0.21081349206349206


	Epoch 4
Training results:
gen_loss: 0.09930701
disc_loss: 2.7849634
disc_acc: 0.10866336633663366

Validation results:
gen_loss: 0.083565064
disc_loss: 2.7240977
disc_acc: 0.061507936507936505


	Epoch 5
Training results:
gen_loss: 0.078202985
disc_loss: 2.7938545
disc_acc: 0.09022277227722772

Validation results:
gen_loss: 0.058764312
disc_loss: 2.7960403
disc_acc: 0.11458333333333333


	Epoch 6
Training results:
gen_loss: 0.06490139
disc_loss: 2.8139966
disc_acc: 0.08155940594059406

Validation results:
gen_loss: 0.05588234
disc_loss: 2.7765226
disc_acc: 0.06101190476190476


	Epoch 7
Training results:
gen_loss: 0.061553292
disc_loss: 2.79309
disc_acc: 0.07858910891089109

Validation results:
gen_loss: 0.063611664
disc_loss: 2.7749271
disc_acc: 0.05853174603174603


	Epoch 8
Training results:
gen_loss: 0.062293712
disc_loss: 2.8064299
disc_acc: 0.06670792079207921

Validation results:
gen_loss: 0.07660348
disc_loss: 2.7972016
disc_acc: 0.0625


	Epoch 9
Training results:
gen_loss: 0.06421087
disc_loss: 2.8099155
disc_acc: 0.06596534653465347

Validation results:
gen_loss: 0.04797118
disc_loss: 2.8030546
disc_acc: 0.053075396825396824


	Epoch 10
Training results:
gen_loss: 0.0539637
disc_loss: 2.8052342
disc_acc: 0.06448019801980198

Validation results:
gen_loss: 0.051185664
disc_loss: 2.7967346
disc_acc: 0.05853174603174603


	Epoch 11
Training results:
gen_loss: 0.04816583
disc_loss: 2.800792
disc_acc: 0.06646039603960396

Validation results:
gen_loss: 0.051426683
disc_loss: 2.7938032
disc_acc: 0.07390873015873016


	Epoch 12
Training results:
gen_loss: 0.046765424
disc_loss: 2.8023026
disc_acc: 0.061386138613861385

Validation results:
gen_loss: 0.037584696
disc_loss: 2.8107936
disc_acc: 0.05853174603174603


	Epoch 13
Training results:
gen_loss: 0.043554753
disc_loss: 2.8042164
disc_acc: 0.06262376237623762

Validation results:
gen_loss: 0.03996095
disc_loss: 2.7863483
disc_acc: 0.06349206349206349


	Epoch 14
Training results:
gen_loss: 0.037987832
disc_loss: 2.804469
disc_acc: 0.0629950495049505

Validation results:
gen_loss: 0.049115855
disc_loss: 2.800119
disc_acc: 0.06845238095238096


	Epoch 15
Training results:
gen_loss: 0.035795394
disc_loss: 2.8048651
disc_acc: 0.06732673267326733

Validation results:
gen_loss: 0.035046324
disc_loss: 2.8063483
disc_acc: 0.06349206349206349


	Epoch 16
Training results:
gen_loss: 0.03202749
disc_loss: 2.8047771
disc_acc: 0.05891089108910891

Validation results:
gen_loss: 0.024477001
disc_loss: 2.810791
disc_acc: 0.057539682539682536


	Epoch 17
Training results:
gen_loss: 0.033358976
disc_loss: 2.8201182
disc_acc: 0.05866336633663366

Validation results:
gen_loss: 0.05352926
disc_loss: 2.7855768
disc_acc: 0.05853174603174603


	Epoch 18
Training results:
gen_loss: 0.0329486
disc_loss: 2.8099759
disc_acc: 0.05952970297029703

Validation results:
gen_loss: 0.03737835
disc_loss: 2.7909336
disc_acc: 0.05505952380952381


	Epoch 19
Training results:
gen_loss: 0.03186419
disc_loss: 2.8139122
disc_acc: 0.06225247524752475

Validation results:
gen_loss: 0.023201186
disc_loss: 2.8226213
disc_acc: 0.06349206349206349


	Epoch 20
Training results:
gen_loss: 0.025352158
disc_loss: 2.8141253
disc_acc: 0.06324257425742574

Validation results:
gen_loss: 0.01909317
disc_loss: 2.8046513
disc_acc: 0.061507936507936505


	Epoch 21
Training results:
gen_loss: 0.02434941
disc_loss: 2.8125136
disc_acc: 0.06188118811881188

Validation results:
gen_loss: 0.01922462
disc_loss: 2.7954943
disc_acc: 0.05853174603174603


	Epoch 22
Training results:
gen_loss: 0.020337345
disc_loss: 2.8012671
disc_acc: 0.059777227722772275

Validation results:
gen_loss: 0.016804174
disc_loss: 2.7956805
disc_acc: 0.0625


	Epoch 23
Training results:
gen_loss: 0.02212987
disc_loss: 2.819613
disc_acc: 0.06051980198019802

Validation results:
gen_loss: 0.01943072
disc_loss: 2.802627
disc_acc: 0.05853174603174603


	Epoch 24
Training results:
gen_loss: 0.017274497
disc_loss: 2.8057432
disc_acc: 0.06522277227722773

Validation results:
gen_loss: 0.020750424
disc_loss: 2.8258214
disc_acc: 0.06398809523809523


	Epoch 25
Training results:
gen_loss: 0.015144738
disc_loss: 2.8153841
disc_acc: 0.06398514851485149

Validation results:
gen_loss: 0.014226312
disc_loss: 2.8340702
disc_acc: 0.06101190476190476


	Epoch 26
Training results:
gen_loss: 0.015264547
disc_loss: 2.8113282
disc_acc: 0.06324257425742574

Validation results:
gen_loss: 0.008431215
disc_loss: 2.7982557
disc_acc: 0.05704365079365079


	Epoch 27
Training results:
gen_loss: 0.01416089
disc_loss: 2.8132184
disc_acc: 0.0625

Validation results:
gen_loss: 0.011129623
disc_loss: 2.8346515
disc_acc: 0.05704365079365079


	Epoch 28
Training results:
gen_loss: 0.011267524
disc_loss: 2.8155966
disc_acc: 0.059777227722772275

Validation results:
gen_loss: 0.012507596
disc_loss: 2.830222
disc_acc: 0.053075396825396824


	Epoch 29
Training results:
gen_loss: 0.010279955
disc_loss: 2.8189106
disc_acc: 0.06113861386138614

Validation results:
gen_loss: 0.0147538
disc_loss: 2.7967725
disc_acc: 0.07043650793650794


	Epoch 30
Training results:
gen_loss: 0.008486995
disc_loss: 2.8090994
disc_acc: 0.062004950495049505

Validation results:
gen_loss: 0.0074296934
disc_loss: 2.7981403
disc_acc: 0.062003968253968256


	Epoch 31
Training results:
gen_loss: 0.0068570212
disc_loss: 2.8158321
disc_acc: 0.06386138613861386

Validation results:
gen_loss: 0.0059981733
disc_loss: 2.8460255
disc_acc: 0.07093253968253968


	Epoch 32
Training results:
gen_loss: 0.0069421832
disc_loss: 2.8193038
disc_acc: 0.058292079207920795

Validation results:
gen_loss: 0.006927128
disc_loss: 2.7864792
disc_acc: 0.05853174603174603


	Epoch 33
Training results:
gen_loss: 0.006015472
disc_loss: 2.8158016
disc_acc: 0.06188118811881188

Validation results:
gen_loss: 0.0037590715
disc_loss: 2.8255498
disc_acc: 0.06299603174603174


	Epoch 34
Training results:
gen_loss: 0.005231036
disc_loss: 2.8007407
disc_acc: 0.06522277227722773

Validation results:
gen_loss: 0.0044617318
disc_loss: 2.8502805
disc_acc: 0.053075396825396824


	Epoch 35
Training results:
gen_loss: 0.00524744
disc_loss: 2.806117
disc_acc: 0.062004950495049505

Validation results:
gen_loss: 0.0046278997
disc_loss: 2.8190038
disc_acc: 0.062003968253968256


	Epoch 36
Training results:
gen_loss: 0.00459792
disc_loss: 2.8338466
disc_acc: 0.06423267326732673

Validation results:
gen_loss: 0.0051055723
disc_loss: 2.984472
disc_acc: 0.05853174603174603


	Epoch 37
Training results:
gen_loss: 0.0050311172
disc_loss: 2.820968
disc_acc: 0.06188118811881188

Validation results:
gen_loss: 0.008181442
disc_loss: 2.794437
disc_acc: 0.062003968253968256


	Epoch 38
Training results:
gen_loss: 0.0054385904
disc_loss: 2.7991457
disc_acc: 0.0655940594059406

Validation results:
gen_loss: 0.0035690726
disc_loss: 2.7960868
disc_acc: 0.06845238095238096


	Epoch 39
Training results:
gen_loss: 0.004266276
disc_loss: 2.8033073
disc_acc: 0.06373762376237624

Validation results:
gen_loss: 0.0029603252
disc_loss: 2.858236
disc_acc: 0.05853174603174603


	Epoch 40
Training results:
gen_loss: 0.0043064146
disc_loss: 2.8164134
disc_acc: 0.060767326732673266

Validation results:
gen_loss: 0.0022572528
disc_loss: 2.8187091
disc_acc: 0.062003968253968256


	Epoch 41
Training results:
gen_loss: 0.003765227
disc_loss: 2.8118465
disc_acc: 0.06212871287128713

Validation results:
gen_loss: 0.0023898932
disc_loss: 2.7821515
disc_acc: 0.05853174603174603


	Epoch 42
Training results:
gen_loss: 0.0030964147
disc_loss: 2.8142085
disc_acc: 0.06002475247524752

Validation results:
gen_loss: 0.005750106
disc_loss: 2.812865
disc_acc: 0.060515873015873016


	Epoch 43
Training results:
gen_loss: 0.003989097
disc_loss: 2.831296
disc_acc: 0.07091584158415841

Validation results:
gen_loss: 0.006764635
disc_loss: 2.830208
disc_acc: 0.05257936507936508


	Epoch 44
Training results:
gen_loss: 0.0032762373
disc_loss: 2.8158588
disc_acc: 0.057920792079207924

Validation results:
gen_loss: 0.0016807577
disc_loss: 2.798342
disc_acc: 0.05505952380952381


	Epoch 45
Training results:
gen_loss: 0.0016761001
disc_loss: 2.8042877
disc_acc: 0.06423267326732673

Validation results:
gen_loss: 0.0018290621
disc_loss: 2.8006792
disc_acc: 0.06349206349206349


	Epoch 46
Training results:
gen_loss: 0.0015919739
disc_loss: 2.803124
disc_acc: 0.062376237623762376

Validation results:
gen_loss: 0.00091497827
disc_loss: 2.8610394
disc_acc: 0.06845238095238096


	Epoch 47
Training results:
gen_loss: 0.0012911863
disc_loss: 2.8201292
disc_acc: 0.05965346534653465

Validation results:
gen_loss: 0.0011964003
disc_loss: 2.791888
disc_acc: 0.053075396825396824


	Epoch 48
Training results:
gen_loss: 0.0012549646
disc_loss: 2.8132334
disc_acc: 0.061386138613861385

Validation results:
gen_loss: 0.0013127177
disc_loss: 2.822828
disc_acc: 0.062003968253968256


	Epoch 49
Training results:
gen_loss: 0.0010659663
disc_loss: 2.806906
disc_acc: 0.06274752475247525

Validation results:
gen_loss: 0.001594178
disc_loss: 2.785188
disc_acc: 0.07341269841269842


	Epoch 50
Training results:
gen_loss: 0.0012222748
disc_loss: 2.8247535
disc_acc: 0.057920792079207924

Validation results:
gen_loss: 0.0017052629
disc_loss: 2.7813246
disc_acc: 0.06398809523809523


	Epoch 51
Training results:
gen_loss: 0.0011557591
disc_loss: 2.8069258
disc_acc: 0.057673267326732676

Validation results:
gen_loss: 0.0007508822
disc_loss: 2.8111157
disc_acc: 0.057539682539682536


	Epoch 52
Training results:
gen_loss: 0.0037906645
disc_loss: 2.812651
disc_acc: 0.05680693069306931

Validation results:
gen_loss: 0.000952716
disc_loss: 2.7948987
disc_acc: 0.060515873015873016


	Epoch 53
Training results:
gen_loss: 0.00094284327
disc_loss: 2.8146865
disc_acc: 0.05965346534653465

Validation results:
gen_loss: 0.0009546455
disc_loss: 2.7855024
disc_acc: 0.06299603174603174


	Epoch 54
Training results:
gen_loss: 0.001098727
disc_loss: 2.8090458
disc_acc: 0.05841584158415842

Validation results:
gen_loss: 0.0010287265
disc_loss: 2.8203228
disc_acc: 0.06398809523809523


	Epoch 55
Training results:
gen_loss: 0.0010080826
disc_loss: 2.8220668
disc_acc: 0.06064356435643564

Validation results:
gen_loss: 0.0007012768
disc_loss: 2.78946
disc_acc: 0.057539682539682536


	Epoch 56
Training results:
gen_loss: 0.0008528393
disc_loss: 2.8094945
disc_acc: 0.06448019801980198

Validation results:
gen_loss: 0.00066849234
disc_loss: 2.794402
disc_acc: 0.05257936507936508


	Epoch 57
Training results:
gen_loss: 0.0006717059
disc_loss: 2.8030434
disc_acc: 0.05866336633663366

Validation results:
gen_loss: 0.0005907205
disc_loss: 2.7896523
disc_acc: 0.060515873015873016


	Epoch 58
Training results:
gen_loss: 0.0005804915
disc_loss: 2.8247082
disc_acc: 0.06212871287128713

Validation results:
gen_loss: 0.0003634046
disc_loss: 2.8546858
disc_acc: 0.05853174603174603


	Epoch 59
Training results:
gen_loss: 0.00031338853
disc_loss: 2.8172731
disc_acc: 0.062004950495049505

Validation results:
gen_loss: 8.048833e-06
disc_loss: 2.8155653
disc_acc: 0.06299603174603174


	Epoch 60
Training results:
gen_loss: 8.330717e-06
disc_loss: 2.7964282
disc_acc: 0.06398514851485149

Validation results:
gen_loss: 6.7750957e-06
disc_loss: 2.8300462
disc_acc: 0.06001984126984127


	Epoch 61
Training results:
gen_loss: 7.4148047e-06
disc_loss: 2.7997386
disc_acc: 0.06534653465346535

Validation results:
gen_loss: 3.88441e-06
disc_loss: 2.7804146
disc_acc: 0.06349206349206349


	Epoch 62
Training results:
gen_loss: 9.392465e-06
disc_loss: 2.8143873
disc_acc: 0.06311881188118812

Validation results:
gen_loss: 6.7800747e-06
disc_loss: 2.790117
disc_acc: 0.062003968253968256


	Epoch 63
Training results:
gen_loss: 1.0660055e-05
disc_loss: 2.8095212
disc_acc: 0.06188118811881188

Validation results:
gen_loss: 4.601063e-06
disc_loss: 2.789492
disc_acc: 0.062003968253968256


	Epoch 64
Training results:
gen_loss: 4.990118e-06
disc_loss: 2.8091695
disc_acc: 0.06225247524752475

Validation results:
gen_loss: 2.6571918e-06
disc_loss: 2.7789385
disc_acc: 0.06101190476190476


	Epoch 65
Training results:
gen_loss: 3.42441e-06
disc_loss: 2.8040502
disc_acc: 0.06163366336633663

Validation results:
gen_loss: 3.455278e-06
disc_loss: 2.8126245
disc_acc: 0.0689484126984127


	Epoch 66
Training results:
gen_loss: 1.05325225e-05
disc_loss: 2.8186648
disc_acc: 0.06311881188118812

Validation results:
gen_loss: 1.0297426e-05
disc_loss: 2.8267846
disc_acc: 0.05505952380952381


	Epoch 67
Training results:
gen_loss: 4.6104083e-06
disc_loss: 2.8136468
disc_acc: 0.06163366336633663

Validation results:
gen_loss: 5.048634e-06
disc_loss: 2.785356
disc_acc: 0.06398809523809523


	Epoch 68
Training results:
gen_loss: 6.4970845e-06
disc_loss: 2.8085165
disc_acc: 0.06646039603960396

Validation results:
gen_loss: 3.0790147e-06
disc_loss: 2.798108
disc_acc: 0.06349206349206349


	Epoch 69
Training results:
gen_loss: 4.312195e-06
disc_loss: 2.809278
disc_acc: 0.06175742574257426

Validation results:
gen_loss: 3.1133252e-06
disc_loss: 2.8222094
disc_acc: 0.062003968253968256


	Epoch 70
Training results:
gen_loss: 8.92652e-06
disc_loss: 2.8107746
disc_acc: 0.06485148514851485

Validation results:
gen_loss: 3.6014112e-06
disc_loss: 2.85077
disc_acc: 0.05704365079365079


	Epoch 71
Training results:
gen_loss: 3.8616513e-06
disc_loss: 2.805403
disc_acc: 0.061386138613861385

Validation results:
gen_loss: 6.6458497e-06
disc_loss: 2.895651
disc_acc: 0.05704365079365079


	Epoch 72
Training results:
gen_loss: 1.9033125e-05
disc_loss: 2.8319154
disc_acc: 0.06349009900990099

Validation results:
gen_loss: 6.63867e-06
disc_loss: 2.7785137
disc_acc: 0.0689484126984127


	Epoch 73
Training results:
gen_loss: 3.9239803e-06
disc_loss: 2.802574
disc_acc: 0.06311881188118812

Validation results:
gen_loss: 1.5002202e-06
disc_loss: 2.8236456
disc_acc: 0.06349206349206349


	Epoch 74
Training results:
gen_loss: 2.4846622e-06
disc_loss: 2.8021836
disc_acc: 0.06150990099009901

Validation results:
gen_loss: 3.292015e-06
disc_loss: 2.8005762
disc_acc: 0.057539682539682536


	Epoch 75
Training results:
gen_loss: 6.17902e-06
disc_loss: 2.8152363
disc_acc: 0.06175742574257426

Validation results:
gen_loss: 6.7737546e-06
disc_loss: 2.8243697
disc_acc: 0.053075396825396824


	Epoch 76
Training results:
gen_loss: 2.9144187e-06
disc_loss: 2.8071604
disc_acc: 0.06027227722772277

Validation results:
gen_loss: 2.2208278e-06
disc_loss: 2.814445
disc_acc: 0.05803571428571429


	Epoch 77
Training results:
gen_loss: 7.1418117e-06
disc_loss: 2.8170304
disc_acc: 0.05705445544554456

Validation results:
gen_loss: 1.3010074e-06
disc_loss: 2.7972894
disc_acc: 0.05853174603174603


	Epoch 78
Training results:
gen_loss: 2.0151588e-06
disc_loss: 2.800849
disc_acc: 0.06126237623762376

Validation results:
gen_loss: 3.3013273e-06
disc_loss: 2.8423474
disc_acc: 0.05853174603174603


	Epoch 79
Training results:
gen_loss: 6.1870883e-06
disc_loss: 2.8129966
disc_acc: 0.06188118811881188

Validation results:
gen_loss: 7.64563e-05
disc_loss: 2.7959585
disc_acc: 0.05853174603174603


	Epoch 80
Training results:
gen_loss: 1.02292015e-05
disc_loss: 2.8219664
disc_acc: 0.062376237623762376

Validation results:
gen_loss: 4.7461184e-05
disc_loss: 2.798775
disc_acc: 0.05853174603174603


	Epoch 81
Training results:
gen_loss: 0.0001003428
disc_loss: 2.8020542
disc_acc: 0.06225247524752475

Validation results:
gen_loss: 3.5603483e-05
disc_loss: 2.7983415
disc_acc: 0.06398809523809523


	Epoch 82
Training results:
gen_loss: 0.00039078476
disc_loss: 2.8405128
disc_acc: 0.05866336633663366

Validation results:
gen_loss: 7.4642103e-06
disc_loss: 2.7973351
disc_acc: 0.062003968253968256


	Epoch 83
Training results:
gen_loss: 2.9835721e-06
disc_loss: 2.7967117
disc_acc: 0.06064356435643564

Validation results:
gen_loss: 1.5392658e-06
disc_loss: 2.8077176
disc_acc: 0.062003968253968256


	Epoch 84
Training results:
gen_loss: 1.2464328e-06
disc_loss: 2.7959657
disc_acc: 0.06150990099009901

Validation results:
gen_loss: 6.1150575e-07
disc_loss: 2.7806678
disc_acc: 0.062003968253968256


	Epoch 85
Training results:
gen_loss: 1.4768124e-06
disc_loss: 2.8057313
disc_acc: 0.06188118811881188

Validation results:
gen_loss: 4.129781e-06
disc_loss: 2.8090293
disc_acc: 0.06398809523809523


	Epoch 86
Training results:
gen_loss: 3.278573e-06
disc_loss: 2.81536
disc_acc: 0.06534653465346535

Validation results:
gen_loss: 5.3791086e-06
disc_loss: 2.798689
disc_acc: 0.062003968253968256


	Epoch 87
Training results:
gen_loss: 2.6635548e-06
disc_loss: 2.8112803
disc_acc: 0.06311881188118812

Validation results:
gen_loss: 1.4957025e-06
disc_loss: 2.7978752
disc_acc: 0.06001984126984127


	Epoch 88
Training results:
gen_loss: 4.4541544e-06
disc_loss: 2.8087006
disc_acc: 0.06311881188118812

Validation results:
gen_loss: 1.1197422e-05
disc_loss: 2.8373103
disc_acc: 0.07390873015873016


	Epoch 89
Training results:
gen_loss: 5.679008e-06
disc_loss: 2.8093371
disc_acc: 0.06485148514851485

Validation results:
gen_loss: 4.6791583e-06
disc_loss: 2.808599
disc_acc: 0.062003968253968256


	Epoch 90
Training results:
gen_loss: 3.5033058e-06
disc_loss: 2.8094378
disc_acc: 0.06584158415841584

Validation results:
gen_loss: 7.6890274e-07
disc_loss: 2.7910888
disc_acc: 0.05704365079365079


	Epoch 91
Training results:
gen_loss: 2.074699e-06
disc_loss: 2.8100607
disc_acc: 0.06212871287128713

Validation results:
gen_loss: 1.8827852e-06
disc_loss: 2.7848172
disc_acc: 0.057539682539682536


	Epoch 92
Training results:
gen_loss: 3.2545938e-06
disc_loss: 2.8323555
disc_acc: 0.06274752475247525

Validation results:
gen_loss: 8.0708725e-07
disc_loss: 2.8052218
disc_acc: 0.061507936507936505


	Epoch 93
Training results:
gen_loss: 1.2413274e-06
disc_loss: 2.7943745
disc_acc: 0.06410891089108911

Validation results:
gen_loss: 9.301864e-07
disc_loss: 2.8115094
disc_acc: 0.06101190476190476


	Epoch 94
Training results:
gen_loss: 9.2780607e-07
disc_loss: 2.806155
disc_acc: 0.0650990099009901

Validation results:
gen_loss: 1.5100011e-06
disc_loss: 2.83022
disc_acc: 0.062003968253968256


	Epoch 95
Training results:
gen_loss: 1.9151755e-06
disc_loss: 2.8105977
disc_acc: 0.06324257425742574

Validation results:
gen_loss: 8.6750924e-07
disc_loss: 2.7851188
disc_acc: 0.06349206349206349


	Epoch 96
Training results:
gen_loss: 1.3253688e-06
disc_loss: 2.8060367
disc_acc: 0.059158415841584155

Validation results:
gen_loss: 8.1787823e-07
disc_loss: 2.7889202
disc_acc: 0.06398809523809523


	Epoch 97
Training results:
gen_loss: 3.0964757e-06
disc_loss: 2.8057892
disc_acc: 0.06658415841584159

Validation results:
gen_loss: 5.756467e-06
disc_loss: 2.7864325
disc_acc: 0.05853174603174603


	Epoch 98
Training results:
gen_loss: 3.065347e-06
disc_loss: 2.806928
disc_acc: 0.06373762376237624

Validation results:
gen_loss: 1.6196466e-06
disc_loss: 2.7983174
disc_acc: 0.06746031746031746


	Epoch 99
Training results:
gen_loss: 1.5449168e-06
disc_loss: 2.805013
disc_acc: 0.05655940594059406

Validation results:
gen_loss: 1.7910692e-06
disc_loss: 2.818815
disc_acc: 0.052083333333333336


	Epoch 100
Training results:
gen_loss: 4.000113e-06
disc_loss: 2.8242164
disc_acc: 0.05965346534653465

Validation results:
gen_loss: 1.0380828e-06
disc_loss: 2.7997663
disc_acc: 0.05505952380952381



Training new discriminator on static trained discriminator.
	Initial performance
Training results:
gen_loss: 0.021037178
disc_loss: 5.5039525
disc_acc: 0.0

Validation results:
gen_loss: 0.021146376
disc_loss: 5.4834414
disc_acc: 0.0


	Epoch 1
Training results:
gen_loss: 0.0014087153
disc_loss: 3.171708
disc_acc: 0.05965346534653465

Validation results:
gen_loss: 0.0006416503
disc_loss: 3.0786998
disc_acc: 0.060515873015873016


	Epoch 2
Training results:
gen_loss: 0.0013278316
disc_loss: 2.8483677
disc_acc: 0.06547029702970297

Validation results:
gen_loss: 0.0014270693
disc_loss: 2.8036385
disc_acc: 0.05853174603174603


	Epoch 3
Training results:
gen_loss: 0.0020918357
disc_loss: 2.8337903
disc_acc: 0.05903465346534653

Validation results:
gen_loss: 0.0022747412
disc_loss: 2.8379853
disc_acc: 0.060515873015873016


	Epoch 4
Training results:
gen_loss: 0.002168339
disc_loss: 2.827201
disc_acc: 0.061386138613861385

Validation results:
gen_loss: 0.0020833742
disc_loss: 2.8802278
disc_acc: 0.05505952380952381


	Epoch 5
Training results:
gen_loss: 0.0025329783
disc_loss: 2.8142514
disc_acc: 0.0629950495049505

Validation results:
gen_loss: 0.0029268062
disc_loss: 2.813441
disc_acc: 0.07390873015873016


	Epoch 6
Training results:
gen_loss: 0.002483739
disc_loss: 2.818424
disc_acc: 0.06547029702970297

Validation results:
gen_loss: 0.0024779001
disc_loss: 2.8353045
disc_acc: 0.05803571428571429


	Epoch 7
Training results:
gen_loss: 0.0024239714
disc_loss: 2.807126
disc_acc: 0.06089108910891089

Validation results:
gen_loss: 0.0022045795
disc_loss: 2.8216164
disc_acc: 0.06299603174603174


	Epoch 8
Training results:
gen_loss: 0.0027222137
disc_loss: 2.8059127
disc_acc: 0.0629950495049505

Validation results:
gen_loss: 0.0026011264
disc_loss: 2.8076563
disc_acc: 0.07093253968253968


	Epoch 9
Training results:
gen_loss: 0.002776347
disc_loss: 2.8041477
disc_acc: 0.05457920792079208

Validation results:
gen_loss: 0.0032229829
disc_loss: 2.7813766
disc_acc: 0.057539682539682536


	Epoch 10
Training results:
gen_loss: 0.002940687
disc_loss: 2.8040628
disc_acc: 0.0650990099009901

Validation results:
gen_loss: 0.0022726401
disc_loss: 2.7864869
disc_acc: 0.06349206349206349


	Epoch 11
Training results:
gen_loss: 0.0025776608
disc_loss: 2.8113244
disc_acc: 0.06522277227722773

Validation results:
gen_loss: 0.0030359977
disc_loss: 2.795684
disc_acc: 0.053075396825396824


	Epoch 12
Training results:
gen_loss: 0.0028012176
disc_loss: 2.8133838
disc_acc: 0.06126237623762376

Validation results:
gen_loss: 0.0033574167
disc_loss: 2.8316665
disc_acc: 0.06349206349206349


	Epoch 13
Training results:
gen_loss: 0.0032037904
disc_loss: 2.8119879
disc_acc: 0.056683168316831685

Validation results:
gen_loss: 0.0029448129
disc_loss: 2.787591
disc_acc: 0.06101190476190476


	Epoch 14
Training results:
gen_loss: 0.003568795
disc_loss: 2.8099585
disc_acc: 0.06621287128712872

Validation results:
gen_loss: 0.004229261
disc_loss: 2.838799
disc_acc: 0.057539682539682536


	Epoch 15
Training results:
gen_loss: 0.0039773476
disc_loss: 2.8265848
disc_acc: 0.06188118811881188

Validation results:
gen_loss: 0.0038150793
disc_loss: 2.7854552
disc_acc: 0.06398809523809523


	Epoch 16
Training results:
gen_loss: 0.0047564036
disc_loss: 2.8086808
disc_acc: 0.05841584158415842

Validation results:
gen_loss: 0.0046831532
disc_loss: 2.8012006
disc_acc: 0.05704365079365079


	Epoch 17
Training results:
gen_loss: 0.004717184
disc_loss: 2.8100874
disc_acc: 0.06571782178217822

Validation results:
gen_loss: 0.0042056045
disc_loss: 2.7974722
disc_acc: 0.05853174603174603


	Epoch 18
Training results:
gen_loss: 0.0043079136
disc_loss: 2.828586
disc_acc: 0.0629950495049505

Validation results:
gen_loss: 0.0042625857
disc_loss: 2.788035
disc_acc: 0.05853174603174603


	Epoch 19
Training results:
gen_loss: 0.004283308
disc_loss: 2.810242
disc_acc: 0.059777227722772275

Validation results:
gen_loss: 0.0038716341
disc_loss: 2.7982738
disc_acc: 0.053075396825396824


	Epoch 20
Training results:
gen_loss: 0.003878346
disc_loss: 2.802121
disc_acc: 0.06225247524752475

Validation results:
gen_loss: 0.0037347754
disc_loss: 2.787575
disc_acc: 0.05257936507936508


	Epoch 21
Training results:
gen_loss: 0.0037552821
disc_loss: 2.8025424
disc_acc: 0.057920792079207924

Validation results:
gen_loss: 0.0038288462
disc_loss: 2.8486302
disc_acc: 0.06349206349206349


	Epoch 22
Training results:
gen_loss: 0.0038141303
disc_loss: 2.82856
disc_acc: 0.05816831683168317

Validation results:
gen_loss: 0.0038191637
disc_loss: 2.8172617
disc_acc: 0.07093253968253968


	Epoch 23
Training results:
gen_loss: 0.0036487523
disc_loss: 2.8089778
disc_acc: 0.0629950495049505

Validation results:
gen_loss: 0.0039280723
disc_loss: 2.8305767
disc_acc: 0.06349206349206349


	Epoch 24
Training results:
gen_loss: 0.0037087328
disc_loss: 2.8141155
disc_acc: 0.05965346534653465

Validation results:
gen_loss: 0.0041382364
disc_loss: 2.8294668
disc_acc: 0.05853174603174603


	Epoch 25
Training results:
gen_loss: 0.0041319453
disc_loss: 2.8117783
disc_acc: 0.06324257425742574

Validation results:
gen_loss: 0.0044012875
disc_loss: 2.7814395
disc_acc: 0.0689484126984127


	Epoch 26
Training results:
gen_loss: 0.0040357234
disc_loss: 2.8082876
disc_acc: 0.06175742574257426

Validation results:
gen_loss: 0.004005934
disc_loss: 2.8169081
disc_acc: 0.06349206349206349


	Epoch 27
Training results:
gen_loss: 0.003815415
disc_loss: 2.8121645
disc_acc: 0.061014851485148514

Validation results:
gen_loss: 0.0035430728
disc_loss: 2.7848759
disc_acc: 0.06299603174603174


	Epoch 28
Training results:
gen_loss: 0.003728627
disc_loss: 2.7997596
disc_acc: 0.06621287128712872

Validation results:
gen_loss: 0.004361899
disc_loss: 2.7993872
disc_acc: 0.05505952380952381


	Epoch 29
Training results:
gen_loss: 0.003716281
disc_loss: 2.8098216
disc_acc: 0.06485148514851485

Validation results:
gen_loss: 0.0042248396
disc_loss: 2.7897573
disc_acc: 0.057539682539682536


	Epoch 30
Training results:
gen_loss: 0.0043180333
disc_loss: 2.8156044
disc_acc: 0.05928217821782178

Validation results:
gen_loss: 0.004022087
disc_loss: 2.802552
disc_acc: 0.0679563492063492


	Epoch 31
Training results:
gen_loss: 0.003913398
disc_loss: 2.8105402
disc_acc: 0.062004950495049505

Validation results:
gen_loss: 0.003810757
disc_loss: 2.807052
disc_acc: 0.06299603174603174


	Epoch 32
Training results:
gen_loss: 0.0039629163
disc_loss: 2.8137348
disc_acc: 0.05853960396039604

Validation results:
gen_loss: 0.0041450234
disc_loss: 2.8061197
disc_acc: 0.053075396825396824


	Epoch 33
Training results:
gen_loss: 0.0051292693
disc_loss: 2.8048787
disc_acc: 0.0525990099009901

Validation results:
gen_loss: 0.004719222
disc_loss: 2.8167484
disc_acc: 0.05853174603174603


	Epoch 34
Training results:
gen_loss: 0.0037886368
disc_loss: 2.8486357
disc_acc: 0.06596534653465347

Validation results:
gen_loss: 0.0025299876
disc_loss: 2.8883176
disc_acc: 0.05853174603174603


	Epoch 35
Training results:
gen_loss: 0.002832511
disc_loss: 2.797239
disc_acc: 0.06051980198019802

Validation results:
gen_loss: 0.0026738825
disc_loss: 2.7818701
disc_acc: 0.062003968253968256


	Epoch 36
Training results:
gen_loss: 0.0029726499
disc_loss: 2.8160982
disc_acc: 0.06188118811881188

Validation results:
gen_loss: 0.002114091
disc_loss: 2.8393092
disc_acc: 0.057539682539682536


	Epoch 37
Training results:
gen_loss: 0.002819056
disc_loss: 2.8038158
disc_acc: 0.0599009900990099

Validation results:
gen_loss: 0.003763916
disc_loss: 2.7780025
disc_acc: 0.07043650793650794


	Epoch 38
Training results:
gen_loss: 0.0029267515
disc_loss: 2.8005445
disc_acc: 0.06113861386138614

Validation results:
gen_loss: 0.0028534005
disc_loss: 2.7849846
disc_acc: 0.07093253968253968


	Epoch 39
Training results:
gen_loss: 0.0030467869
disc_loss: 2.8033824
disc_acc: 0.06126237623762376

Validation results:
gen_loss: 0.003449141
disc_loss: 2.8134608
disc_acc: 0.061507936507936505


	Epoch 40
Training results:
gen_loss: 0.003069253
disc_loss: 2.8159087
disc_acc: 0.06175742574257426

Validation results:
gen_loss: 0.002223177
disc_loss: 2.8048966
disc_acc: 0.06398809523809523


	Epoch 41
Training results:
gen_loss: 0.0026978895
disc_loss: 2.8063805
disc_acc: 0.058787128712871284

Validation results:
gen_loss: 0.002344051
disc_loss: 2.8224814
disc_acc: 0.054563492063492064


	Epoch 42
Training results:
gen_loss: 0.0025435963
disc_loss: 2.811103
disc_acc: 0.06212871287128713

Validation results:
gen_loss: 0.0017207919
disc_loss: 2.863809
disc_acc: 0.05803571428571429


	Epoch 43
Training results:
gen_loss: 0.0021023585
disc_loss: 2.81439
disc_acc: 0.05705445544554456

Validation results:
gen_loss: 0.0020742817
disc_loss: 2.8142648
disc_acc: 0.053075396825396824


	Epoch 44
Training results:
gen_loss: 0.0021443807
disc_loss: 2.8091972
disc_acc: 0.05841584158415842

Validation results:
gen_loss: 0.0023349905
disc_loss: 2.8052263
disc_acc: 0.062003968253968256


	Epoch 45
Training results:
gen_loss: 0.0021348693
disc_loss: 2.813216
disc_acc: 0.06386138613861386

Validation results:
gen_loss: 0.002391984
disc_loss: 2.7839682
disc_acc: 0.0679563492063492


	Epoch 46
Training results:
gen_loss: 0.0021683746
disc_loss: 2.8019662
disc_acc: 0.06212871287128713

Validation results:
gen_loss: 0.0021144005
disc_loss: 2.7920997
disc_acc: 0.057539682539682536


	Epoch 47
Training results:
gen_loss: 0.0020478894
disc_loss: 2.820191
disc_acc: 0.06002475247524752

Validation results:
gen_loss: 0.002170307
disc_loss: 2.8027253
disc_acc: 0.061507936507936505


	Epoch 48
Training results:
gen_loss: 0.002041211
disc_loss: 2.8080409
disc_acc: 0.060396039603960394

Validation results:
gen_loss: 0.0016177042
disc_loss: 2.81263
disc_acc: 0.057539682539682536


	Epoch 49
Training results:
gen_loss: 0.0019473556
disc_loss: 2.8230708
disc_acc: 0.06225247524752475

Validation results:
gen_loss: 0.0020414281
disc_loss: 2.805028
disc_acc: 0.06349206349206349


	Epoch 50
Training results:
gen_loss: 0.0019256705
disc_loss: 2.7994044
disc_acc: 0.060396039603960394

Validation results:
gen_loss: 0.0018460465
disc_loss: 2.806086
disc_acc: 0.06349206349206349


	Epoch 51
Training results:
gen_loss: 0.0019100572
disc_loss: 2.7977355
disc_acc: 0.05804455445544555

Validation results:
gen_loss: 0.0023416532
disc_loss: 2.7983994
disc_acc: 0.07390873015873016


	Epoch 52
Training results:
gen_loss: 0.0017095986
disc_loss: 2.8396454
disc_acc: 0.06262376237623762

Validation results:
gen_loss: 0.0008181347
disc_loss: 2.8269467
disc_acc: 0.0679563492063492


	Epoch 53
Training results:
gen_loss: 0.0012077738
disc_loss: 2.8028882
disc_acc: 0.06386138613861386

Validation results:
gen_loss: 0.0012084001
disc_loss: 2.8376803
disc_acc: 0.05803571428571429


	Epoch 54
Training results:
gen_loss: 0.001101605
disc_loss: 2.7973237
disc_acc: 0.06386138613861386

Validation results:
gen_loss: 1.4470327e-06
disc_loss: 2.8244271
disc_acc: 0.0625


	Epoch 55
Training results:
gen_loss: 1.4807891e-06
disc_loss: 2.8057141
disc_acc: 0.057920792079207924

Validation results:
gen_loss: 1.7726283e-06
disc_loss: 2.8115864
disc_acc: 0.0689484126984127


	Epoch 56
Training results:
gen_loss: 1.3941242e-06
disc_loss: 2.8074234
disc_acc: 0.06547029702970297

Validation results:
gen_loss: 1.1156516e-06
disc_loss: 2.8775465
disc_acc: 0.053075396825396824


	Epoch 57
Training results:
gen_loss: 1.2199023e-06
disc_loss: 2.8205283
disc_acc: 0.06311881188118812

Validation results:
gen_loss: 7.7859556e-07
disc_loss: 2.8138392
disc_acc: 0.07093253968253968


	Epoch 58
Training results:
gen_loss: 1.0211564e-06
disc_loss: 2.8045506
disc_acc: 0.06188118811881188

Validation results:
gen_loss: 6.7810436e-07
disc_loss: 2.7776113
disc_acc: 0.06845238095238096


	Epoch 59
Training results:
gen_loss: 9.223731e-07
disc_loss: 2.8048549
disc_acc: 0.06027227722772277

Validation results:
gen_loss: 1.3545418e-06
disc_loss: 2.845775
disc_acc: 0.052083333333333336


	Epoch 60
Training results:
gen_loss: 2.3330497e-06
disc_loss: 2.8206365
disc_acc: 0.06448019801980198

Validation results:
gen_loss: 7.5997167e-07
disc_loss: 2.7928355
disc_acc: 0.054563492063492064


	Epoch 61
Training results:
gen_loss: 1.044191e-06
disc_loss: 2.7960725
disc_acc: 0.06051980198019802

Validation results:
gen_loss: 3.5178104e-07
disc_loss: 2.7978122
disc_acc: 0.060515873015873016


	Epoch 62
Training results:
gen_loss: 7.676625e-07
disc_loss: 2.808432
disc_acc: 0.0650990099009901

Validation results:
gen_loss: 5.151365e-07
disc_loss: 2.8344238
disc_acc: 0.05257936507936508


	Epoch 63
Training results:
gen_loss: 8.9842706e-07
disc_loss: 2.8098962
disc_acc: 0.0629950495049505

Validation results:
gen_loss: 8.122323e-07
disc_loss: 2.7932575
disc_acc: 0.061507936507936505


	Epoch 64
Training results:
gen_loss: 1.6688358e-06
disc_loss: 2.8174238
disc_acc: 0.06485148514851485

Validation results:
gen_loss: 7.6668925e-07
disc_loss: 2.7904818
disc_acc: 0.0689484126984127


	Epoch 65
Training results:
gen_loss: 1.0047993e-06
disc_loss: 2.8069718
disc_acc: 0.06435643564356436

Validation results:
gen_loss: 4.3009695e-07
disc_loss: 2.7925572
disc_acc: 0.057539682539682536


	Epoch 66
Training results:
gen_loss: 8.4941547e-07
disc_loss: 2.814591
disc_acc: 0.056064356435643566

Validation results:
gen_loss: 1.0416304e-06
disc_loss: 2.7994745
disc_acc: 0.06349206349206349


	Epoch 67
Training results:
gen_loss: 2.7898584e-06
disc_loss: 2.8138764
disc_acc: 0.06633663366336634

Validation results:
gen_loss: 2.633729e-06
disc_loss: 2.799639
disc_acc: 0.07390873015873016


	Epoch 68
Training results:
gen_loss: 2.967909e-06
disc_loss: 2.816565
disc_acc: 0.060396039603960394

Validation results:
gen_loss: 5.484989e-07
disc_loss: 2.806497
disc_acc: 0.05704365079365079


	Epoch 69
Training results:
gen_loss: 6.126516e-07
disc_loss: 2.7961025
disc_acc: 0.058292079207920795

Validation results:
gen_loss: 9.1973374e-07
disc_loss: 2.7979732
disc_acc: 0.06398809523809523


	Epoch 70
Training results:
gen_loss: 1.8721347e-06
disc_loss: 2.823749
disc_acc: 0.06287128712871287

Validation results:
gen_loss: 9.785306e-07
disc_loss: 2.8818305
disc_acc: 0.06845238095238096


	Epoch 71
Training results:
gen_loss: 1.2830745e-06
disc_loss: 2.8185596
disc_acc: 0.06002475247524752

Validation results:
gen_loss: 1.1609168e-06
disc_loss: 2.7839859
disc_acc: 0.053075396825396824


	Epoch 72
Training results:
gen_loss: 6.231791e-07
disc_loss: 2.8021922
disc_acc: 0.06324257425742574

Validation results:
gen_loss: 7.8635316e-07
disc_loss: 2.7855198
disc_acc: 0.05853174603174603


	Epoch 73
Training results:
gen_loss: 7.4818246e-07
disc_loss: 2.8019857
disc_acc: 0.06175742574257426

Validation results:
gen_loss: 2.5127724e-07
disc_loss: 2.7983165
disc_acc: 0.07093253968253968


	Epoch 74
Training results:
gen_loss: 2.2986706e-06
disc_loss: 2.8296294
disc_acc: 0.05705445544554456

Validation results:
gen_loss: 1.6716468e-06
disc_loss: 2.8584335
disc_acc: 0.07093253968253968


	Epoch 75
Training results:
gen_loss: 1.3748288e-06
disc_loss: 2.8156383
disc_acc: 0.05804455445544555

Validation results:
gen_loss: 7.122343e-07
disc_loss: 2.7804508
disc_acc: 0.057539682539682536


	Epoch 76
Training results:
gen_loss: 6.729627e-07
disc_loss: 2.800763
disc_acc: 0.06262376237623762

Validation results:
gen_loss: 2.6503135e-06
disc_loss: 2.7952695
disc_acc: 0.053075396825396824


	Epoch 77
Training results:
gen_loss: 2.4355782e-06
disc_loss: 2.8165658
disc_acc: 0.06423267326732673

Validation results:
gen_loss: 4.300825e-06
disc_loss: 2.7942324
disc_acc: 0.07390873015873016


	Epoch 78
Training results:
gen_loss: 1.6241299e-06
disc_loss: 2.8084311
disc_acc: 0.06806930693069307

Validation results:
gen_loss: 8.9140127e-07
disc_loss: 2.8256705
disc_acc: 0.05257936507936508


	Epoch 79
Training results:
gen_loss: 1.2660743e-06
disc_loss: 2.8055446
disc_acc: 0.059777227722772275

Validation results:
gen_loss: 7.3937974e-07
disc_loss: 2.7805133
disc_acc: 0.06349206349206349


	Epoch 80
Training results:
gen_loss: 1.0784939e-06
disc_loss: 2.809374
disc_acc: 0.06336633663366337

Validation results:
gen_loss: 2.1361013e-06
disc_loss: 2.9998982
disc_acc: 0.07390873015873016


	Epoch 81
Training results:
gen_loss: 1.4195788e-06
disc_loss: 2.8103817
disc_acc: 0.060148514851485146

Validation results:
gen_loss: 1.63614e-06
disc_loss: 2.8634071
disc_acc: 0.06349206349206349


	Epoch 82
Training results:
gen_loss: 6.888479e-07
disc_loss: 2.8061113
disc_acc: 0.06608910891089109

Validation results:
gen_loss: 2.0937237e-06
disc_loss: 2.7967544
disc_acc: 0.054563492063492064


	Epoch 83
Training results:
gen_loss: 1.1000109e-06
disc_loss: 2.804332
disc_acc: 0.060396039603960394

Validation results:
gen_loss: 4.683365e-07
disc_loss: 2.7878969
disc_acc: 0.057539682539682536


	Epoch 84
Training results:
gen_loss: 6.378949e-07
disc_loss: 2.8060844
disc_acc: 0.06398514851485149

Validation results:
gen_loss: 1.7028354e-06
disc_loss: 2.7882395
disc_acc: 0.06398809523809523


	Epoch 85
Training results:
gen_loss: 2.4268797e-06
disc_loss: 2.8186069
disc_acc: 0.06423267326732673

Validation results:
gen_loss: 5.9448354e-07
disc_loss: 2.7924888
disc_acc: 0.0689484126984127


	Epoch 86
Training results:
gen_loss: 6.724576e-07
disc_loss: 2.7995038
disc_acc: 0.06485148514851485

Validation results:
gen_loss: 5.705444e-07
disc_loss: 2.8207614
disc_acc: 0.05853174603174603


	Epoch 87
Training results:
gen_loss: 1.375199e-06
disc_loss: 2.819836
disc_acc: 0.06522277227722773

Validation results:
gen_loss: 3.9552748e-07
disc_loss: 2.797189
disc_acc: 0.062003968253968256


	Epoch 88
Training results:
gen_loss: 7.6980643e-07
disc_loss: 2.805992
disc_acc: 0.06373762376237624

Validation results:
gen_loss: 4.4168465e-07
disc_loss: 2.8025553
disc_acc: 0.057539682539682536


	Epoch 89
Training results:
gen_loss: 4.1341485e-07
disc_loss: 2.7997804
disc_acc: 0.06534653465346535

Validation results:
gen_loss: 3.7574614e-07
disc_loss: 2.799612
disc_acc: 0.06845238095238096


	Epoch 90
Training results:
gen_loss: 1.0589196e-06
disc_loss: 2.8142955
disc_acc: 0.06051980198019802

Validation results:
gen_loss: 4.83091e-07
disc_loss: 2.8267596
disc_acc: 0.05257936507936508


	Epoch 91
Training results:
gen_loss: 1.4069412e-06
disc_loss: 2.8174775
disc_acc: 0.062004950495049505

Validation results:
gen_loss: 1.2151953e-06
disc_loss: 2.8253522
disc_acc: 0.060515873015873016


	Epoch 92
Training results:
gen_loss: 1.659795e-06
disc_loss: 2.813089
disc_acc: 0.06126237623762376

Validation results:
gen_loss: 4.7080917e-07
disc_loss: 2.8065827
disc_acc: 0.062003968253968256


	Epoch 93
Training results:
gen_loss: 4.279887e-07
disc_loss: 2.8013706
disc_acc: 0.059777227722772275

Validation results:
gen_loss: 7.199508e-07
disc_loss: 2.8297727
disc_acc: 0.05505952380952381


	Epoch 94
Training results:
gen_loss: 7.6405956e-07
disc_loss: 2.8089914
disc_acc: 0.06163366336633663

Validation results:
gen_loss: 7.362485e-07
disc_loss: 2.8045933
disc_acc: 0.05505952380952381


	Epoch 95
Training results:
gen_loss: 1.3098895e-06
disc_loss: 2.8107324
disc_acc: 0.062376237623762376

Validation results:
gen_loss: 2.3114946e-07
disc_loss: 2.8413808
disc_acc: 0.06349206349206349


	Epoch 96
Training results:
gen_loss: 7.6052953e-07
disc_loss: 2.8157666
disc_acc: 0.06757425742574258

Validation results:
gen_loss: 1.4355618e-06
disc_loss: 2.800381
disc_acc: 0.0689484126984127


	Epoch 97
Training results:
gen_loss: 5.6098554e-07
disc_loss: 2.7998216
disc_acc: 0.060148514851485146

Validation results:
gen_loss: 3.2364042e-07
disc_loss: 2.7987692
disc_acc: 0.07390873015873016


	Epoch 98
Training results:
gen_loss: 1.3704234e-06
disc_loss: 2.8237135
disc_acc: 0.05754950495049505

Validation results:
gen_loss: 5.929608e-07
disc_loss: 2.8258612
disc_acc: 0.06398809523809523


	Epoch 99
Training results:
gen_loss: 6.1458104e-07
disc_loss: 2.804463
disc_acc: 0.06373762376237624

Validation results:
gen_loss: 3.5935923e-07
disc_loss: 2.8373234
disc_acc: 0.057539682539682536


	Epoch 100
Training results:
gen_loss: 3.9402627e-07
disc_loss: 2.7956257
disc_acc: 0.06386138613861386

Validation results:
gen_loss: 2.991481e-07
disc_loss: 2.791559
disc_acc: 0.06299603174603174



gen_train_loss: 1.3862987, 2.0140748, 0.29096654, 0.14945215, 0.09930701, 0.078202985, 0.06490139, 0.061553292, 0.062293712, 0.06421087, 0.0539637, 0.04816583, 0.046765424, 0.043554753, 0.037987832, 0.035795394, 0.03202749, 0.033358976, 0.0329486, 0.03186419, 0.025352158, 0.02434941, 0.020337345, 0.02212987, 0.017274497, 0.015144738, 0.015264547, 0.01416089, 0.011267524, 0.010279955, 0.008486995, 0.0068570212, 0.0069421832, 0.006015472, 0.005231036, 0.00524744, 0.00459792, 0.0050311172, 0.0054385904, 0.004266276, 0.0043064146, 0.003765227, 0.0030964147, 0.003989097, 0.0032762373, 0.0016761001, 0.0015919739, 0.0012911863, 0.0012549646, 0.0010659663, 0.0012222748, 0.0011557591, 0.0037906645, 0.00094284327, 0.001098727, 0.0010080826, 0.0008528393, 0.0006717059, 0.0005804915, 0.00031338853, 8.330717e-06, 7.4148047e-06, 9.392465e-06, 1.0660055e-05, 4.990118e-06, 3.42441e-06, 1.05325225e-05, 4.6104083e-06, 6.4970845e-06, 4.312195e-06, 8.92652e-06, 3.8616513e-06, 1.9033125e-05, 3.9239803e-06, 2.4846622e-06, 6.17902e-06, 2.9144187e-06, 7.1418117e-06, 2.0151588e-06, 6.1870883e-06, 1.02292015e-05, 0.0001003428, 0.00039078476, 2.9835721e-06, 1.2464328e-06, 1.4768124e-06, 3.278573e-06, 2.6635548e-06, 4.4541544e-06, 5.679008e-06, 3.5033058e-06, 2.074699e-06, 3.2545938e-06, 1.2413274e-06, 9.2780607e-07, 1.9151755e-06, 1.3253688e-06, 3.0964757e-06, 3.065347e-06, 1.5449168e-06, 4.000113e-06, 0.021037178, 0.0014087153, 0.0013278316, 0.0020918357, 0.002168339, 0.0025329783, 0.002483739, 0.0024239714, 0.0027222137, 0.002776347, 0.002940687, 0.0025776608, 0.0028012176, 0.0032037904, 0.003568795, 0.0039773476, 0.0047564036, 0.004717184, 0.0043079136, 0.004283308, 0.003878346, 0.0037552821, 0.0038141303, 0.0036487523, 0.0037087328, 0.0041319453, 0.0040357234, 0.003815415, 0.003728627, 0.003716281, 0.0043180333, 0.003913398, 0.0039629163, 0.0051292693, 0.0037886368, 0.002832511, 0.0029726499, 0.002819056, 0.0029267515, 0.0030467869, 0.003069253, 0.0026978895, 0.0025435963, 0.0021023585, 0.0021443807, 0.0021348693, 0.0021683746, 0.0020478894, 0.002041211, 0.0019473556, 0.0019256705, 0.0019100572, 0.0017095986, 0.0012077738, 0.001101605, 1.4807891e-06, 1.3941242e-06, 1.2199023e-06, 1.0211564e-06, 9.223731e-07, 2.3330497e-06, 1.044191e-06, 7.676625e-07, 8.9842706e-07, 1.6688358e-06, 1.0047993e-06, 8.4941547e-07, 2.7898584e-06, 2.967909e-06, 6.126516e-07, 1.8721347e-06, 1.2830745e-06, 6.231791e-07, 7.4818246e-07, 2.2986706e-06, 1.3748288e-06, 6.729627e-07, 2.4355782e-06, 1.6241299e-06, 1.2660743e-06, 1.0784939e-06, 1.4195788e-06, 6.888479e-07, 1.1000109e-06, 6.378949e-07, 2.4268797e-06, 6.724576e-07, 1.375199e-06, 7.6980643e-07, 4.1341485e-07, 1.0589196e-06, 1.4069412e-06, 1.659795e-06, 4.279887e-07, 7.6405956e-07, 1.3098895e-06, 7.6052953e-07, 5.6098554e-07, 1.3704234e-06, 6.1458104e-07, 3.9402627e-07
disc_train_loss: 5.53727, 1.7803814, 2.6011825, 2.7354913, 2.7849634, 2.7938545, 2.8139966, 2.79309, 2.8064299, 2.8099155, 2.8052342, 2.800792, 2.8023026, 2.8042164, 2.804469, 2.8048651, 2.8047771, 2.8201182, 2.8099759, 2.8139122, 2.8141253, 2.8125136, 2.8012671, 2.819613, 2.8057432, 2.8153841, 2.8113282, 2.8132184, 2.8155966, 2.8189106, 2.8090994, 2.8158321, 2.8193038, 2.8158016, 2.8007407, 2.806117, 2.8338466, 2.820968, 2.7991457, 2.8033073, 2.8164134, 2.8118465, 2.8142085, 2.831296, 2.8158588, 2.8042877, 2.803124, 2.8201292, 2.8132334, 2.806906, 2.8247535, 2.8069258, 2.812651, 2.8146865, 2.8090458, 2.8220668, 2.8094945, 2.8030434, 2.8247082, 2.8172731, 2.7964282, 2.7997386, 2.8143873, 2.8095212, 2.8091695, 2.8040502, 2.8186648, 2.8136468, 2.8085165, 2.809278, 2.8107746, 2.805403, 2.8319154, 2.802574, 2.8021836, 2.8152363, 2.8071604, 2.8170304, 2.800849, 2.8129966, 2.8219664, 2.8020542, 2.8405128, 2.7967117, 2.7959657, 2.8057313, 2.81536, 2.8112803, 2.8087006, 2.8093371, 2.8094378, 2.8100607, 2.8323555, 2.7943745, 2.806155, 2.8105977, 2.8060367, 2.8057892, 2.806928, 2.805013, 2.8242164, 5.5039525, 3.171708, 2.8483677, 2.8337903, 2.827201, 2.8142514, 2.818424, 2.807126, 2.8059127, 2.8041477, 2.8040628, 2.8113244, 2.8133838, 2.8119879, 2.8099585, 2.8265848, 2.8086808, 2.8100874, 2.828586, 2.810242, 2.802121, 2.8025424, 2.82856, 2.8089778, 2.8141155, 2.8117783, 2.8082876, 2.8121645, 2.7997596, 2.8098216, 2.8156044, 2.8105402, 2.8137348, 2.8048787, 2.8486357, 2.797239, 2.8160982, 2.8038158, 2.8005445, 2.8033824, 2.8159087, 2.8063805, 2.811103, 2.81439, 2.8091972, 2.813216, 2.8019662, 2.820191, 2.8080409, 2.8230708, 2.7994044, 2.7977355, 2.8396454, 2.8028882, 2.7973237, 2.8057141, 2.8074234, 2.8205283, 2.8045506, 2.8048549, 2.8206365, 2.7960725, 2.808432, 2.8098962, 2.8174238, 2.8069718, 2.814591, 2.8138764, 2.816565, 2.7961025, 2.823749, 2.8185596, 2.8021922, 2.8019857, 2.8296294, 2.8156383, 2.800763, 2.8165658, 2.8084311, 2.8055446, 2.809374, 2.8103817, 2.8061113, 2.804332, 2.8060844, 2.8186069, 2.7995038, 2.819836, 2.805992, 2.7997804, 2.8142955, 2.8174775, 2.813089, 2.8013706, 2.8089914, 2.8107324, 2.8157666, 2.7998216, 2.8237135, 2.804463, 2.7956257
disc_train_acc: 0.002103960396039604, 0.6193069306930693, 0.25346534653465347, 0.14764851485148514, 0.10866336633663366, 0.09022277227722772, 0.08155940594059406, 0.07858910891089109, 0.06670792079207921, 0.06596534653465347, 0.06448019801980198, 0.06646039603960396, 0.061386138613861385, 0.06262376237623762, 0.0629950495049505, 0.06732673267326733, 0.05891089108910891, 0.05866336633663366, 0.05952970297029703, 0.06225247524752475, 0.06324257425742574, 0.06188118811881188, 0.059777227722772275, 0.06051980198019802, 0.06522277227722773, 0.06398514851485149, 0.06324257425742574, 0.0625, 0.059777227722772275, 0.06113861386138614, 0.062004950495049505, 0.06386138613861386, 0.058292079207920795, 0.06188118811881188, 0.06522277227722773, 0.062004950495049505, 0.06423267326732673, 0.06188118811881188, 0.0655940594059406, 0.06373762376237624, 0.060767326732673266, 0.06212871287128713, 0.06002475247524752, 0.07091584158415841, 0.057920792079207924, 0.06423267326732673, 0.062376237623762376, 0.05965346534653465, 0.061386138613861385, 0.06274752475247525, 0.057920792079207924, 0.057673267326732676, 0.05680693069306931, 0.05965346534653465, 0.05841584158415842, 0.06064356435643564, 0.06448019801980198, 0.05866336633663366, 0.06212871287128713, 0.062004950495049505, 0.06398514851485149, 0.06534653465346535, 0.06311881188118812, 0.06188118811881188, 0.06225247524752475, 0.06163366336633663, 0.06311881188118812, 0.06163366336633663, 0.06646039603960396, 0.06175742574257426, 0.06485148514851485, 0.061386138613861385, 0.06349009900990099, 0.06311881188118812, 0.06150990099009901, 0.06175742574257426, 0.06027227722772277, 0.05705445544554456, 0.06126237623762376, 0.06188118811881188, 0.062376237623762376, 0.06225247524752475, 0.05866336633663366, 0.06064356435643564, 0.06150990099009901, 0.06188118811881188, 0.06534653465346535, 0.06311881188118812, 0.06311881188118812, 0.06485148514851485, 0.06584158415841584, 0.06212871287128713, 0.06274752475247525, 0.06410891089108911, 0.0650990099009901, 0.06324257425742574, 0.059158415841584155, 0.06658415841584159, 0.06373762376237624, 0.05655940594059406, 0.05965346534653465, 0.0, 0.05965346534653465, 0.06547029702970297, 0.05903465346534653, 0.061386138613861385, 0.0629950495049505, 0.06547029702970297, 0.06089108910891089, 0.0629950495049505, 0.05457920792079208, 0.0650990099009901, 0.06522277227722773, 0.06126237623762376, 0.056683168316831685, 0.06621287128712872, 0.06188118811881188, 0.05841584158415842, 0.06571782178217822, 0.0629950495049505, 0.059777227722772275, 0.06225247524752475, 0.057920792079207924, 0.05816831683168317, 0.0629950495049505, 0.05965346534653465, 0.06324257425742574, 0.06175742574257426, 0.061014851485148514, 0.06621287128712872, 0.06485148514851485, 0.05928217821782178, 0.062004950495049505, 0.05853960396039604, 0.0525990099009901, 0.06596534653465347, 0.06051980198019802, 0.06188118811881188, 0.0599009900990099, 0.06113861386138614, 0.06126237623762376, 0.06175742574257426, 0.058787128712871284, 0.06212871287128713, 0.05705445544554456, 0.05841584158415842, 0.06386138613861386, 0.06212871287128713, 0.06002475247524752, 0.060396039603960394, 0.06225247524752475, 0.060396039603960394, 0.05804455445544555, 0.06262376237623762, 0.06386138613861386, 0.06386138613861386, 0.057920792079207924, 0.06547029702970297, 0.06311881188118812, 0.06188118811881188, 0.06027227722772277, 0.06448019801980198, 0.06051980198019802, 0.0650990099009901, 0.0629950495049505, 0.06485148514851485, 0.06435643564356436, 0.056064356435643566, 0.06633663366336634, 0.060396039603960394, 0.058292079207920795, 0.06287128712871287, 0.06002475247524752, 0.06324257425742574, 0.06175742574257426, 0.05705445544554456, 0.05804455445544555, 0.06262376237623762, 0.06423267326732673, 0.06806930693069307, 0.059777227722772275, 0.06336633663366337, 0.060148514851485146, 0.06608910891089109, 0.060396039603960394, 0.06398514851485149, 0.06423267326732673, 0.06485148514851485, 0.06522277227722773, 0.06373762376237624, 0.06534653465346535, 0.06051980198019802, 0.062004950495049505, 0.06126237623762376, 0.059777227722772275, 0.06163366336633663, 0.062376237623762376, 0.06757425742574258, 0.060148514851485146, 0.05754950495049505, 0.06373762376237624, 0.06386138613861386
gen_val_loss: 1.3850006, 0.48245567, 0.19620453, 0.09023665, 0.083565064, 0.058764312, 0.05588234, 0.063611664, 0.07660348, 0.04797118, 0.051185664, 0.051426683, 0.037584696, 0.03996095, 0.049115855, 0.035046324, 0.024477001, 0.05352926, 0.03737835, 0.023201186, 0.01909317, 0.01922462, 0.016804174, 0.01943072, 0.020750424, 0.014226312, 0.008431215, 0.011129623, 0.012507596, 0.0147538, 0.0074296934, 0.0059981733, 0.006927128, 0.0037590715, 0.0044617318, 0.0046278997, 0.0051055723, 0.008181442, 0.0035690726, 0.0029603252, 0.0022572528, 0.0023898932, 0.005750106, 0.006764635, 0.0016807577, 0.0018290621, 0.00091497827, 0.0011964003, 0.0013127177, 0.001594178, 0.0017052629, 0.0007508822, 0.000952716, 0.0009546455, 0.0010287265, 0.0007012768, 0.00066849234, 0.0005907205, 0.0003634046, 8.048833e-06, 6.7750957e-06, 3.88441e-06, 6.7800747e-06, 4.601063e-06, 2.6571918e-06, 3.455278e-06, 1.0297426e-05, 5.048634e-06, 3.0790147e-06, 3.1133252e-06, 3.6014112e-06, 6.6458497e-06, 6.63867e-06, 1.5002202e-06, 3.292015e-06, 6.7737546e-06, 2.2208278e-06, 1.3010074e-06, 3.3013273e-06, 7.64563e-05, 4.7461184e-05, 3.5603483e-05, 7.4642103e-06, 1.5392658e-06, 6.1150575e-07, 4.129781e-06, 5.3791086e-06, 1.4957025e-06, 1.1197422e-05, 4.6791583e-06, 7.6890274e-07, 1.8827852e-06, 8.0708725e-07, 9.301864e-07, 1.5100011e-06, 8.6750924e-07, 8.1787823e-07, 5.756467e-06, 1.6196466e-06, 1.7910692e-06, 1.0380828e-06, 0.021146376, 0.0006416503, 0.0014270693, 0.0022747412, 0.0020833742, 0.0029268062, 0.0024779001, 0.0022045795, 0.0026011264, 0.0032229829, 0.0022726401, 0.0030359977, 0.0033574167, 0.0029448129, 0.004229261, 0.0038150793, 0.0046831532, 0.0042056045, 0.0042625857, 0.0038716341, 0.0037347754, 0.0038288462, 0.0038191637, 0.0039280723, 0.0041382364, 0.0044012875, 0.004005934, 0.0035430728, 0.004361899, 0.0042248396, 0.004022087, 0.003810757, 0.0041450234, 0.004719222, 0.0025299876, 0.0026738825, 0.002114091, 0.003763916, 0.0028534005, 0.003449141, 0.002223177, 0.002344051, 0.0017207919, 0.0020742817, 0.0023349905, 0.002391984, 0.0021144005, 0.002170307, 0.0016177042, 0.0020414281, 0.0018460465, 0.0023416532, 0.0008181347, 0.0012084001, 1.4470327e-06, 1.7726283e-06, 1.1156516e-06, 7.7859556e-07, 6.7810436e-07, 1.3545418e-06, 7.5997167e-07, 3.5178104e-07, 5.151365e-07, 8.122323e-07, 7.6668925e-07, 4.3009695e-07, 1.0416304e-06, 2.633729e-06, 5.484989e-07, 9.1973374e-07, 9.785306e-07, 1.1609168e-06, 7.8635316e-07, 2.5127724e-07, 1.6716468e-06, 7.122343e-07, 2.6503135e-06, 4.300825e-06, 8.9140127e-07, 7.3937974e-07, 2.1361013e-06, 1.63614e-06, 2.0937237e-06, 4.683365e-07, 1.7028354e-06, 5.9448354e-07, 5.705444e-07, 3.9552748e-07, 4.4168465e-07, 3.7574614e-07, 4.83091e-07, 1.2151953e-06, 4.7080917e-07, 7.199508e-07, 7.362485e-07, 2.3114946e-07, 1.4355618e-06, 3.2364042e-07, 5.929608e-07, 3.5935923e-07, 2.991481e-07
disc_val_loss: 5.5387955, 1.7443458, 2.5961096, 2.834205, 2.7240977, 2.7960403, 2.7765226, 2.7749271, 2.7972016, 2.8030546, 2.7967346, 2.7938032, 2.8107936, 2.7863483, 2.800119, 2.8063483, 2.810791, 2.7855768, 2.7909336, 2.8226213, 2.8046513, 2.7954943, 2.7956805, 2.802627, 2.8258214, 2.8340702, 2.7982557, 2.8346515, 2.830222, 2.7967725, 2.7981403, 2.8460255, 2.7864792, 2.8255498, 2.8502805, 2.8190038, 2.984472, 2.794437, 2.7960868, 2.858236, 2.8187091, 2.7821515, 2.812865, 2.830208, 2.798342, 2.8006792, 2.8610394, 2.791888, 2.822828, 2.785188, 2.7813246, 2.8111157, 2.7948987, 2.7855024, 2.8203228, 2.78946, 2.794402, 2.7896523, 2.8546858, 2.8155653, 2.8300462, 2.7804146, 2.790117, 2.789492, 2.7789385, 2.8126245, 2.8267846, 2.785356, 2.798108, 2.8222094, 2.85077, 2.895651, 2.7785137, 2.8236456, 2.8005762, 2.8243697, 2.814445, 2.7972894, 2.8423474, 2.7959585, 2.798775, 2.7983415, 2.7973351, 2.8077176, 2.7806678, 2.8090293, 2.798689, 2.7978752, 2.8373103, 2.808599, 2.7910888, 2.7848172, 2.8052218, 2.8115094, 2.83022, 2.7851188, 2.7889202, 2.7864325, 2.7983174, 2.818815, 2.7997663, 5.4834414, 3.0786998, 2.8036385, 2.8379853, 2.8802278, 2.813441, 2.8353045, 2.8216164, 2.8076563, 2.7813766, 2.7864869, 2.795684, 2.8316665, 2.787591, 2.838799, 2.7854552, 2.8012006, 2.7974722, 2.788035, 2.7982738, 2.787575, 2.8486302, 2.8172617, 2.8305767, 2.8294668, 2.7814395, 2.8169081, 2.7848759, 2.7993872, 2.7897573, 2.802552, 2.807052, 2.8061197, 2.8167484, 2.8883176, 2.7818701, 2.8393092, 2.7780025, 2.7849846, 2.8134608, 2.8048966, 2.8224814, 2.863809, 2.8142648, 2.8052263, 2.7839682, 2.7920997, 2.8027253, 2.81263, 2.805028, 2.806086, 2.7983994, 2.8269467, 2.8376803, 2.8244271, 2.8115864, 2.8775465, 2.8138392, 2.7776113, 2.845775, 2.7928355, 2.7978122, 2.8344238, 2.7932575, 2.7904818, 2.7925572, 2.7994745, 2.799639, 2.806497, 2.7979732, 2.8818305, 2.7839859, 2.7855198, 2.7983165, 2.8584335, 2.7804508, 2.7952695, 2.7942324, 2.8256705, 2.7805133, 2.9998982, 2.8634071, 2.7967544, 2.7878969, 2.7882395, 2.7924888, 2.8207614, 2.797189, 2.8025553, 2.799612, 2.8267596, 2.8253522, 2.8065827, 2.8297727, 2.8045933, 2.8413808, 2.800381, 2.7987692, 2.8258612, 2.8373234, 2.791559
disc_val_acc: 0.004464285714285714, 0.5243055555555556, 0.2584325396825397, 0.21081349206349206, 0.061507936507936505, 0.11458333333333333, 0.06101190476190476, 0.05853174603174603, 0.0625, 0.053075396825396824, 0.05853174603174603, 0.07390873015873016, 0.05853174603174603, 0.06349206349206349, 0.06845238095238096, 0.06349206349206349, 0.057539682539682536, 0.05853174603174603, 0.05505952380952381, 0.06349206349206349, 0.061507936507936505, 0.05853174603174603, 0.0625, 0.05853174603174603, 0.06398809523809523, 0.06101190476190476, 0.05704365079365079, 0.05704365079365079, 0.053075396825396824, 0.07043650793650794, 0.062003968253968256, 0.07093253968253968, 0.05853174603174603, 0.06299603174603174, 0.053075396825396824, 0.062003968253968256, 0.05853174603174603, 0.062003968253968256, 0.06845238095238096, 0.05853174603174603, 0.062003968253968256, 0.05853174603174603, 0.060515873015873016, 0.05257936507936508, 0.05505952380952381, 0.06349206349206349, 0.06845238095238096, 0.053075396825396824, 0.062003968253968256, 0.07341269841269842, 0.06398809523809523, 0.057539682539682536, 0.060515873015873016, 0.06299603174603174, 0.06398809523809523, 0.057539682539682536, 0.05257936507936508, 0.060515873015873016, 0.05853174603174603, 0.06299603174603174, 0.06001984126984127, 0.06349206349206349, 0.062003968253968256, 0.062003968253968256, 0.06101190476190476, 0.0689484126984127, 0.05505952380952381, 0.06398809523809523, 0.06349206349206349, 0.062003968253968256, 0.05704365079365079, 0.05704365079365079, 0.0689484126984127, 0.06349206349206349, 0.057539682539682536, 0.053075396825396824, 0.05803571428571429, 0.05853174603174603, 0.05853174603174603, 0.05853174603174603, 0.05853174603174603, 0.06398809523809523, 0.062003968253968256, 0.062003968253968256, 0.062003968253968256, 0.06398809523809523, 0.062003968253968256, 0.06001984126984127, 0.07390873015873016, 0.062003968253968256, 0.05704365079365079, 0.057539682539682536, 0.061507936507936505, 0.06101190476190476, 0.062003968253968256, 0.06349206349206349, 0.06398809523809523, 0.05853174603174603, 0.06746031746031746, 0.052083333333333336, 0.05505952380952381, 0.0, 0.060515873015873016, 0.05853174603174603, 0.060515873015873016, 0.05505952380952381, 0.07390873015873016, 0.05803571428571429, 0.06299603174603174, 0.07093253968253968, 0.057539682539682536, 0.06349206349206349, 0.053075396825396824, 0.06349206349206349, 0.06101190476190476, 0.057539682539682536, 0.06398809523809523, 0.05704365079365079, 0.05853174603174603, 0.05853174603174603, 0.053075396825396824, 0.05257936507936508, 0.06349206349206349, 0.07093253968253968, 0.06349206349206349, 0.05853174603174603, 0.0689484126984127, 0.06349206349206349, 0.06299603174603174, 0.05505952380952381, 0.057539682539682536, 0.0679563492063492, 0.06299603174603174, 0.053075396825396824, 0.05853174603174603, 0.05853174603174603, 0.062003968253968256, 0.057539682539682536, 0.07043650793650794, 0.07093253968253968, 0.061507936507936505, 0.06398809523809523, 0.054563492063492064, 0.05803571428571429, 0.053075396825396824, 0.062003968253968256, 0.0679563492063492, 0.057539682539682536, 0.061507936507936505, 0.057539682539682536, 0.06349206349206349, 0.06349206349206349, 0.07390873015873016, 0.0679563492063492, 0.05803571428571429, 0.0625, 0.0689484126984127, 0.053075396825396824, 0.07093253968253968, 0.06845238095238096, 0.052083333333333336, 0.054563492063492064, 0.060515873015873016, 0.05257936507936508, 0.061507936507936505, 0.0689484126984127, 0.057539682539682536, 0.06349206349206349, 0.07390873015873016, 0.05704365079365079, 0.06398809523809523, 0.06845238095238096, 0.053075396825396824, 0.05853174603174603, 0.07093253968253968, 0.07093253968253968, 0.057539682539682536, 0.053075396825396824, 0.07390873015873016, 0.05257936507936508, 0.06349206349206349, 0.07390873015873016, 0.06349206349206349, 0.054563492063492064, 0.057539682539682536, 0.06398809523809523, 0.0689484126984127, 0.05853174603174603, 0.062003968253968256, 0.057539682539682536, 0.06845238095238096, 0.05257936507936508, 0.060515873015873016, 0.062003968253968256, 0.05505952380952381, 0.05505952380952381, 0.06349206349206349, 0.0689484126984127, 0.07390873015873016, 0.06398809523809523, 0.057539682539682536, 0.06299603174603174

