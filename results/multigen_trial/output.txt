Beginning trial described in ./config/multigen_trial.json.
Experiment type: multiple generators each corresponding to 1 key.
Experiment settings:
	byte: 0
	keys: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15]
	key_dataset_kwargs:
		keep_data_in_memory: True
		data_path: ./data
		download: True
		extract: True
		preprocess: True
		delete_download_after_extraction: False
		delete_extracted_after_preprocess: False
	dataloader_kwargs:
		batch_size: 16
		shuffle: True
	dataset_prop_for_validation: 0.2
	trace_map_constructor: None
	trace_map_kwargs:
		layers: [256]
		hidden_activation: <class 'torch.nn.modules.activation.ReLU'>
	plaintext_map_constructor: None
	plaintext_map_kwargs:
		layers: [64]
		hidden_activation: <class 'torch.nn.modules.activation.ReLU'>
	key_map_constructor: <function get_zero_map at 0x7f94a8c1b3a0>
	key_map_kwargs:
	cumulative_map_constructor: None
	cumulative_map_kwargs:
		layers: [256]
		hidden_activation: <class 'torch.nn.modules.activation.ReLU'>
	discriminator_constructor: <function get_google_style_resnet_discriminator at 0x7f94a56aa820>
	discriminator_kwargs:
	discriminator_loss_constructor: <class 'torch.nn.modules.loss.CrossEntropyLoss'>
	discriminator_loss_kwargs:
	discriminator_optimizer_constructor: <class 'torch.optim.adam.Adam'>
	discriminator_optimizer_kwargs:
	generator_loss_constructor: <class 'loss_functions.BatchStdLoss'>
	generator_loss_kwargs:
	generator_optimizer_constructor: <class 'torch.optim.adam.Adam'>
	generator_optimizer_kwargs:
	device: cuda
	discriminator_pretraining_epochs: 0
	generator_pretraining_epochs: 0
	gan_training_epochs: 500
	discriminator_posttraining_epochs: 500
	seed: 0
Loading datasets.
AesKeyGroupDataset:
	Available keys: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15]
	Key transform: Compose(
    IntToBinary()
    ToTensor1D()
)
	Byte: 0
	Number of samples available: 10112
	Trace size: torch.Size([1, 3000])
	Key size: torch.Size([1, 8])
	Plaintext size: torch.Size([1, 8])
	Key index size: ()
Constructing generator.
KeyOnlyGenerator(
  (key_trace_map): Sequential(
    (0): Flatten(start_dim=1, end_dim=-1)
    (1): Linear(in_features=8, out_features=8, bias=False)
    (2): Linear(in_features=8, out_features=3000, bias=False)
    (3): Unflatten(dim=-1, unflattened_size=torch.Size([1, 3000]))
  )
)

Constructing discriminator.
Discriminator(
  (model): Sequential(
    (0): MaxPool1d(kernel_size=4, stride=4, padding=0, dilation=1, ceil_mode=False)
    (1): Stack(
      (F): Sequential(
        (0): Block(
          (F): Sequential(
            (0): BatchNorm1d(1, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (1): ReLU()
            (2): Conv1d(1, 16, kernel_size=(1,), stride=(1,), bias=False)
            (3): BatchNorm1d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (4): ReLU()
            (5): Conv1d(16, 16, kernel_size=(3,), stride=(1,), padding=(1,), bias=False)
            (6): BatchNorm1d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (7): ReLU()
            (8): Conv1d(16, 64, kernel_size=(1,), stride=(1,))
          )
          (Id): Conv1d(1, 64, kernel_size=(1,), stride=(1,))
        )
        (1): Block(
          (F): Sequential(
            (0): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (1): ReLU()
            (2): Conv1d(64, 16, kernel_size=(1,), stride=(1,), bias=False)
            (3): BatchNorm1d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (4): ReLU()
            (5): Conv1d(16, 16, kernel_size=(3,), stride=(1,), padding=(1,), bias=False)
            (6): BatchNorm1d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (7): ReLU()
            (8): Conv1d(16, 64, kernel_size=(1,), stride=(1,))
          )
          (Id): Identity()
        )
        (2): Block(
          (F): Sequential(
            (0): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (1): ReLU()
            (2): Conv1d(64, 16, kernel_size=(1,), stride=(1,), bias=False)
            (3): BatchNorm1d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (4): ReLU()
            (5): Conv1d(16, 16, kernel_size=(3,), stride=(2,), padding=(1,), bias=False)
            (6): BatchNorm1d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (7): ReLU()
            (8): Conv1d(16, 64, kernel_size=(1,), stride=(1,))
          )
          (Id): MaxPool1d(kernel_size=1, stride=2, padding=0, dilation=1, ceil_mode=False)
        )
      )
    )
    (2): Stack(
      (F): Sequential(
        (0): Block(
          (F): Sequential(
            (0): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (1): ReLU()
            (2): Conv1d(64, 32, kernel_size=(1,), stride=(1,), bias=False)
            (3): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (4): ReLU()
            (5): Conv1d(32, 32, kernel_size=(3,), stride=(1,), padding=(1,), bias=False)
            (6): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (7): ReLU()
            (8): Conv1d(32, 128, kernel_size=(1,), stride=(1,))
          )
          (Id): Conv1d(64, 128, kernel_size=(1,), stride=(1,))
        )
        (1): Block(
          (F): Sequential(
            (0): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (1): ReLU()
            (2): Conv1d(128, 32, kernel_size=(1,), stride=(1,), bias=False)
            (3): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (4): ReLU()
            (5): Conv1d(32, 32, kernel_size=(3,), stride=(1,), padding=(1,), bias=False)
            (6): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (7): ReLU()
            (8): Conv1d(32, 128, kernel_size=(1,), stride=(1,))
          )
          (Id): Identity()
        )
        (2): Block(
          (F): Sequential(
            (0): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (1): ReLU()
            (2): Conv1d(128, 32, kernel_size=(1,), stride=(1,), bias=False)
            (3): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (4): ReLU()
            (5): Conv1d(32, 32, kernel_size=(3,), stride=(1,), padding=(1,), bias=False)
            (6): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (7): ReLU()
            (8): Conv1d(32, 128, kernel_size=(1,), stride=(1,))
          )
          (Id): Identity()
        )
        (3): Block(
          (F): Sequential(
            (0): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (1): ReLU()
            (2): Conv1d(128, 32, kernel_size=(1,), stride=(1,), bias=False)
            (3): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (4): ReLU()
            (5): Conv1d(32, 32, kernel_size=(3,), stride=(2,), padding=(1,), bias=False)
            (6): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (7): ReLU()
            (8): Conv1d(32, 128, kernel_size=(1,), stride=(1,))
          )
          (Id): MaxPool1d(kernel_size=1, stride=2, padding=0, dilation=1, ceil_mode=False)
        )
      )
    )
    (3): Stack(
      (F): Sequential(
        (0): Block(
          (F): Sequential(
            (0): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (1): ReLU()
            (2): Conv1d(128, 64, kernel_size=(1,), stride=(1,), bias=False)
            (3): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (4): ReLU()
            (5): Conv1d(64, 64, kernel_size=(3,), stride=(1,), padding=(1,), bias=False)
            (6): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (7): ReLU()
            (8): Conv1d(64, 256, kernel_size=(1,), stride=(1,))
          )
          (Id): Conv1d(128, 256, kernel_size=(1,), stride=(1,))
        )
        (1): Block(
          (F): Sequential(
            (0): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (1): ReLU()
            (2): Conv1d(256, 64, kernel_size=(1,), stride=(1,), bias=False)
            (3): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (4): ReLU()
            (5): Conv1d(64, 64, kernel_size=(3,), stride=(1,), padding=(1,), bias=False)
            (6): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (7): ReLU()
            (8): Conv1d(64, 256, kernel_size=(1,), stride=(1,))
          )
          (Id): Identity()
        )
        (2): Block(
          (F): Sequential(
            (0): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (1): ReLU()
            (2): Conv1d(256, 64, kernel_size=(1,), stride=(1,), bias=False)
            (3): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (4): ReLU()
            (5): Conv1d(64, 64, kernel_size=(3,), stride=(1,), padding=(1,), bias=False)
            (6): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (7): ReLU()
            (8): Conv1d(64, 256, kernel_size=(1,), stride=(1,))
          )
          (Id): Identity()
        )
        (3): Block(
          (F): Sequential(
            (0): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (1): ReLU()
            (2): Conv1d(256, 64, kernel_size=(1,), stride=(1,), bias=False)
            (3): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (4): ReLU()
            (5): Conv1d(64, 64, kernel_size=(3,), stride=(2,), padding=(1,), bias=False)
            (6): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (7): ReLU()
            (8): Conv1d(64, 256, kernel_size=(1,), stride=(1,))
          )
          (Id): MaxPool1d(kernel_size=1, stride=2, padding=0, dilation=1, ceil_mode=False)
        )
      )
    )
    (4): Stack(
      (F): Sequential(
        (0): Block(
          (F): Sequential(
            (0): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (1): ReLU()
            (2): Conv1d(256, 128, kernel_size=(1,), stride=(1,), bias=False)
            (3): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (4): ReLU()
            (5): Conv1d(128, 128, kernel_size=(3,), stride=(1,), padding=(1,), bias=False)
            (6): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (7): ReLU()
            (8): Conv1d(128, 512, kernel_size=(1,), stride=(1,))
          )
          (Id): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
        )
        (1): Block(
          (F): Sequential(
            (0): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (1): ReLU()
            (2): Conv1d(512, 128, kernel_size=(1,), stride=(1,), bias=False)
            (3): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (4): ReLU()
            (5): Conv1d(128, 128, kernel_size=(3,), stride=(1,), padding=(1,), bias=False)
            (6): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (7): ReLU()
            (8): Conv1d(128, 512, kernel_size=(1,), stride=(1,))
          )
          (Id): Identity()
        )
        (2): Block(
          (F): Sequential(
            (0): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (1): ReLU()
            (2): Conv1d(512, 128, kernel_size=(1,), stride=(1,), bias=False)
            (3): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (4): ReLU()
            (5): Conv1d(128, 128, kernel_size=(3,), stride=(2,), padding=(1,), bias=False)
            (6): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (7): ReLU()
            (8): Conv1d(128, 512, kernel_size=(1,), stride=(1,))
          )
          (Id): MaxPool1d(kernel_size=1, stride=2, padding=0, dilation=1, ceil_mode=False)
        )
      )
    )
    (5): Flatten(start_dim=1, end_dim=-1)
    (6): LazyLinear(in_features=0, out_features=256, bias=True)
  )
)

Calculating initial results.
Training results:
gen_loss: 0.004578718
disc_loss: 5.5712934
disc_acc: 0.0

Validation results:
gen_loss: 0.0045252773
disc_loss: 5.570874
disc_acc: 0.0


Training discriminator and generator simultaneously.
	Epoch 1
Training results:
gen_loss: 2.5651238
disc_loss: 2.1467245
disc_acc: 0.42264851485148514

Validation results:
gen_loss: 2.7682154
disc_loss: 1.0818641
disc_acc: 0.5426587301587301


	Epoch 2
Training results:
gen_loss: 3.5892026
disc_loss: 0.94879866
disc_acc: 0.6491336633663366

Validation results:
gen_loss: 4.4339967
disc_loss: 0.98817426
disc_acc: 0.6607142857142857


	Epoch 3
Training results:
gen_loss: 4.7076006
disc_loss: 0.7655655
disc_acc: 0.7206683168316832

Validation results:
gen_loss: 5.242852
disc_loss: 0.58267856
disc_acc: 0.7688492063492064


	Epoch 4
Training results:
gen_loss: 5.843518
disc_loss: 0.5810537
disc_acc: 0.7930693069306931

Validation results:
gen_loss: 5.761262
disc_loss: 0.81593543
disc_acc: 0.7286706349206349


	Epoch 5
Training results:
gen_loss: 6.1834497
disc_loss: 0.55227727
disc_acc: 0.8077970297029703

Validation results:
gen_loss: 6.434574
disc_loss: 0.5491013
disc_acc: 0.8035714285714286


	Epoch 6
Training results:
gen_loss: 6.272717
disc_loss: 0.4836948
disc_acc: 0.8299504950495049

Validation results:
gen_loss: 6.8248534
disc_loss: 0.37975955
disc_acc: 0.8561507936507936


	Epoch 7
Training results:
gen_loss: 7.5607276
disc_loss: 0.43686226
disc_acc: 0.848019801980198

Validation results:
gen_loss: 8.603017
disc_loss: 0.56963384
disc_acc: 0.814484126984127


	Epoch 8
Training results:
gen_loss: 7.410952
disc_loss: 0.43814793
disc_acc: 0.8485148514851485

Validation results:
gen_loss: 7.7129517
disc_loss: 0.6845771
disc_acc: 0.8030753968253969


	Epoch 9
Training results:
gen_loss: 6.979266
disc_loss: 0.3721705
disc_acc: 0.8705445544554455

Validation results:
gen_loss: 9.391827
disc_loss: 0.6158537
disc_acc: 0.814484126984127


	Epoch 10
Training results:
gen_loss: 7.717928
disc_loss: 0.4028062
disc_acc: 0.8650990099009901

Validation results:
gen_loss: 9.390971
disc_loss: 0.49519137
disc_acc: 0.8616071428571429


	Epoch 11
Training results:
gen_loss: 8.110597
disc_loss: 0.33961436
disc_acc: 0.8797029702970297

Validation results:
gen_loss: 8.582453
disc_loss: 0.46413246
disc_acc: 0.8492063492063492


	Epoch 12
Training results:
gen_loss: 9.228245
disc_loss: 0.38082865
disc_acc: 0.871410891089109

Validation results:
gen_loss: 8.927569
disc_loss: 0.5871805
disc_acc: 0.814484126984127


	Epoch 13
Training results:
gen_loss: 10.095384
disc_loss: 0.35541853
disc_acc: 0.8836633663366337

Validation results:
gen_loss: 9.689357
disc_loss: 0.44459343
disc_acc: 0.8482142857142857


	Epoch 14
Training results:
gen_loss: 9.881261
disc_loss: 0.27477947
disc_acc: 0.9092821782178218

Validation results:
gen_loss: 11.568691
disc_loss: 0.5768041
disc_acc: 0.8387896825396826


	Epoch 15
Training results:
gen_loss: 11.510379
disc_loss: 0.3048587
disc_acc: 0.8997524752475248

Validation results:
gen_loss: 15.012876
disc_loss: 0.40207115
disc_acc: 0.8824404761904762


	Epoch 16
Training results:
gen_loss: 11.226344
disc_loss: 0.34341612
disc_acc: 0.8925742574257426

Validation results:
gen_loss: 11.223435
disc_loss: 0.44608358
disc_acc: 0.871031746031746


	Epoch 17
Training results:
gen_loss: 11.738682
disc_loss: 0.25468165
disc_acc: 0.9162128712871287

Validation results:
gen_loss: 11.937678
disc_loss: 0.5967064
disc_acc: 0.847718253968254


	Epoch 18
Training results:
gen_loss: 11.143848
disc_loss: 0.24350905
disc_acc: 0.9191831683168317

Validation results:
gen_loss: 9.532513
disc_loss: 0.44329438
disc_acc: 0.8799603174603174


	Epoch 19
Training results:
gen_loss: 11.908234
disc_loss: 0.2568004
disc_acc: 0.9210396039603961

Validation results:
gen_loss: 12.521753
disc_loss: 0.4012807
disc_acc: 0.8819444444444444


	Epoch 20
Training results:
gen_loss: 12.817752
disc_loss: 0.36082673
disc_acc: 0.8944306930693069

Validation results:
gen_loss: 12.713649
disc_loss: 0.37489593
disc_acc: 0.8869047619047619


	Epoch 21
Training results:
gen_loss: 11.872915
disc_loss: 0.21173073
disc_acc: 0.9327970297029703

Validation results:
gen_loss: 10.701234
disc_loss: 0.31851995
disc_acc: 0.9057539682539683


	Epoch 22
Training results:
gen_loss: 12.05871
disc_loss: 0.24576665
disc_acc: 0.9258663366336634

Validation results:
gen_loss: 11.543612
disc_loss: 0.6213593
disc_acc: 0.8313492063492064


	Epoch 23
Training results:
gen_loss: 12.311412
disc_loss: 0.20501523
disc_acc: 0.936509900990099

Validation results:
gen_loss: 11.564173
disc_loss: 0.44888964
disc_acc: 0.8878968253968254


	Epoch 24
Training results:
gen_loss: 12.686986
disc_loss: 0.23693956
disc_acc: 0.9295792079207921

Validation results:
gen_loss: 12.081355
disc_loss: 0.5303208
disc_acc: 0.8804563492063492


	Epoch 25
Training results:
gen_loss: 12.27307
disc_loss: 0.21543144
disc_acc: 0.9337871287128713

Validation results:
gen_loss: 12.776858
disc_loss: 0.48276952
disc_acc: 0.8973214285714286


	Epoch 26
Training results:
gen_loss: 12.99272
disc_loss: 0.27984312
disc_acc: 0.9191831683168317

Validation results:
gen_loss: 13.727044
disc_loss: 0.4829996
disc_acc: 0.8725198412698413


	Epoch 27
Training results:
gen_loss: 13.489879
disc_loss: 0.19142492
disc_acc: 0.9386138613861386

Validation results:
gen_loss: 12.357568
disc_loss: 0.7076475
disc_acc: 0.847718253968254


	Epoch 28
Training results:
gen_loss: 13.556956
disc_loss: 0.17332183
disc_acc: 0.943440594059406

Validation results:
gen_loss: 17.566154
disc_loss: 0.93670946
disc_acc: 0.8214285714285714


	Epoch 29
Training results:
gen_loss: 14.643663
disc_loss: 0.3432081
disc_acc: 0.9116336633663367

Validation results:
gen_loss: 13.601758
disc_loss: 1.146577
disc_acc: 0.8020833333333334


	Epoch 30
Training results:
gen_loss: 14.14284
disc_loss: 0.16098589
disc_acc: 0.950990099009901

Validation results:
gen_loss: 13.126782
disc_loss: 0.37883013
disc_acc: 0.9067460317460317


	Epoch 31
Training results:
gen_loss: 13.238492
disc_loss: 0.13365684
disc_acc: 0.9584158415841584

Validation results:
gen_loss: 12.407825
disc_loss: 0.48834574
disc_acc: 0.8804563492063492


	Epoch 32
Training results:
gen_loss: 13.908578
disc_loss: 0.16776586
disc_acc: 0.9517326732673267

Validation results:
gen_loss: 12.945551
disc_loss: 0.38826528
disc_acc: 0.904265873015873


	Epoch 33
Training results:
gen_loss: 14.154787
disc_loss: 0.17546238
disc_acc: 0.9516089108910891

Validation results:
gen_loss: 18.508953
disc_loss: 1.0514356
disc_acc: 0.8268849206349206


	Epoch 34
Training results:
gen_loss: 14.596844
disc_loss: 0.35388786
disc_acc: 0.9206683168316832

Validation results:
gen_loss: 11.817457
disc_loss: 0.44688556
disc_acc: 0.8893849206349206


	Epoch 35
Training results:
gen_loss: 14.338892
disc_loss: 0.122719735
disc_acc: 0.9621287128712871

Validation results:
gen_loss: 15.955288
disc_loss: 0.34718654
disc_acc: 0.9151785714285714


	Epoch 36
Training results:
gen_loss: 14.146249
disc_loss: 0.15481964
disc_acc: 0.9579207920792079

Validation results:
gen_loss: 15.701419
disc_loss: 0.6741871
disc_acc: 0.8665674603174603


	Epoch 37
Training results:
gen_loss: 16.616966
disc_loss: 0.276421
disc_acc: 0.9334158415841585

Validation results:
gen_loss: 14.341218
disc_loss: 0.6938041
disc_acc: 0.8442460317460317


	Epoch 38
Training results:
gen_loss: 16.987904
disc_loss: 0.18287328
disc_acc: 0.9522277227722772

Validation results:
gen_loss: 15.025178
disc_loss: 0.3319172
disc_acc: 0.9241071428571429


	Epoch 39
Training results:
gen_loss: 15.422368
disc_loss: 0.10213003
disc_acc: 0.9688118811881188

Validation results:
gen_loss: 16.075256
disc_loss: 0.47783563
disc_acc: 0.9032738095238095


	Epoch 40
Training results:
gen_loss: 16.859474
disc_loss: 0.15408702
disc_acc: 0.9573019801980198

Validation results:
gen_loss: 13.896533
disc_loss: 1.1336468
disc_acc: 0.8189484126984127


	Epoch 41
Training results:
gen_loss: 17.490332
disc_loss: 0.27720964
disc_acc: 0.9382425742574257

Validation results:
gen_loss: 16.50604
disc_loss: 0.7438919
disc_acc: 0.8799603174603174


	Epoch 42
Training results:
gen_loss: 16.799658
disc_loss: 0.12786835
disc_acc: 0.9659653465346535

Validation results:
gen_loss: 13.745111
disc_loss: 0.6474555
disc_acc: 0.878968253968254


	Epoch 43
Training results:
gen_loss: 15.89764
disc_loss: 0.14018257
disc_acc: 0.961509900990099

Validation results:
gen_loss: 15.981181
disc_loss: 0.49220315
disc_acc: 0.904265873015873


	Epoch 44
Training results:
gen_loss: 16.302618
disc_loss: 0.15345559
disc_acc: 0.9602722772277228

Validation results:
gen_loss: 18.394043
disc_loss: 1.0195068
disc_acc: 0.8720238095238095


	Epoch 45
Training results:
gen_loss: 17.951763
disc_loss: 0.20691611
disc_acc: 0.9535891089108911

Validation results:
gen_loss: 18.534304
disc_loss: 0.68252546
disc_acc: 0.8829365079365079


	Epoch 46
Training results:
gen_loss: 17.626558
disc_loss: 0.17454913
disc_acc: 0.9568069306930693

Validation results:
gen_loss: 16.284615
disc_loss: 0.51465607
disc_acc: 0.9027777777777778


	Epoch 47
Training results:
gen_loss: 17.204819
disc_loss: 0.14753808
disc_acc: 0.9653465346534653

Validation results:
gen_loss: 14.066231
disc_loss: 0.33536223
disc_acc: 0.9206349206349206


	Epoch 48
Training results:
gen_loss: 16.440832
disc_loss: 0.1229962
disc_acc: 0.9689356435643565

Validation results:
gen_loss: 17.029716
disc_loss: 0.43670455
disc_acc: 0.9171626984126984


	Epoch 49
Training results:
gen_loss: 15.942344
disc_loss: 0.21000406
disc_acc: 0.9521039603960396

Validation results:
gen_loss: 14.153354
disc_loss: 0.5606991
disc_acc: 0.8973214285714286


	Epoch 50
Training results:
gen_loss: 17.985064
disc_loss: 0.1395294
disc_acc: 0.9659653465346535

Validation results:
gen_loss: 16.638687
disc_loss: 0.42789394
disc_acc: 0.9186507936507936


	Epoch 51
Training results:
gen_loss: 17.412888
disc_loss: 0.14250945
disc_acc: 0.965470297029703

Validation results:
gen_loss: 15.330996
disc_loss: 0.58639836
disc_acc: 0.8953373015873016


	Epoch 52
Training results:
gen_loss: 18.164778
disc_loss: 0.22293189
disc_acc: 0.9502475247524752

Validation results:
gen_loss: 18.718021
disc_loss: 0.9219991
disc_acc: 0.8774801587301587


	Epoch 53
Training results:
gen_loss: 19.186796
disc_loss: 0.16329168
disc_acc: 0.9617574257425743

Validation results:
gen_loss: 17.464817
disc_loss: 0.58156943
disc_acc: 0.9047619047619048


	Epoch 54
Training results:
gen_loss: 18.54165
disc_loss: 0.1374026
disc_acc: 0.9660891089108911

Validation results:
gen_loss: 19.580542
disc_loss: 0.45951518
disc_acc: 0.9270833333333334


	Epoch 55
Training results:
gen_loss: 17.674782
disc_loss: 0.11087063
disc_acc: 0.9724009900990099

Validation results:
gen_loss: 15.268002
disc_loss: 0.50139916
disc_acc: 0.9092261904761905


	Epoch 56
Training results:
gen_loss: 18.65085
disc_loss: 0.12949887
disc_acc: 0.9722772277227723

Validation results:
gen_loss: 24.326084
disc_loss: 1.5896678
disc_acc: 0.8244047619047619


	Epoch 57
Training results:
gen_loss: 21.306843
disc_loss: 0.38278997
disc_acc: 0.9403465346534653

Validation results:
gen_loss: 18.441454
disc_loss: 0.76287377
disc_acc: 0.8943452380952381


	Epoch 58
Training results:
gen_loss: 19.116848
disc_loss: 0.117612995
disc_acc: 0.975

Validation results:
gen_loss: 20.135767
disc_loss: 0.6418585
disc_acc: 0.9092261904761905


	Epoch 59
Training results:
gen_loss: 18.935822
disc_loss: 0.08018649
disc_acc: 0.9790841584158416

Validation results:
gen_loss: 19.081928
disc_loss: 0.605394
disc_acc: 0.9221230158730159


	Epoch 60
Training results:
gen_loss: 19.357975
disc_loss: 0.08905206
disc_acc: 0.9806930693069307

Validation results:
gen_loss: 19.494774
disc_loss: 0.44493312
disc_acc: 0.9305555555555556


	Epoch 61
Training results:
gen_loss: 19.176222
disc_loss: 0.15528338
disc_acc: 0.967450495049505

Validation results:
gen_loss: 21.76735
disc_loss: 0.81501067
disc_acc: 0.8824404761904762


	Epoch 62
Training results:
gen_loss: 19.52319
disc_loss: 0.16335273
disc_acc: 0.968440594059406

Validation results:
gen_loss: 18.3236
disc_loss: 0.8952239
disc_acc: 0.8888888888888888


	Epoch 63
Training results:
gen_loss: 18.365185
disc_loss: 0.14870891
disc_acc: 0.9689356435643565

Validation results:
gen_loss: 15.24675
disc_loss: 0.41134977
disc_acc: 0.9295634920634921


	Epoch 64
Training results:
gen_loss: 17.138529
disc_loss: 0.099745825
disc_acc: 0.976980198019802

Validation results:
gen_loss: 15.848496
disc_loss: 1.1185675
disc_acc: 0.8625992063492064


	Epoch 65
Training results:
gen_loss: 21.795008
disc_loss: 0.27527016
disc_acc: 0.9527227722772277

Validation results:
gen_loss: 20.822432
disc_loss: 0.55663645
disc_acc: 0.9181547619047619


	Epoch 66
Training results:
gen_loss: 20.996029
disc_loss: 0.1199277
disc_acc: 0.974009900990099

Validation results:
gen_loss: 22.594662
disc_loss: 0.65560466
disc_acc: 0.9092261904761905


	Epoch 67
Training results:
gen_loss: 20.612656
disc_loss: 0.14613731
disc_acc: 0.9711633663366337

Validation results:
gen_loss: 18.32125
disc_loss: 0.6513321
disc_acc: 0.9012896825396826


	Epoch 68
Training results:
gen_loss: 20.22754
disc_loss: 0.15641226
disc_acc: 0.9669554455445545

Validation results:
gen_loss: 18.800566
disc_loss: 0.7956645
disc_acc: 0.9017857142857143


	Epoch 69
Training results:
gen_loss: 20.21915
disc_loss: 0.0941073
disc_acc: 0.9792079207920792

Validation results:
gen_loss: 21.225885
disc_loss: 0.7964882
disc_acc: 0.9012896825396826


	Epoch 70
Training results:
gen_loss: 20.085922
disc_loss: 0.15870506
disc_acc: 0.9712871287128713

Validation results:
gen_loss: 18.68553
disc_loss: 0.67104036
disc_acc: 0.9201388888888888


	Epoch 71
Training results:
gen_loss: 22.44509
disc_loss: 0.26488647
disc_acc: 0.961509900990099

Validation results:
gen_loss: 17.377968
disc_loss: 0.694865
disc_acc: 0.8938492063492064


	Epoch 72
Training results:
gen_loss: 20.85006
disc_loss: 0.1092701
disc_acc: 0.975990099009901

Validation results:
gen_loss: 18.927412
disc_loss: 0.73713857
disc_acc: 0.9067460317460317


	Epoch 73
Training results:
gen_loss: 21.18055
disc_loss: 0.13384101
disc_acc: 0.9743811881188119

Validation results:
gen_loss: 22.336628
disc_loss: 0.7570168
disc_acc: 0.9166666666666666


	Epoch 74
Training results:
gen_loss: 22.46395
disc_loss: 0.10521779
disc_acc: 0.9774752475247525

Validation results:
gen_loss: 20.441338
disc_loss: 1.8324144
disc_acc: 0.8377976190476191


	Epoch 75
Training results:
gen_loss: 24.425003
disc_loss: 0.3425074
disc_acc: 0.9556930693069307

Validation results:
gen_loss: 22.961885
disc_loss: 0.7550716
disc_acc: 0.9017857142857143


	Epoch 76
Training results:
gen_loss: 22.038939
disc_loss: 0.06842329
disc_acc: 0.9861386138613861

Validation results:
gen_loss: 22.038637
disc_loss: 0.67975765
disc_acc: 0.9206349206349206


	Epoch 77
Training results:
gen_loss: 21.60666
disc_loss: 0.10252624
disc_acc: 0.9805693069306931

Validation results:
gen_loss: 25.310572
disc_loss: 0.9713706
disc_acc: 0.9017857142857143


	Epoch 78
Training results:
gen_loss: 23.623863
disc_loss: 0.18752223
disc_acc: 0.9660891089108911

Validation results:
gen_loss: 23.545435
disc_loss: 0.65571123
disc_acc: 0.9236111111111112


	Epoch 79
Training results:
gen_loss: 22.128323
disc_loss: 0.099785455
disc_acc: 0.9811881188118812

Validation results:
gen_loss: 20.27614
disc_loss: 0.9516453
disc_acc: 0.8953373015873016


	Epoch 80
Training results:
gen_loss: 23.155777
disc_loss: 0.150795
disc_acc: 0.9736386138613862

Validation results:
gen_loss: 26.79224
disc_loss: 0.86792046
disc_acc: 0.9146825396825397


	Epoch 81
Training results:
gen_loss: 24.315485
disc_loss: 0.15765013
disc_acc: 0.9716584158415842

Validation results:
gen_loss: 23.60737
disc_loss: 0.66686463
disc_acc: 0.9246031746031746


	Epoch 82
Training results:
gen_loss: 26.057554
disc_loss: 0.19932142
disc_acc: 0.9715346534653465

Validation results:
gen_loss: 26.165771
disc_loss: 0.6791075
disc_acc: 0.9250992063492064


	Epoch 83
Training results:
gen_loss: 24.448832
disc_loss: 0.16407223
disc_acc: 0.973019801980198

Validation results:
gen_loss: 23.599678
disc_loss: 0.9666843
disc_acc: 0.9072420634920635


	Epoch 84
Training results:
gen_loss: 23.86689
disc_loss: 0.124254495
disc_acc: 0.9774752475247525

Validation results:
gen_loss: 21.163645
disc_loss: 1.1820081
disc_acc: 0.8898809523809523


	Epoch 85
Training results:
gen_loss: 22.745321
disc_loss: 0.06362894
disc_acc: 0.9887376237623763

Validation results:
gen_loss: 21.55777
disc_loss: 0.70512
disc_acc: 0.9246031746031746


	Epoch 86
Training results:
gen_loss: 24.439257
disc_loss: 0.20489988
disc_acc: 0.9679455445544555

Validation results:
gen_loss: 25.404394
disc_loss: 1.2043861
disc_acc: 0.8908730158730159


	Epoch 87
Training results:
gen_loss: 24.360386
disc_loss: 0.12841845
disc_acc: 0.9790841584158416

Validation results:
gen_loss: 24.210371
disc_loss: 1.1284647
disc_acc: 0.9067460317460317


	Epoch 88
Training results:
gen_loss: 23.88806
disc_loss: 0.11079035
disc_acc: 0.9803217821782179

Validation results:
gen_loss: 25.275581
disc_loss: 0.8410009
disc_acc: 0.9186507936507936


	Epoch 89
Training results:
gen_loss: 23.964376
disc_loss: 0.15576686
disc_acc: 0.9742574257425742

Validation results:
gen_loss: 26.651587
disc_loss: 1.8595991
disc_acc: 0.8596230158730159


	Epoch 90
Training results:
gen_loss: 26.72589
disc_loss: 0.20100978
disc_acc: 0.9699257425742575

Validation results:
gen_loss: 26.675383
disc_loss: 0.94721323
disc_acc: 0.9126984126984127


	Epoch 91
Training results:
gen_loss: 26.148336
disc_loss: 0.10271662
disc_acc: 0.9845297029702971

Validation results:
gen_loss: 24.981857
disc_loss: 0.84939355
disc_acc: 0.9161706349206349


	Epoch 92
Training results:
gen_loss: 26.561726
disc_loss: 0.14193772
disc_acc: 0.9757425742574257

Validation results:
gen_loss: 28.32103
disc_loss: 0.8083561
disc_acc: 0.9216269841269841


	Epoch 93
Training results:
gen_loss: 26.123165
disc_loss: 0.10814766
disc_acc: 0.9820544554455446

Validation results:
gen_loss: 23.63004
disc_loss: 0.92896247
disc_acc: 0.9161706349206349


	Epoch 94
Training results:
gen_loss: 24.46243
disc_loss: 0.14897579
disc_acc: 0.9763613861386139

Validation results:
gen_loss: 27.280478
disc_loss: 1.0277542
disc_acc: 0.9057539682539683


	Epoch 95
Training results:
gen_loss: 27.364079
disc_loss: 0.20320603
disc_acc: 0.9725247524752475

Validation results:
gen_loss: 27.802868
disc_loss: 1.1982716
disc_acc: 0.8953373015873016


	Epoch 96
Training results:
gen_loss: 27.827713
disc_loss: 0.15469025
disc_acc: 0.9790841584158416

Validation results:
gen_loss: 26.016531
disc_loss: 0.9422043
disc_acc: 0.9087301587301587


	Epoch 97
Training results:
gen_loss: 25.873726
disc_loss: 0.09349855
disc_acc: 0.9837871287128713

Validation results:
gen_loss: 26.299454
disc_loss: 0.82670265
disc_acc: 0.9255952380952381


	Epoch 98
Training results:
gen_loss: 27.120472
disc_loss: 0.17513153
disc_acc: 0.974009900990099

Validation results:
gen_loss: 25.590523
disc_loss: 1.3568825
disc_acc: 0.8948412698412699


	Epoch 99
Training results:
gen_loss: 27.06861
disc_loss: 0.20851417
disc_acc: 0.9737623762376237

Validation results:
gen_loss: 24.864216
disc_loss: 1.1287276
disc_acc: 0.9092261904761905


	Epoch 100
Training results:
gen_loss: 26.317057
disc_loss: 0.155739
disc_acc: 0.9771039603960396

Validation results:
gen_loss: 29.547056
disc_loss: 0.9230107
disc_acc: 0.9171626984126984


	Epoch 101
Training results:
gen_loss: 27.121426
disc_loss: 0.120935164
disc_acc: 0.9794554455445544

Validation results:
gen_loss: 28.753206
disc_loss: 0.9644479
disc_acc: 0.9241071428571429


	Epoch 102
Training results:
gen_loss: 28.697887
disc_loss: 0.07584647
disc_acc: 0.9875

Validation results:
gen_loss: 29.068459
disc_loss: 0.82110137
disc_acc: 0.9246031746031746


	Epoch 103
Training results:
gen_loss: 29.03082
disc_loss: 0.14748609
disc_acc: 0.9800742574257426

Validation results:
gen_loss: 35.163544
disc_loss: 1.3833125
disc_acc: 0.9052579365079365


	Epoch 104
Training results:
gen_loss: 33.632977
disc_loss: 0.32381055
disc_acc: 0.966460396039604

Validation results:
gen_loss: 33.812595
disc_loss: 1.2462778
disc_acc: 0.9136904761904762


	Epoch 105
Training results:
gen_loss: 33.194168
disc_loss: 0.10373082
disc_acc: 0.9842821782178218

Validation results:
gen_loss: 31.278645
disc_loss: 0.83520806
disc_acc: 0.9280753968253969


	Epoch 106
Training results:
gen_loss: 32.232933
disc_loss: 0.088322654
disc_acc: 0.9847772277227723

Validation results:
gen_loss: 34.869633
disc_loss: 1.0086117
disc_acc: 0.9280753968253969


	Epoch 107
Training results:
gen_loss: 33.637066
disc_loss: 0.13887767
disc_acc: 0.9788366336633664

Validation results:
gen_loss: 31.72412
disc_loss: 1.1106999
disc_acc: 0.9171626984126984


	Epoch 108
Training results:
gen_loss: 31.56563
disc_loss: 0.15736507
disc_acc: 0.9797029702970297

Validation results:
gen_loss: 33.34491
disc_loss: 1.1021261
disc_acc: 0.9107142857142857


	Epoch 109
Training results:
gen_loss: 31.85164
disc_loss: 0.1310015
disc_acc: 0.9845297029702971

Validation results:
gen_loss: 31.425217
disc_loss: 1.1249726
disc_acc: 0.9226190476190477


	Epoch 110
Training results:
gen_loss: 32.381134
disc_loss: 0.14339884
disc_acc: 0.9810643564356436

Validation results:
gen_loss: 31.231556
disc_loss: 0.98483354
disc_acc: 0.9211309523809523


	Epoch 111
Training results:
gen_loss: 32.819138
disc_loss: 0.15346384
disc_acc: 0.9795792079207921

Validation results:
gen_loss: 34.48775
disc_loss: 1.327314
disc_acc: 0.902281746031746


	Epoch 112
Training results:
gen_loss: 33.618927
disc_loss: 0.30178586
disc_acc: 0.9709158415841584

Validation results:
gen_loss: 33.666267
disc_loss: 1.2029426
disc_acc: 0.9191468253968254


	Epoch 113
Training results:
gen_loss: 34.696583
disc_loss: 0.13112852
disc_acc: 0.9826732673267327

Validation results:
gen_loss: 38.05271
disc_loss: 1.1556058
disc_acc: 0.9270833333333334


	Epoch 114
Training results:
gen_loss: 35.869404
disc_loss: 0.08190545
disc_acc: 0.9896039603960396

Validation results:
gen_loss: 35.984993
disc_loss: 1.0082427
disc_acc: 0.9250992063492064


	Epoch 115
Training results:
gen_loss: 32.313644
disc_loss: 0.105248116
disc_acc: 0.9857673267326733

Validation results:
gen_loss: 40.555202
disc_loss: 1.1942343
disc_acc: 0.9275793650793651


	Epoch 116
Training results:
gen_loss: 35.436558
disc_loss: 0.3109813
disc_acc: 0.9668316831683168

Validation results:
gen_loss: 35.638283
disc_loss: 1.542647
disc_acc: 0.9112103174603174


	Epoch 117
Training results:
gen_loss: 33.97291
disc_loss: 0.07662007
disc_acc: 0.9886138613861386

Validation results:
gen_loss: 34.189224
disc_loss: 1.0587642
disc_acc: 0.9280753968253969


	Epoch 118
Training results:
gen_loss: 32.614315
disc_loss: 0.065210074
disc_acc: 0.9908415841584158

Validation results:
gen_loss: 32.69532
disc_loss: 0.91819805
disc_acc: 0.9370039682539683


	Epoch 119
Training results:
gen_loss: 32.6155
disc_loss: 0.11568504
disc_acc: 0.9860148514851486

Validation results:
gen_loss: 32.699413
disc_loss: 1.4634901
disc_acc: 0.8963293650793651


	Epoch 120
Training results:
gen_loss: 37.730568
disc_loss: 0.28280112
disc_acc: 0.9727722772277227

Validation results:
gen_loss: 40.728954
disc_loss: 1.860392
disc_acc: 0.8993055555555556


	Epoch 121
Training results:
gen_loss: 40.99229
disc_loss: 0.12918179
disc_acc: 0.9846534653465346

Validation results:
gen_loss: 36.71499
disc_loss: 1.707982
disc_acc: 0.8908730158730159


	Epoch 122
Training results:
gen_loss: 36.179363
disc_loss: 0.1183164
disc_acc: 0.9856435643564356

Validation results:
gen_loss: 36.413345
disc_loss: 1.2285819
disc_acc: 0.9231150793650794


	Epoch 123
Training results:
gen_loss: 36.546898
disc_loss: 0.102749385
disc_acc: 0.9858910891089109

Validation results:
gen_loss: 37.190765
disc_loss: 3.067815
disc_acc: 0.8680555555555556


	Epoch 124
Training results:
gen_loss: 37.52475
disc_loss: 0.18134452
disc_acc: 0.9829207920792079

Validation results:
gen_loss: 43.583557
disc_loss: 2.8014405
disc_acc: 0.8745039682539683


	Epoch 125
Training results:
gen_loss: 39.006954
disc_loss: 0.17626588
disc_acc: 0.9801980198019802

Validation results:
gen_loss: 40.762543
disc_loss: 1.2248704
disc_acc: 0.9300595238095238


	Epoch 126
Training results:
gen_loss: 39.43487
disc_loss: 0.12722051
disc_acc: 0.9860148514851486

Validation results:
gen_loss: 35.91422
disc_loss: 1.8948162
disc_acc: 0.8983134920634921


	Epoch 127
Training results:
gen_loss: 39.994186
disc_loss: 0.16090237
disc_acc: 0.9831683168316832

Validation results:
gen_loss: 43.289486
disc_loss: 2.6506774
disc_acc: 0.8824404761904762


	Epoch 128
Training results:
gen_loss: 39.65716
disc_loss: 0.21045789
disc_acc: 0.9790841584158416

Validation results:
gen_loss: 41.14799
disc_loss: 1.7879087
disc_acc: 0.9002976190476191


	Epoch 129
Training results:
gen_loss: 42.394413
disc_loss: 0.18857309
disc_acc: 0.9834158415841584

Validation results:
gen_loss: 45.331062
disc_loss: 1.454777
disc_acc: 0.9250992063492064


	Epoch 130
Training results:
gen_loss: 40.73594
disc_loss: 0.08527855
disc_acc: 0.988490099009901

Validation results:
gen_loss: 42.299397
disc_loss: 1.8187459
disc_acc: 0.9077380952380952


	Epoch 131
Training results:
gen_loss: 37.811176
disc_loss: 0.15841345
disc_acc: 0.9841584158415841

Validation results:
gen_loss: 39.11272
disc_loss: 1.3523737
disc_acc: 0.9186507936507936


	Epoch 132
Training results:
gen_loss: 38.297806
disc_loss: 0.112094074
disc_acc: 0.9863861386138614

Validation results:
gen_loss: 39.204945
disc_loss: 1.3634783
disc_acc: 0.9226190476190477


	Epoch 133
Training results:
gen_loss: 39.592697
disc_loss: 0.23474276
disc_acc: 0.9799504950495049

Validation results:
gen_loss: 37.45629
disc_loss: 1.2432423
disc_acc: 0.9290674603174603


	Epoch 134
Training results:
gen_loss: 38.65277
disc_loss: 0.11875836
disc_acc: 0.9850247524752476

Validation results:
gen_loss: 41.482822
disc_loss: 1.8153578
disc_acc: 0.9117063492063492


	Epoch 135
Training results:
gen_loss: 40.07924
disc_loss: 0.15586458
disc_acc: 0.9835396039603961

Validation results:
gen_loss: 37.83542
disc_loss: 1.3670819
disc_acc: 0.9171626984126984


	Epoch 136
Training results:
gen_loss: 41.303684
disc_loss: 0.14712358
disc_acc: 0.9852722772277228

Validation results:
gen_loss: 40.33661
disc_loss: 1.240846
disc_acc: 0.9310515873015873


	Epoch 137
Training results:
gen_loss: 43.389736
disc_loss: 0.24121404
disc_acc: 0.9794554455445544

Validation results:
gen_loss: 51.806416
disc_loss: 2.2078254
disc_acc: 0.9126984126984127


	Epoch 138
Training results:
gen_loss: 42.923283
disc_loss: 0.11462865
disc_acc: 0.9887376237623763

Validation results:
gen_loss: 41.7234
disc_loss: 1.7821249
disc_acc: 0.9136904761904762


	Epoch 139
Training results:
gen_loss: 42.931824
disc_loss: 0.12059114
disc_acc: 0.9861386138613861

Validation results:
gen_loss: 42.238976
disc_loss: 1.8377035
disc_acc: 0.9097222222222222


	Epoch 140
Training results:
gen_loss: 43.779766
disc_loss: 0.20034815
disc_acc: 0.9813118811881189

Validation results:
gen_loss: 45.002403
disc_loss: 1.6637233
disc_acc: 0.9146825396825397


	Epoch 141
Training results:
gen_loss: 46.430977
disc_loss: 0.2143516
disc_acc: 0.9815594059405941

Validation results:
gen_loss: 47.89464
disc_loss: 1.6503725
disc_acc: 0.9236111111111112


	Epoch 142
Training results:
gen_loss: 43.93157
disc_loss: 0.13611628
disc_acc: 0.9875

Validation results:
gen_loss: 41.145
disc_loss: 1.7957243
disc_acc: 0.9047619047619048


	Epoch 143
Training results:
gen_loss: 41.70554
disc_loss: 0.1277142
disc_acc: 0.9875

Validation results:
gen_loss: 44.408085
disc_loss: 1.8239917
disc_acc: 0.9146825396825397


	Epoch 144
Training results:
gen_loss: 43.636868
disc_loss: 0.17092822
disc_acc: 0.9831683168316832

Validation results:
gen_loss: 43.27529
disc_loss: 1.4257959
disc_acc: 0.9241071428571429


	Epoch 145
Training results:
gen_loss: 41.303963
disc_loss: 0.17456865
disc_acc: 0.9845297029702971

Validation results:
gen_loss: 39.860516
disc_loss: 1.7742534
disc_acc: 0.9181547619047619


	Epoch 146
Training results:
gen_loss: 41.31527
disc_loss: 0.12207262
disc_acc: 0.9871287128712871

Validation results:
gen_loss: 45.34799
disc_loss: 2.2795913
disc_acc: 0.9002976190476191


	Epoch 147
Training results:
gen_loss: 47.45457
disc_loss: 0.3321025
disc_acc: 0.9790841584158416

Validation results:
gen_loss: 47.695866
disc_loss: 1.7602993
disc_acc: 0.9126984126984127


	Epoch 148
Training results:
gen_loss: 45.31187
disc_loss: 0.14208916
disc_acc: 0.9876237623762376

Validation results:
gen_loss: 41.633476
disc_loss: 2.0427222
disc_acc: 0.9092261904761905


	Epoch 149
Training results:
gen_loss: 43.43176
disc_loss: 0.065450355
disc_acc: 0.9926980198019802

Validation results:
gen_loss: 42.473526
disc_loss: 1.4427822
disc_acc: 0.9166666666666666


	Epoch 150
Training results:
gen_loss: 44.037468
disc_loss: 0.16479157
disc_acc: 0.9850247524752476

Validation results:
gen_loss: 44.323326
disc_loss: 2.2599154
disc_acc: 0.8943452380952381


	Epoch 151
Training results:
gen_loss: 44.82108
disc_loss: 0.14422995
disc_acc: 0.9860148514851486

Validation results:
gen_loss: 45.68325
disc_loss: 1.9038026
disc_acc: 0.9112103174603174


	Epoch 152
Training results:
gen_loss: 44.413437
disc_loss: 0.14964361
disc_acc: 0.9850247524752476

Validation results:
gen_loss: 45.70623
disc_loss: 1.5498465
disc_acc: 0.9260912698412699


	Epoch 153
Training results:
gen_loss: 44.825348
disc_loss: 0.11244858
disc_acc: 0.9879950495049505

Validation results:
gen_loss: 41.457138
disc_loss: 2.132893
disc_acc: 0.9037698412698413


	Epoch 154
Training results:
gen_loss: 50.711937
disc_loss: 0.37605265
disc_acc: 0.9764851485148515

Validation results:
gen_loss: 44.87401
disc_loss: 2.0722804
disc_acc: 0.9151785714285714


	Epoch 155
Training results:
gen_loss: 49.117844
disc_loss: 0.13776107
disc_acc: 0.9876237623762376

Validation results:
gen_loss: 46.342297
disc_loss: 1.5760667
disc_acc: 0.9211309523809523


	Epoch 156
Training results:
gen_loss: 49.973465
disc_loss: 0.09651178
disc_acc: 0.9892326732673268

Validation results:
gen_loss: 49.79352
disc_loss: 1.7153838
disc_acc: 0.9236111111111112


	Epoch 157
Training results:
gen_loss: 50.419308
disc_loss: 0.09380509
disc_acc: 0.9909653465346535

Validation results:
gen_loss: 50.657494
disc_loss: 3.1831663
disc_acc: 0.8859126984126984


	Epoch 158
Training results:
gen_loss: 49.673878
disc_loss: 0.18526739
disc_acc: 0.9851485148514851

Validation results:
gen_loss: 49.16345
disc_loss: 3.0717874
disc_acc: 0.8854166666666666


	Epoch 159
Training results:
gen_loss: 46.600433
disc_loss: 0.21583714
disc_acc: 0.9836633663366336

Validation results:
gen_loss: 47.09982
disc_loss: 2.2580922
disc_acc: 0.9087301587301587


	Epoch 160
Training results:
gen_loss: 47.627247
disc_loss: 0.10690557
disc_acc: 0.9902227722772278

Validation results:
gen_loss: 44.94702
disc_loss: 1.7881199
disc_acc: 0.9156746031746031


	Epoch 161
Training results:
gen_loss: 48.587646
disc_loss: 0.093928315
disc_acc: 0.9907178217821783

Validation results:
gen_loss: 48.641567
disc_loss: 2.0421717
disc_acc: 0.9166666666666666


	Epoch 162
Training results:
gen_loss: 49.13013
disc_loss: 0.18525729
disc_acc: 0.9858910891089109

Validation results:
gen_loss: 59.497566
disc_loss: 4.0588045
disc_acc: 0.8859126984126984


	Epoch 163
Training results:
gen_loss: 57.96227
disc_loss: 0.30237168
disc_acc: 0.9806930693069307

Validation results:
gen_loss: 52.932827
disc_loss: 1.7760829
disc_acc: 0.9201388888888888


	Epoch 164
Training results:
gen_loss: 56.65953
disc_loss: 0.10392551
disc_acc: 0.9915841584158416

Validation results:
gen_loss: 54.50896
disc_loss: 2.6453354
disc_acc: 0.9146825396825397


	Epoch 165
Training results:
gen_loss: 57.21084
disc_loss: 0.19534142
disc_acc: 0.9846534653465346

Validation results:
gen_loss: 59.49675
disc_loss: 2.6442978
disc_acc: 0.9002976190476191


	Epoch 166
Training results:
gen_loss: 55.721172
disc_loss: 0.08051419
disc_acc: 0.9922029702970298

Validation results:
gen_loss: 54.6972
disc_loss: 1.9068074
disc_acc: 0.9131944444444444


	Epoch 167
Training results:
gen_loss: 52.909298
disc_loss: 0.1539314
disc_acc: 0.9897277227722773

Validation results:
gen_loss: 47.922993
disc_loss: 3.3609865
disc_acc: 0.8779761904761905


	Epoch 168
Training results:
gen_loss: 57.69874
disc_loss: 0.32191852
disc_acc: 0.975990099009901

Validation results:
gen_loss: 59.42923
disc_loss: 3.2229228
disc_acc: 0.876984126984127


	Epoch 169
Training results:
gen_loss: 60.907207
disc_loss: 0.17713632
disc_acc: 0.9840346534653466

Validation results:
gen_loss: 60.143116
disc_loss: 2.3411238
disc_acc: 0.9136904761904762


	Epoch 170
Training results:
gen_loss: 58.82321
disc_loss: 0.11047892
disc_acc: 0.9908415841584158

Validation results:
gen_loss: 54.9687
disc_loss: 2.4139814
disc_acc: 0.9131944444444444


	Epoch 171
Training results:
gen_loss: 59.094547
disc_loss: 0.0854171
disc_acc: 0.9910891089108911

Validation results:
gen_loss: 56.06373
disc_loss: 2.140908
disc_acc: 0.9191468253968254


	Epoch 172
Training results:
gen_loss: 55.96271
disc_loss: 0.15557344
disc_acc: 0.988490099009901

Validation results:
gen_loss: 58.22442
disc_loss: 1.8855458
disc_acc: 0.9181547619047619


	Epoch 173
Training results:
gen_loss: 54.675117
disc_loss: 0.094083905
disc_acc: 0.9918316831683168

Validation results:
gen_loss: 57.71031
disc_loss: 2.0491533
disc_acc: 0.9290674603174603


	Epoch 174
Training results:
gen_loss: 55.203255
disc_loss: 0.22599366
disc_acc: 0.9834158415841584

Validation results:
gen_loss: 59.189682
disc_loss: 2.6041937
disc_acc: 0.9146825396825397


	Epoch 175
Training results:
gen_loss: 57.13376
disc_loss: 0.1747259
disc_acc: 0.986509900990099

Validation results:
gen_loss: 66.13627
disc_loss: 2.8395123
disc_acc: 0.9141865079365079


	Epoch 176
Training results:
gen_loss: 57.468517
disc_loss: 0.2830666
disc_acc: 0.9825495049504951

Validation results:
gen_loss: 53.322365
disc_loss: 2.726341
disc_acc: 0.9052579365079365


	Epoch 177
Training results:
gen_loss: 58.442165
disc_loss: 0.16896908
disc_acc: 0.988490099009901

Validation results:
gen_loss: 59.176487
disc_loss: 1.7833405
disc_acc: 0.9320436507936508


	Epoch 178
Training results:
gen_loss: 57.0169
disc_loss: 0.060770553
disc_acc: 0.9951732673267327

Validation results:
gen_loss: 59.343777
disc_loss: 1.980508
disc_acc: 0.9275793650793651


	Epoch 179
Training results:
gen_loss: 56.82382
disc_loss: 0.228162
disc_acc: 0.9862623762376238

Validation results:
gen_loss: 64.81733
disc_loss: 2.4654682
disc_acc: 0.9117063492063492


	Epoch 180
Training results:
gen_loss: 60.172077
disc_loss: 0.15768431
disc_acc: 0.9872524752475248

Validation results:
gen_loss: 65.663284
disc_loss: 2.9397218
disc_acc: 0.9027777777777778


	Epoch 181
Training results:
gen_loss: 58.09423
disc_loss: 0.22479506
disc_acc: 0.9867574257425743

Validation results:
gen_loss: 59.128242
disc_loss: 2.1812806
disc_acc: 0.9221230158730159


	Epoch 182
Training results:
gen_loss: 58.466785
disc_loss: 0.10612938
disc_acc: 0.9905940594059406

Validation results:
gen_loss: 60.166603
disc_loss: 2.5039928
disc_acc: 0.9117063492063492


	Epoch 183
Training results:
gen_loss: 60.337616
disc_loss: 0.24047577
disc_acc: 0.9853960396039604

Validation results:
gen_loss: 60.1015
disc_loss: 2.7225611
disc_acc: 0.9047619047619048


	Epoch 184
Training results:
gen_loss: 66.38998
disc_loss: 0.14129981
disc_acc: 0.9873762376237624

Validation results:
gen_loss: 63.71477
disc_loss: 3.2055333
disc_acc: 0.9186507936507936


	Epoch 185
Training results:
gen_loss: 68.02371
disc_loss: 0.1670205
disc_acc: 0.9875

Validation results:
gen_loss: 68.07854
disc_loss: 2.824343
disc_acc: 0.9176587301587301


	Epoch 186
Training results:
gen_loss: 63.704567
disc_loss: 0.18262583
disc_acc: 0.9887376237623763

Validation results:
gen_loss: 62.577957
disc_loss: 2.56744
disc_acc: 0.910218253968254


	Epoch 187
Training results:
gen_loss: 65.550064
disc_loss: 0.17779554
disc_acc: 0.9889851485148515

Validation results:
gen_loss: 63.03757
disc_loss: 2.496676
disc_acc: 0.9141865079365079


	Epoch 188
Training results:
gen_loss: 64.432144
disc_loss: 0.103911355
disc_acc: 0.9920792079207921

Validation results:
gen_loss: 64.53061
disc_loss: 2.0798595
disc_acc: 0.9206349206349206


	Epoch 189
Training results:
gen_loss: 64.941055
disc_loss: 0.24413647
disc_acc: 0.986509900990099

Validation results:
gen_loss: 66.25091
disc_loss: 2.4141955
disc_acc: 0.9216269841269841


	Epoch 190
Training results:
gen_loss: 64.53006
disc_loss: 0.11875813
disc_acc: 0.9912128712871288

Validation results:
gen_loss: 58.73836
disc_loss: 2.1873617
disc_acc: 0.9241071428571429


	Epoch 191
Training results:
gen_loss: 62.925697
disc_loss: 0.20592892
disc_acc: 0.9883663366336634

Validation results:
gen_loss: 68.26775
disc_loss: 2.1380625
disc_acc: 0.9260912698412699


	Epoch 192
Training results:
gen_loss: 65.67921
disc_loss: 0.122338004
disc_acc: 0.9902227722772278

Validation results:
gen_loss: 65.88167
disc_loss: 2.417028
disc_acc: 0.9186507936507936


	Epoch 193
Training results:
gen_loss: 66.77316
disc_loss: 0.20890194
disc_acc: 0.9877475247524753

Validation results:
gen_loss: 68.064804
disc_loss: 3.012318
disc_acc: 0.9122023809523809


	Epoch 194
Training results:
gen_loss: 64.28085
disc_loss: 0.16060963
disc_acc: 0.9897277227722773

Validation results:
gen_loss: 60.118332
disc_loss: 3.0971143
disc_acc: 0.902281746031746


	Epoch 195
Training results:
gen_loss: 64.81736
disc_loss: 0.27045986
disc_acc: 0.9857673267326733

Validation results:
gen_loss: 66.66879
disc_loss: 2.5066776
disc_acc: 0.9216269841269841


	Epoch 196
Training results:
gen_loss: 66.26733
disc_loss: 0.18455723
disc_acc: 0.9896039603960396

Validation results:
gen_loss: 65.05495
disc_loss: 4.018233
disc_acc: 0.9007936507936508


	Epoch 197
Training results:
gen_loss: 68.804794
disc_loss: 0.1373424
disc_acc: 0.9900990099009901

Validation results:
gen_loss: 74.81961
disc_loss: 2.8293273
disc_acc: 0.9131944444444444


	Epoch 198
Training results:
gen_loss: 71.37667
disc_loss: 0.14072435
disc_acc: 0.9910891089108911

Validation results:
gen_loss: 81.79145
disc_loss: 2.9708135
disc_acc: 0.9226190476190477


	Epoch 199
Training results:
gen_loss: 74.56085
disc_loss: 0.23361991
disc_acc: 0.9871287128712871

Validation results:
gen_loss: 69.98519
disc_loss: 2.8628206
disc_acc: 0.9186507936507936


	Epoch 200
Training results:
gen_loss: 66.70086
disc_loss: 0.17310008
disc_acc: 0.9899752475247525

Validation results:
gen_loss: 72.10408
disc_loss: 2.9206717
disc_acc: 0.9231150793650794


	Epoch 201
Training results:
gen_loss: 69.11
disc_loss: 0.17917922
disc_acc: 0.9889851485148515

Validation results:
gen_loss: 68.099014
disc_loss: 3.029767
disc_acc: 0.9146825396825397


	Epoch 202
Training results:
gen_loss: 69.91295
disc_loss: 0.15742692
disc_acc: 0.9896039603960396

Validation results:
gen_loss: 65.7449
disc_loss: 2.7352693
disc_acc: 0.9206349206349206


	Epoch 203
Training results:
gen_loss: 72.13482
disc_loss: 0.38592896
disc_acc: 0.9846534653465346

Validation results:
gen_loss: 70.54642
disc_loss: 3.6883295
disc_acc: 0.9052579365079365


	Epoch 204
Training results:
gen_loss: 74.84406
disc_loss: 0.202259
disc_acc: 0.990470297029703

Validation results:
gen_loss: 71.07614
disc_loss: 3.3838303
disc_acc: 0.9107142857142857


	Epoch 205
Training results:
gen_loss: 70.4685
disc_loss: 0.11184179
disc_acc: 0.9923267326732673

Validation results:
gen_loss: 68.2482
disc_loss: 4.67798
disc_acc: 0.8898809523809523


	Epoch 206
Training results:
gen_loss: 70.971565
disc_loss: 0.23880155
disc_acc: 0.9888613861386139

Validation results:
gen_loss: 65.753
disc_loss: 3.2475698
disc_acc: 0.9072420634920635


	Epoch 207
Training results:
gen_loss: 69.113495
disc_loss: 0.098632134
disc_acc: 0.9919554455445545

Validation results:
gen_loss: 71.27655
disc_loss: 3.2501335
disc_acc: 0.9136904761904762


	Epoch 208
Training results:
gen_loss: 70.670525
disc_loss: 0.15903725
disc_acc: 0.9912128712871288

Validation results:
gen_loss: 69.04685
disc_loss: 2.4187086
disc_acc: 0.9250992063492064


	Epoch 209
Training results:
gen_loss: 70.048035
disc_loss: 0.13566338
disc_acc: 0.9917079207920793

Validation results:
gen_loss: 70.183914
disc_loss: 2.952964
disc_acc: 0.9196428571428571


	Epoch 210
Training results:
gen_loss: 68.394
disc_loss: 0.2509919
disc_acc: 0.9871287128712871

Validation results:
gen_loss: 68.455154
disc_loss: 3.6252716
disc_acc: 0.8933531746031746


	Epoch 211
Training results:
gen_loss: 68.61558
disc_loss: 0.15333766
disc_acc: 0.9910891089108911

Validation results:
gen_loss: 65.80302
disc_loss: 2.3960133
disc_acc: 0.9265873015873016


	Epoch 212
Training results:
gen_loss: 70.29433
disc_loss: 0.14813894
disc_acc: 0.9902227722772278

Validation results:
gen_loss: 85.34647
disc_loss: 4.847249
disc_acc: 0.8809523809523809


	Epoch 213
Training results:
gen_loss: 75.04748
disc_loss: 0.26539692
disc_acc: 0.9879950495049505

Validation results:
gen_loss: 76.261086
disc_loss: 3.412417
disc_acc: 0.9141865079365079


	Epoch 214
Training results:
gen_loss: 73.19864
disc_loss: 0.199788
disc_acc: 0.9887376237623763

Validation results:
gen_loss: 73.07652
disc_loss: 4.8948684
disc_acc: 0.8943452380952381


	Epoch 215
Training results:
gen_loss: 76.32879
disc_loss: 0.2264987
disc_acc: 0.9879950495049505

Validation results:
gen_loss: 74.87913
disc_loss: 4.5827594
disc_acc: 0.8928571428571429


	Epoch 216
Training results:
gen_loss: 72.42203
disc_loss: 0.17745367
disc_acc: 0.9907178217821783

Validation results:
gen_loss: 73.56934
disc_loss: 3.5459285
disc_acc: 0.908234126984127


	Epoch 217
Training results:
gen_loss: 72.85906
disc_loss: 0.034940407
disc_acc: 0.9965346534653465

Validation results:
gen_loss: 73.73215
disc_loss: 3.191851
disc_acc: 0.9191468253968254


	Epoch 218
Training results:
gen_loss: 80.34652
disc_loss: 0.23347557
disc_acc: 0.9883663366336634

Validation results:
gen_loss: 85.80744
disc_loss: 4.4493647
disc_acc: 0.9027777777777778


	Epoch 219
Training results:
gen_loss: 78.271416
disc_loss: 0.31538135
disc_acc: 0.9853960396039604

Validation results:
gen_loss: 77.212654
disc_loss: 3.2680502
disc_acc: 0.9166666666666666


	Epoch 220
Training results:
gen_loss: 74.03639
disc_loss: 0.25219706
disc_acc: 0.9862623762376238

Validation results:
gen_loss: 77.45956
disc_loss: 5.6734986
disc_acc: 0.8978174603174603


	Epoch 221
Training results:
gen_loss: 81.10518
disc_loss: 0.17709927
disc_acc: 0.9925742574257426

Validation results:
gen_loss: 76.240326
disc_loss: 3.5254068
disc_acc: 0.9146825396825397


	Epoch 222
Training results:
gen_loss: 77.833435
disc_loss: 0.08178974
disc_acc: 0.9948019801980198

Validation results:
gen_loss: 80.93795
disc_loss: 3.0859919
disc_acc: 0.9350198412698413


	Epoch 223
Training results:
gen_loss: 79.35743
disc_loss: 0.27153772
disc_acc: 0.9873762376237624

Validation results:
gen_loss: 70.446785
disc_loss: 4.242838
disc_acc: 0.9002976190476191


	Epoch 224
Training results:
gen_loss: 82.573715
disc_loss: 0.22543985
disc_acc: 0.9896039603960396

Validation results:
gen_loss: 82.18675
disc_loss: 3.2212055
disc_acc: 0.9265873015873016


	Epoch 225
Training results:
gen_loss: 77.8888
disc_loss: 0.12304145
disc_acc: 0.9925742574257426

Validation results:
gen_loss: 82.789314
disc_loss: 3.497644
disc_acc: 0.9216269841269841


	Epoch 226
Training results:
gen_loss: 81.62551
disc_loss: 0.25593635
disc_acc: 0.9883663366336634

Validation results:
gen_loss: 82.0399
disc_loss: 3.677658
disc_acc: 0.9146825396825397


	Epoch 227
Training results:
gen_loss: 80.096985
disc_loss: 0.13684474
disc_acc: 0.9931930693069307

Validation results:
gen_loss: 80.46825
disc_loss: 3.2205312
disc_acc: 0.9186507936507936


	Epoch 228
Training results:
gen_loss: 81.593376
disc_loss: 0.26382565
disc_acc: 0.9887376237623763

Validation results:
gen_loss: 85.5556
disc_loss: 4.599012
disc_acc: 0.8988095238095238


	Epoch 229
Training results:
gen_loss: 85.79515
disc_loss: 0.12349579
disc_acc: 0.9926980198019802

Validation results:
gen_loss: 83.99966
disc_loss: 4.513673
disc_acc: 0.9037698412698413


	Epoch 230
Training results:
gen_loss: 85.289055
disc_loss: 0.19037879
disc_acc: 0.991460396039604

Validation results:
gen_loss: 86.07243
disc_loss: 3.9394011
disc_acc: 0.9201388888888888


	Epoch 231
Training results:
gen_loss: 84.340996
disc_loss: 0.2185197
disc_acc: 0.9900990099009901

Validation results:
gen_loss: 84.11984
disc_loss: 4.5993457
disc_acc: 0.9002976190476191


	Epoch 232
Training results:
gen_loss: 83.07376
disc_loss: 0.2052889
disc_acc: 0.9893564356435643

Validation results:
gen_loss: 81.35963
disc_loss: 3.5866785
disc_acc: 0.9216269841269841


	Epoch 233
Training results:
gen_loss: 87.37652
disc_loss: 0.14138073
disc_acc: 0.9926980198019802

Validation results:
gen_loss: 94.03525
disc_loss: 5.063551
disc_acc: 0.8933531746031746


	Epoch 234
Training results:
gen_loss: 84.20302
disc_loss: 0.17749684
disc_acc: 0.9915841584158416

Validation results:
gen_loss: 86.8355
disc_loss: 3.7411938
disc_acc: 0.9181547619047619


	Epoch 235
Training results:
gen_loss: 81.598915
disc_loss: 0.21002066
disc_acc: 0.9898514851485148

Validation results:
gen_loss: 78.45125
disc_loss: 4.254855
disc_acc: 0.9032738095238095


	Epoch 236
Training results:
gen_loss: 86.17848
disc_loss: 0.1909538
disc_acc: 0.9893564356435643

Validation results:
gen_loss: 88.031204
disc_loss: 4.011715
disc_acc: 0.9097222222222222


	Epoch 237
Training results:
gen_loss: 84.142685
disc_loss: 0.15466577
disc_acc: 0.9919554455445545

Validation results:
gen_loss: 81.89149
disc_loss: 3.7463374
disc_acc: 0.9151785714285714


	Epoch 238
Training results:
gen_loss: 74.59823
disc_loss: 0.12713936
disc_acc: 0.9931930693069307

Validation results:
gen_loss: 73.52198
disc_loss: 3.1295736
disc_acc: 0.9206349206349206


	Epoch 239
Training results:
gen_loss: 81.87814
disc_loss: 0.18002413
disc_acc: 0.9910891089108911

Validation results:
gen_loss: 79.31109
disc_loss: 5.6588993
disc_acc: 0.8958333333333334


	Epoch 240
Training results:
gen_loss: 90.63888
disc_loss: 0.2806054
disc_acc: 0.9863861386138614

Validation results:
gen_loss: 90.64574
disc_loss: 3.8781748
disc_acc: 0.9176587301587301


	Epoch 241
Training results:
gen_loss: 83.19039
disc_loss: 0.17906655
disc_acc: 0.9920792079207921

Validation results:
gen_loss: 79.457695
disc_loss: 3.6525197
disc_acc: 0.9166666666666666


	Epoch 242
Training results:
gen_loss: 87.54268
disc_loss: 0.10935332
disc_acc: 0.9948019801980198

Validation results:
gen_loss: 84.796005
disc_loss: 3.8846586
disc_acc: 0.9191468253968254


	Epoch 243
Training results:
gen_loss: 88.862114
disc_loss: 0.3547432
disc_acc: 0.9853960396039604

Validation results:
gen_loss: 96.96719
disc_loss: 5.0789695
disc_acc: 0.9047619047619048


	Epoch 244
Training results:
gen_loss: 96.79949
disc_loss: 0.13659701
disc_acc: 0.9933168316831683

Validation results:
gen_loss: 106.43894
disc_loss: 4.1768484
disc_acc: 0.9176587301587301


	Epoch 245
Training results:
gen_loss: 94.06875
disc_loss: 0.21359044
disc_acc: 0.9905940594059406

Validation results:
gen_loss: 91.50081
disc_loss: 4.226266
disc_acc: 0.9141865079365079


	Epoch 246
Training results:
gen_loss: 91.663864
disc_loss: 0.19592647
disc_acc: 0.9919554455445545

Validation results:
gen_loss: 92.537964
disc_loss: 4.138898
disc_acc: 0.9201388888888888


	Epoch 247
Training results:
gen_loss: 93.10928
disc_loss: 0.21449591
disc_acc: 0.9908415841584158

Validation results:
gen_loss: 92.44367
disc_loss: 4.7913594
disc_acc: 0.9122023809523809


	Epoch 248
Training results:
gen_loss: 93.67408
disc_loss: 0.36852518
disc_acc: 0.9855198019801981

Validation results:
gen_loss: 94.25319
disc_loss: 5.2213993
disc_acc: 0.8988095238095238


	Epoch 249
Training results:
gen_loss: 95.00633
disc_loss: 0.17583755
disc_acc: 0.9923267326732673

Validation results:
gen_loss: 94.98578
disc_loss: 4.2669096
disc_acc: 0.9196428571428571


	Epoch 250
Training results:
gen_loss: 91.68729
disc_loss: 0.11171544
disc_acc: 0.9935643564356436

Validation results:
gen_loss: 93.64455
disc_loss: 3.5819302
disc_acc: 0.9246031746031746


	Epoch 251
Training results:
gen_loss: 93.815025
disc_loss: 0.16518018
disc_acc: 0.993440594059406

Validation results:
gen_loss: 88.58894
disc_loss: 3.6302912
disc_acc: 0.9186507936507936


	Epoch 252
Training results:
gen_loss: 89.85908
disc_loss: 0.13216615
disc_acc: 0.9933168316831683

Validation results:
gen_loss: 93.9212
disc_loss: 2.7632751
disc_acc: 0.9365079365079365


	Epoch 253
Training results:
gen_loss: 92.37194
disc_loss: 0.15102226
disc_acc: 0.9931930693069307

Validation results:
gen_loss: 96.42326
disc_loss: 4.789828
disc_acc: 0.9122023809523809


	Epoch 254
Training results:
gen_loss: 101.48547
disc_loss: 0.2804781
disc_acc: 0.988490099009901

Validation results:
gen_loss: 97.9903
disc_loss: 3.6820624
disc_acc: 0.9285714285714286


	Epoch 255
Training results:
gen_loss: 95.593376
disc_loss: 0.20717412
disc_acc: 0.9910891089108911

Validation results:
gen_loss: 93.32272
disc_loss: 4.023943
disc_acc: 0.9146825396825397


	Epoch 256
Training results:
gen_loss: 91.18767
disc_loss: 0.16213413
disc_acc: 0.9929455445544555

Validation results:
gen_loss: 93.76863
disc_loss: 5.5270042
disc_acc: 0.8998015873015873


	Epoch 257
Training results:
gen_loss: 99.05228
disc_loss: 0.2592153
disc_acc: 0.9886138613861386

Validation results:
gen_loss: 99.292656
disc_loss: 4.8280954
disc_acc: 0.9181547619047619


	Epoch 258
Training results:
gen_loss: 94.15285
disc_loss: 0.1482212
disc_acc: 0.993440594059406

Validation results:
gen_loss: 92.2123
disc_loss: 4.788839
disc_acc: 0.9117063492063492


	Epoch 259
Training results:
gen_loss: 98.45373
disc_loss: 0.26703128
disc_acc: 0.9896039603960396

Validation results:
gen_loss: 112.564606
disc_loss: 4.793308
disc_acc: 0.9161706349206349


	Epoch 260
Training results:
gen_loss: 95.205696
disc_loss: 0.19875835
disc_acc: 0.991460396039604

Validation results:
gen_loss: 93.239494
disc_loss: 4.122248
disc_acc: 0.9191468253968254


	Epoch 261
Training results:
gen_loss: 94.56795
disc_loss: 0.1064684
disc_acc: 0.9955445544554455

Validation results:
gen_loss: 94.5694
disc_loss: 6.3655567
disc_acc: 0.8898809523809523


	Epoch 262
Training results:
gen_loss: 99.10206
disc_loss: 0.17076641
disc_acc: 0.994059405940594

Validation results:
gen_loss: 95.85922
disc_loss: 3.656916
disc_acc: 0.9265873015873016


	Epoch 263
Training results:
gen_loss: 95.95259
disc_loss: 0.28494477
disc_acc: 0.9897277227722773

Validation results:
gen_loss: 99.1885
disc_loss: 5.5731864
disc_acc: 0.9067460317460317


	Epoch 264
Training results:
gen_loss: 104.39177
disc_loss: 0.28002572
disc_acc: 0.9902227722772278

Validation results:
gen_loss: 111.47296
disc_loss: 3.9969096
disc_acc: 0.9206349206349206


	Epoch 265
Training results:
gen_loss: 104.19074
disc_loss: 0.1918003
disc_acc: 0.993440594059406

Validation results:
gen_loss: 103.37602
disc_loss: 4.897587
disc_acc: 0.9211309523809523


	Epoch 266
Training results:
gen_loss: 100.388565
disc_loss: 0.18499777
disc_acc: 0.9920792079207921

Validation results:
gen_loss: 101.148224
disc_loss: 5.2432504
disc_acc: 0.9126984126984127


	Epoch 267
Training results:
gen_loss: 100.15136
disc_loss: 0.24559896
disc_acc: 0.9913366336633663

Validation results:
gen_loss: 97.39619
disc_loss: 4.174113
disc_acc: 0.9260912698412699


	Epoch 268
Training results:
gen_loss: 95.073235
disc_loss: 0.14749406
disc_acc: 0.9931930693069307

Validation results:
gen_loss: 96.05985
disc_loss: 5.288321
disc_acc: 0.9037698412698413


	Epoch 269
Training results:
gen_loss: 98.803215
disc_loss: 0.23313111
disc_acc: 0.9920792079207921

Validation results:
gen_loss: 96.33565
disc_loss: 3.887116
disc_acc: 0.9275793650793651


	Epoch 270
Training results:
gen_loss: 101.00128
disc_loss: 0.19219473
disc_acc: 0.9908415841584158

Validation results:
gen_loss: 98.744064
disc_loss: 4.9726825
disc_acc: 0.9037698412698413


	Epoch 271
Training results:
gen_loss: 99.81611
disc_loss: 0.21976969
disc_acc: 0.9928217821782178

Validation results:
gen_loss: 98.06424
disc_loss: 4.743837
disc_acc: 0.9201388888888888


	Epoch 272
Training results:
gen_loss: 98.341255
disc_loss: 0.11434694
disc_acc: 0.995049504950495

Validation results:
gen_loss: 99.28891
disc_loss: 4.0510397
disc_acc: 0.9246031746031746


	Epoch 273
Training results:
gen_loss: 103.11493
disc_loss: 0.3990224
disc_acc: 0.986509900990099

Validation results:
gen_loss: 118.50231
disc_loss: 5.0936537
disc_acc: 0.9191468253968254


	Epoch 274
Training results:
gen_loss: 106.22604
disc_loss: 0.3518832
disc_acc: 0.9882425742574258

Validation results:
gen_loss: 99.85647
disc_loss: 3.743953
disc_acc: 0.9241071428571429


	Epoch 275
Training results:
gen_loss: 103.34065
disc_loss: 0.15808001
disc_acc: 0.9926980198019802

Validation results:
gen_loss: 121.19664
disc_loss: 6.233922
disc_acc: 0.9077380952380952


	Epoch 276
Training results:
gen_loss: 112.379654
disc_loss: 0.12858663
disc_acc: 0.9939356435643565

Validation results:
gen_loss: 100.6946
disc_loss: 6.1895947
disc_acc: 0.8968253968253969


	Epoch 277
Training results:
gen_loss: 108.96934
disc_loss: 0.18041581
disc_acc: 0.9917079207920793

Validation results:
gen_loss: 108.86364
disc_loss: 5.882373
disc_acc: 0.9032738095238095


	Epoch 278
Training results:
gen_loss: 108.878136
disc_loss: 0.19031635
disc_acc: 0.9922029702970298

Validation results:
gen_loss: 101.455635
disc_loss: 5.745754
disc_acc: 0.9047619047619048


	Epoch 279
Training results:
gen_loss: 102.6513
disc_loss: 0.21576521
disc_acc: 0.9913366336633663

Validation results:
gen_loss: 102.16461
disc_loss: 4.731286
disc_acc: 0.9171626984126984


	Epoch 280
Training results:
gen_loss: 102.30419
disc_loss: 0.07388631
disc_acc: 0.9959158415841585

Validation results:
gen_loss: 102.929596
disc_loss: 3.7896678
disc_acc: 0.9221230158730159


	Epoch 281
Training results:
gen_loss: 107.730804
disc_loss: 0.15409023
disc_acc: 0.994430693069307

Validation results:
gen_loss: 104.28692
disc_loss: 5.5709677
disc_acc: 0.9077380952380952


	Epoch 282
Training results:
gen_loss: 116.684616
disc_loss: 0.64601266
disc_acc: 0.9834158415841584

Validation results:
gen_loss: 112.115746
disc_loss: 6.671407
disc_acc: 0.902281746031746


	Epoch 283
Training results:
gen_loss: 115.067375
disc_loss: 0.2144941
disc_acc: 0.9907178217821783

Validation results:
gen_loss: 117.341835
disc_loss: 3.9678106
disc_acc: 0.9290674603174603


	Epoch 284
Training results:
gen_loss: 116.42619
disc_loss: 0.13878769
disc_acc: 0.9939356435643565

Validation results:
gen_loss: 123.71804
disc_loss: 4.824086
disc_acc: 0.9201388888888888


	Epoch 285
Training results:
gen_loss: 119.72319
disc_loss: 0.19865096
disc_acc: 0.9917079207920793

Validation results:
gen_loss: 122.15615
disc_loss: 5.941447
disc_acc: 0.904265873015873


	Epoch 286
Training results:
gen_loss: 116.12265
disc_loss: 0.14859287
disc_acc: 0.9939356435643565

Validation results:
gen_loss: 117.04418
disc_loss: 5.4158645
disc_acc: 0.9241071428571429


	Epoch 287
Training results:
gen_loss: 115.44225
disc_loss: 0.13921034
disc_acc: 0.9945544554455445

Validation results:
gen_loss: 115.86703
disc_loss: 5.4209256
disc_acc: 0.9216269841269841


	Epoch 288
Training results:
gen_loss: 110.2082
disc_loss: 0.26019222
disc_acc: 0.9918316831683168

Validation results:
gen_loss: 124.75419
disc_loss: 4.1665297
disc_acc: 0.9255952380952381


	Epoch 289
Training results:
gen_loss: 122.574
disc_loss: 0.27734894
disc_acc: 0.9910891089108911

Validation results:
gen_loss: 122.27365
disc_loss: 4.0565863
disc_acc: 0.935515873015873


	Epoch 290
Training results:
gen_loss: 113.00065
disc_loss: 0.16815305
disc_acc: 0.9948019801980198

Validation results:
gen_loss: 116.405266
disc_loss: 4.270968
disc_acc: 0.9280753968253969


	Epoch 291
Training results:
gen_loss: 118.08654
disc_loss: 0.18917191
disc_acc: 0.9923267326732673

Validation results:
gen_loss: 115.619606
disc_loss: 4.977373
disc_acc: 0.9171626984126984


	Epoch 292
Training results:
gen_loss: 125.17031
disc_loss: 0.16651507
disc_acc: 0.9928217821782178

Validation results:
gen_loss: 131.80449
disc_loss: 7.2061524
disc_acc: 0.9087301587301587


	Epoch 293
Training results:
gen_loss: 124.11339
disc_loss: 0.4292161
disc_acc: 0.9877475247524753

Validation results:
gen_loss: 115.96487
disc_loss: 5.2739787
disc_acc: 0.9092261904761905


	Epoch 294
Training results:
gen_loss: 123.83681
disc_loss: 0.1399517
disc_acc: 0.9943069306930693

Validation results:
gen_loss: 133.07478
disc_loss: 5.7174616
disc_acc: 0.9181547619047619


	Epoch 295
Training results:
gen_loss: 124.88763
disc_loss: 0.23128507
disc_acc: 0.9920792079207921

Validation results:
gen_loss: 125.917145
disc_loss: 5.427431
disc_acc: 0.9236111111111112


	Epoch 296
Training results:
gen_loss: 124.17175
disc_loss: 0.26088083
disc_acc: 0.9918316831683168

Validation results:
gen_loss: 131.34952
disc_loss: 6.70102
disc_acc: 0.902281746031746


	Epoch 297
Training results:
gen_loss: 115.02591
disc_loss: 0.2384328
disc_acc: 0.9922029702970298

Validation results:
gen_loss: 109.17341
disc_loss: 6.476368
disc_acc: 0.9117063492063492


	Epoch 298
Training results:
gen_loss: 116.80981
disc_loss: 0.10624727
disc_acc: 0.9961633663366337

Validation results:
gen_loss: 118.74167
disc_loss: 5.9009895
disc_acc: 0.9236111111111112


	Epoch 299
Training results:
gen_loss: 114.522964
disc_loss: 0.15986675
disc_acc: 0.9943069306930693

Validation results:
gen_loss: 114.60631
disc_loss: 6.099128
disc_acc: 0.9067460317460317


	Epoch 300
Training results:
gen_loss: 135.91284
disc_loss: 0.4076051
disc_acc: 0.9871287128712871

Validation results:
gen_loss: 121.17991
disc_loss: 6.9130664
disc_acc: 0.910218253968254


	Epoch 301
Training results:
gen_loss: 130.03523
disc_loss: 0.2785793
disc_acc: 0.9920792079207921

Validation results:
gen_loss: 122.18216
disc_loss: 7.015352
disc_acc: 0.9122023809523809


	Epoch 302
Training results:
gen_loss: 123.52836
disc_loss: 0.13098036
disc_acc: 0.995049504950495

Validation results:
gen_loss: 128.32323
disc_loss: 4.78999
disc_acc: 0.9300595238095238


	Epoch 303
Training results:
gen_loss: 131.1651
disc_loss: 0.1030106
disc_acc: 0.9965346534653465

Validation results:
gen_loss: 140.0492
disc_loss: 6.1351476
disc_acc: 0.9151785714285714


	Epoch 304
Training results:
gen_loss: 129.74747
disc_loss: 0.18557219
disc_acc: 0.994430693069307

Validation results:
gen_loss: 122.69604
disc_loss: 6.0761714
disc_acc: 0.9166666666666666


	Epoch 305
Training results:
gen_loss: 119.823616
disc_loss: 0.28622347
disc_acc: 0.9903465346534653

Validation results:
gen_loss: 120.070076
disc_loss: 5.491715
disc_acc: 0.9146825396825397


	Epoch 306
Training results:
gen_loss: 128.26323
disc_loss: 0.23263513
disc_acc: 0.9912128712871288

Validation results:
gen_loss: 129.32175
disc_loss: 6.152791
disc_acc: 0.9072420634920635


	Epoch 307
Training results:
gen_loss: 134.83174
disc_loss: 0.24250923
disc_acc: 0.9922029702970298

Validation results:
gen_loss: 136.58516
disc_loss: 6.486664
disc_acc: 0.9146825396825397


	Epoch 308
Training results:
gen_loss: 131.38689
disc_loss: 0.19747734
disc_acc: 0.9941831683168317

Validation results:
gen_loss: 133.28012
disc_loss: 5.7997255
disc_acc: 0.9250992063492064


	Epoch 309
Training results:
gen_loss: 125.654724
disc_loss: 0.20119081
disc_acc: 0.9941831683168317

Validation results:
gen_loss: 133.91008
disc_loss: 5.417246
disc_acc: 0.9270833333333334


	Epoch 310
Training results:
gen_loss: 133.68289
disc_loss: 0.28353488
disc_acc: 0.9903465346534653

Validation results:
gen_loss: 148.17935
disc_loss: 8.198225
disc_acc: 0.9067460317460317


	Epoch 311
Training results:
gen_loss: 148.0733
disc_loss: 0.33981878
disc_acc: 0.9912128712871288

Validation results:
gen_loss: 162.13664
disc_loss: 6.9479585
disc_acc: 0.9122023809523809


	Epoch 312
Training results:
gen_loss: 141.76146
disc_loss: 0.21873257
disc_acc: 0.993440594059406

Validation results:
gen_loss: 151.38907
disc_loss: 5.0017886
disc_acc: 0.9360119047619048


	Epoch 313
Training results:
gen_loss: 150.83041
disc_loss: 0.19425392
disc_acc: 0.9943069306930693

Validation results:
gen_loss: 152.41304
disc_loss: 6.2912416
disc_acc: 0.9176587301587301


	Epoch 314
Training results:
gen_loss: 142.42741
disc_loss: 0.25088692
disc_acc: 0.9923267326732673

Validation results:
gen_loss: 135.88101
disc_loss: 4.9806676
disc_acc: 0.9320436507936508


	Epoch 315
Training results:
gen_loss: 132.88768
disc_loss: 0.3186635
disc_acc: 0.9923267326732673

Validation results:
gen_loss: 132.98917
disc_loss: 6.281353
disc_acc: 0.9112103174603174


	Epoch 316
Training results:
gen_loss: 139.10739
disc_loss: 0.25024968
disc_acc: 0.9930693069306931

Validation results:
gen_loss: 132.99661
disc_loss: 5.688881
disc_acc: 0.9250992063492064


	Epoch 317
Training results:
gen_loss: 131.42754
disc_loss: 0.22394457
disc_acc: 0.993440594059406

Validation results:
gen_loss: 131.69196
disc_loss: 6.141599
disc_acc: 0.9226190476190477


	Epoch 318
Training results:
gen_loss: 135.25789
disc_loss: 0.14491853
disc_acc: 0.9956683168316832

Validation results:
gen_loss: 126.95869
disc_loss: 7.0590353
disc_acc: 0.8963293650793651


	Epoch 319
Training results:
gen_loss: 131.54988
disc_loss: 0.28949344
disc_acc: 0.991460396039604

Validation results:
gen_loss: 125.56655
disc_loss: 6.627374
disc_acc: 0.9126984126984127


	Epoch 320
Training results:
gen_loss: 129.21239
disc_loss: 0.17465052
disc_acc: 0.9941831683168317

Validation results:
gen_loss: 130.87267
disc_loss: 7.664702
disc_acc: 0.9077380952380952


	Epoch 321
Training results:
gen_loss: 130.30121
disc_loss: 0.17599067
disc_acc: 0.9943069306930693

Validation results:
gen_loss: 132.50998
disc_loss: 6.5257225
disc_acc: 0.9275793650793651


	Epoch 322
Training results:
gen_loss: 142.70018
disc_loss: 0.37841162
disc_acc: 0.989480198019802

Validation results:
gen_loss: 148.62445
disc_loss: 7.861973
disc_acc: 0.9087301587301587


	Epoch 323
Training results:
gen_loss: 133.56186
disc_loss: 0.24462609
disc_acc: 0.9933168316831683

Validation results:
gen_loss: 126.65578
disc_loss: 8.013114
disc_acc: 0.9117063492063492


	Epoch 324
Training results:
gen_loss: 128.45316
disc_loss: 0.14948216
disc_acc: 0.994430693069307

Validation results:
gen_loss: 130.1939
disc_loss: 6.0079246
disc_acc: 0.9196428571428571


	Epoch 325
Training results:
gen_loss: 132.4262
disc_loss: 0.2491921
disc_acc: 0.9923267326732673

Validation results:
gen_loss: 147.22552
disc_loss: 5.300538
disc_acc: 0.9246031746031746


	Epoch 326
Training results:
gen_loss: 150.5451
disc_loss: 0.26338208
disc_acc: 0.9920792079207921

Validation results:
gen_loss: 152.54663
disc_loss: 7.449917
disc_acc: 0.9176587301587301


	Epoch 327
Training results:
gen_loss: 156.28168
disc_loss: 0.15355143
disc_acc: 0.9949257425742575

Validation results:
gen_loss: 152.61462
disc_loss: 6.9667773
disc_acc: 0.9270833333333334


	Epoch 328
Training results:
gen_loss: 149.33818
disc_loss: 0.11162773
disc_acc: 0.9966584158415842

Validation results:
gen_loss: 138.7266
disc_loss: 5.872825
disc_acc: 0.9206349206349206


	Epoch 329
Training results:
gen_loss: 148.89195
disc_loss: 0.39618802
disc_acc: 0.989480198019802

Validation results:
gen_loss: 161.58589
disc_loss: 6.8172674
disc_acc: 0.9226190476190477


	Epoch 330
Training results:
gen_loss: 154.46184
disc_loss: 0.2663833
disc_acc: 0.9923267326732673

Validation results:
gen_loss: 148.90535
disc_loss: 5.420929
disc_acc: 0.9315476190476191


	Epoch 331
Training results:
gen_loss: 153.15929
disc_loss: 0.21523446
disc_acc: 0.994059405940594

Validation results:
gen_loss: 144.6403
disc_loss: 7.5777793
disc_acc: 0.9146825396825397


	Epoch 332
Training results:
gen_loss: 158.93344
disc_loss: 0.15372117
disc_acc: 0.9961633663366337

Validation results:
gen_loss: 160.28528
disc_loss: 8.153764
disc_acc: 0.9126984126984127


	Epoch 333
Training results:
gen_loss: 147.54062
disc_loss: 0.15325531
disc_acc: 0.9957920792079208

Validation results:
gen_loss: 141.28926
disc_loss: 7.4340525
disc_acc: 0.9047619047619048


	Epoch 334
Training results:
gen_loss: 134.26872
disc_loss: 0.30641904
disc_acc: 0.9920792079207921

Validation results:
gen_loss: 140.29646
disc_loss: 6.6340103
disc_acc: 0.9166666666666666


	Epoch 335
Training results:
gen_loss: 145.59483
disc_loss: 0.27418715
disc_acc: 0.9929455445544555

Validation results:
gen_loss: 142.13742
disc_loss: 5.8683534
disc_acc: 0.9275793650793651


	Epoch 336
Training results:
gen_loss: 144.76567
disc_loss: 0.18699262
disc_acc: 0.993440594059406

Validation results:
gen_loss: 159.6798
disc_loss: 7.8793626
disc_acc: 0.9112103174603174


	Epoch 337
Training results:
gen_loss: 152.00513
disc_loss: 0.44617638
disc_acc: 0.9896039603960396

Validation results:
gen_loss: 168.1845
disc_loss: 9.102963
disc_acc: 0.9057539682539683


	Epoch 338
Training results:
gen_loss: 159.63556
disc_loss: 0.21245258
disc_acc: 0.9943069306930693

Validation results:
gen_loss: 140.93839
disc_loss: 7.499341
disc_acc: 0.9151785714285714


	Epoch 339
Training results:
gen_loss: 156.15228
disc_loss: 0.12876303
disc_acc: 0.9957920792079208

Validation results:
gen_loss: 166.81721
disc_loss: 7.1140623
disc_acc: 0.9236111111111112


	Epoch 340
Training results:
gen_loss: 158.40028
disc_loss: 0.31954795
disc_acc: 0.9915841584158416

Validation results:
gen_loss: 158.7507
disc_loss: 7.9625163
disc_acc: 0.9146825396825397


	Epoch 341
Training results:
gen_loss: 156.8113
disc_loss: 0.4178163
disc_acc: 0.990470297029703

Validation results:
gen_loss: 167.29082
disc_loss: 7.355009
disc_acc: 0.9221230158730159


	Epoch 342
Training results:
gen_loss: 158.6518
disc_loss: 0.26571465
disc_acc: 0.9943069306930693

Validation results:
gen_loss: 162.20819
disc_loss: 6.3863883
disc_acc: 0.9315476190476191


	Epoch 343
Training results:
gen_loss: 155.51488
disc_loss: 0.117635384
disc_acc: 0.996410891089109

Validation results:
gen_loss: 154.8113
disc_loss: 6.8606725
disc_acc: 0.9196428571428571


	Epoch 344
Training results:
gen_loss: 152.27257
disc_loss: 0.1749557
disc_acc: 0.995049504950495

Validation results:
gen_loss: 151.56908
disc_loss: 6.107802
disc_acc: 0.9255952380952381


	Epoch 345
Training results:
gen_loss: 145.6526
disc_loss: 0.08473944
disc_acc: 0.996410891089109

Validation results:
gen_loss: 152.36731
disc_loss: 7.2106695
disc_acc: 0.9196428571428571


	Epoch 346
Training results:
gen_loss: 158.76779
disc_loss: 0.31616116
disc_acc: 0.9913366336633663

Validation results:
gen_loss: 160.18715
disc_loss: 6.7882123
disc_acc: 0.9241071428571429


	Epoch 347
Training results:
gen_loss: 161.33202
disc_loss: 0.6208124
disc_acc: 0.9903465346534653

Validation results:
gen_loss: 171.8147
disc_loss: 9.465298
disc_acc: 0.9087301587301587


	Epoch 348
Training results:
gen_loss: 172.63673
disc_loss: 0.39501646
disc_acc: 0.9929455445544555

Validation results:
gen_loss: 165.82767
disc_loss: 6.744434
disc_acc: 0.9290674603174603


	Epoch 349
Training results:
gen_loss: 163.4457
disc_loss: 0.20874101
disc_acc: 0.9949257425742575

Validation results:
gen_loss: 168.33377
disc_loss: 8.285885
disc_acc: 0.9181547619047619


	Epoch 350
Training results:
gen_loss: 168.19868
disc_loss: 0.12236376
disc_acc: 0.9969059405940595

Validation results:
gen_loss: 178.74098
disc_loss: 7.472915
disc_acc: 0.9206349206349206


	Epoch 351
Training results:
gen_loss: 169.72466
disc_loss: 0.29730526
disc_acc: 0.9928217821782178

Validation results:
gen_loss: 156.36667
disc_loss: 7.2770524
disc_acc: 0.9107142857142857


	Epoch 352
Training results:
gen_loss: 164.6142
disc_loss: 0.35031193
disc_acc: 0.9929455445544555

Validation results:
gen_loss: 156.12073
disc_loss: 6.5024
disc_acc: 0.9241071428571429


	Epoch 353
Training results:
gen_loss: 166.82265
disc_loss: 0.12798578
disc_acc: 0.9956683168316832

Validation results:
gen_loss: 176.05406
disc_loss: 6.779035
disc_acc: 0.9236111111111112


	Epoch 354
Training results:
gen_loss: 164.4927
disc_loss: 0.20197137
disc_acc: 0.9956683168316832

Validation results:
gen_loss: 164.63248
disc_loss: 10.1454735
disc_acc: 0.9037698412698413


	Epoch 355
Training results:
gen_loss: 150.40918
disc_loss: 0.1781503
disc_acc: 0.994430693069307

Validation results:
gen_loss: 154.94829
disc_loss: 10.21564
disc_acc: 0.904265873015873


	Epoch 356
Training results:
gen_loss: 166.36983
disc_loss: 0.49764016
disc_acc: 0.9899752475247525

Validation results:
gen_loss: 173.94405
disc_loss: 9.72549
disc_acc: 0.90625


	Epoch 357
Training results:
gen_loss: 166.17361
disc_loss: 0.1796766
disc_acc: 0.995049504950495

Validation results:
gen_loss: 158.45041
disc_loss: 8.395349
disc_acc: 0.9191468253968254


	Epoch 358
Training results:
gen_loss: 161.22227
disc_loss: 0.13718961
disc_acc: 0.9966584158415842

Validation results:
gen_loss: 148.10947
disc_loss: 10.354094
disc_acc: 0.8943452380952381


	Epoch 359
Training results:
gen_loss: 158.6283
disc_loss: 0.41599578
disc_acc: 0.991460396039604

Validation results:
gen_loss: 163.89917
disc_loss: 6.966901
disc_acc: 0.9196428571428571


	Epoch 360
Training results:
gen_loss: 163.58289
disc_loss: 0.19997308
disc_acc: 0.9938118811881188

Validation results:
gen_loss: 162.87982
disc_loss: 7.713667
disc_acc: 0.9250992063492064


	Epoch 361
Training results:
gen_loss: 158.57831
disc_loss: 0.21288712
disc_acc: 0.9935643564356436

Validation results:
gen_loss: 153.12254
disc_loss: 7.257531
disc_acc: 0.9171626984126984


	Epoch 362
Training results:
gen_loss: 169.96576
disc_loss: 0.27901033
disc_acc: 0.993440594059406

Validation results:
gen_loss: 163.21129
disc_loss: 8.183892
disc_acc: 0.9216269841269841


	Epoch 363
Training results:
gen_loss: 173.25894
disc_loss: 0.25993243
disc_acc: 0.9939356435643565

Validation results:
gen_loss: 182.74037
disc_loss: 12.382181
disc_acc: 0.8958333333333334


	Epoch 364
Training results:
gen_loss: 173.13394
disc_loss: 0.25526297
disc_acc: 0.994059405940594

Validation results:
gen_loss: 169.429
disc_loss: 6.855338
disc_acc: 0.9196428571428571


	Epoch 365
Training results:
gen_loss: 171.47655
disc_loss: 0.1872234
disc_acc: 0.9951732673267327

Validation results:
gen_loss: 167.78801
disc_loss: 7.9149275
disc_acc: 0.9181547619047619


	Epoch 366
Training results:
gen_loss: 162.90727
disc_loss: 0.14980294
disc_acc: 0.995420792079208

Validation results:
gen_loss: 180.43893
disc_loss: 10.508953
disc_acc: 0.90625


	Epoch 367
Training results:
gen_loss: 183.50508
disc_loss: 0.37273523
disc_acc: 0.9913366336633663

Validation results:
gen_loss: 174.8809
disc_loss: 9.491793
disc_acc: 0.9136904761904762


	Epoch 368
Training results:
gen_loss: 175.0771
disc_loss: 0.3107298
disc_acc: 0.9930693069306931

Validation results:
gen_loss: 183.37271
disc_loss: 7.545659
disc_acc: 0.9216269841269841


	Epoch 369
Training results:
gen_loss: 185.481
disc_loss: 0.24970177
disc_acc: 0.9939356435643565

Validation results:
gen_loss: 192.91475
disc_loss: 7.4900246
disc_acc: 0.9221230158730159


	Epoch 370
Training results:
gen_loss: 179.38412
disc_loss: 0.16122083
disc_acc: 0.9956683168316832

Validation results:
gen_loss: 186.42876
disc_loss: 9.969078
disc_acc: 0.90625


	Epoch 371
Training results:
gen_loss: 179.46712
disc_loss: 0.2308494
disc_acc: 0.9952970297029703

Validation results:
gen_loss: 195.88385
disc_loss: 7.8343725
disc_acc: 0.9211309523809523


	Epoch 372
Training results:
gen_loss: 182.17479
disc_loss: 0.18310282
disc_acc: 0.995420792079208

Validation results:
gen_loss: 173.27718
disc_loss: 8.256981
disc_acc: 0.90625


	Epoch 373
Training results:
gen_loss: 192.06773
disc_loss: 0.3035779
disc_acc: 0.9930693069306931

Validation results:
gen_loss: 193.22784
disc_loss: 11.677448
disc_acc: 0.9117063492063492


	Epoch 374
Training results:
gen_loss: 187.50848
disc_loss: 0.3915026
disc_acc: 0.9930693069306931

Validation results:
gen_loss: 183.96053
disc_loss: 6.721618
disc_acc: 0.9280753968253969


	Epoch 375
Training results:
gen_loss: 178.84915
disc_loss: 0.18373181
disc_acc: 0.9948019801980198

Validation results:
gen_loss: 177.99211
disc_loss: 8.886631
disc_acc: 0.9166666666666666


	Epoch 376
Training results:
gen_loss: 178.08998
disc_loss: 0.28172383
disc_acc: 0.993440594059406

Validation results:
gen_loss: 180.86983
disc_loss: 9.621118
disc_acc: 0.9117063492063492


	Epoch 377
Training results:
gen_loss: 177.48172
disc_loss: 0.34450924
disc_acc: 0.9930693069306931

Validation results:
gen_loss: 177.60564
disc_loss: 12.6784315
disc_acc: 0.8938492063492064


	Epoch 378
Training results:
gen_loss: 178.59549
disc_loss: 0.2027386
disc_acc: 0.9956683168316832

Validation results:
gen_loss: 185.63104
disc_loss: 7.459863
disc_acc: 0.9285714285714286


	Epoch 379
Training results:
gen_loss: 187.55739
disc_loss: 0.42999244
disc_acc: 0.9912128712871288

Validation results:
gen_loss: 197.79047
disc_loss: 9.306908
disc_acc: 0.9191468253968254


	Epoch 380
Training results:
gen_loss: 189.63815
disc_loss: 0.042918168
disc_acc: 0.9985148514851485

Validation results:
gen_loss: 200.23488
disc_loss: 7.785804
disc_acc: 0.9310515873015873


	Epoch 381
Training results:
gen_loss: 181.6248
disc_loss: 0.124770716
disc_acc: 0.9969059405940595

Validation results:
gen_loss: 170.38243
disc_loss: 6.675118
disc_acc: 0.9250992063492064


	Epoch 382
Training results:
gen_loss: 182.9785
disc_loss: 0.73835367
disc_acc: 0.9881188118811881

Validation results:
gen_loss: 188.65254
disc_loss: 7.337874
disc_acc: 0.9211309523809523


	Epoch 383
Training results:
gen_loss: 192.69656
disc_loss: 0.19634023
disc_acc: 0.9949257425742575

Validation results:
gen_loss: 200.55754
disc_loss: 7.248848
disc_acc: 0.9345238095238095


	Epoch 384
Training results:
gen_loss: 187.69656
disc_loss: 0.15181513
disc_acc: 0.9952970297029703

Validation results:
gen_loss: 185.44542
disc_loss: 8.211712
disc_acc: 0.9211309523809523


	Epoch 385
Training results:
gen_loss: 191.10696
disc_loss: 0.25480023
disc_acc: 0.9941831683168317

Validation results:
gen_loss: 185.67767
disc_loss: 10.690726
disc_acc: 0.9087301587301587


	Epoch 386
Training results:
gen_loss: 202.23155
disc_loss: 0.13065058
disc_acc: 0.9966584158415842

Validation results:
gen_loss: 221.56207
disc_loss: 9.512676
disc_acc: 0.9260912698412699


	Epoch 387
Training results:
gen_loss: 199.11308
disc_loss: 0.3842389
disc_acc: 0.993440594059406

Validation results:
gen_loss: 171.39203
disc_loss: 10.02101
disc_acc: 0.908234126984127


	Epoch 388
Training results:
gen_loss: 184.53394
disc_loss: 0.48758525
disc_acc: 0.9913366336633663

Validation results:
gen_loss: 185.64383
disc_loss: 8.300359
disc_acc: 0.9250992063492064


	Epoch 389
Training results:
gen_loss: 187.83018
disc_loss: 0.36854598
disc_acc: 0.9926980198019802

Validation results:
gen_loss: 189.39403
disc_loss: 9.570182
disc_acc: 0.9176587301587301


	Epoch 390
Training results:
gen_loss: 185.5621
disc_loss: 0.17473918
disc_acc: 0.9952970297029703

Validation results:
gen_loss: 190.78304
disc_loss: 10.211438
disc_acc: 0.9136904761904762


	Epoch 391
Training results:
gen_loss: 180.47705
disc_loss: 0.15009148
disc_acc: 0.9969059405940595

Validation results:
gen_loss: 181.40324
disc_loss: 7.8105063
disc_acc: 0.9196428571428571


	Epoch 392
Training results:
gen_loss: 189.59677
disc_loss: 0.23584807
disc_acc: 0.995049504950495

Validation results:
gen_loss: 190.5344
disc_loss: 8.590146
disc_acc: 0.9211309523809523


	Epoch 393
Training results:
gen_loss: 200.4388
disc_loss: 0.4105625
disc_acc: 0.9920792079207921

Validation results:
gen_loss: 189.81326
disc_loss: 7.9567866
disc_acc: 0.9315476190476191


	Epoch 394
Training results:
gen_loss: 198.1332
disc_loss: 0.21987115
disc_acc: 0.9959158415841585

Validation results:
gen_loss: 186.78554
disc_loss: 8.861129
disc_acc: 0.9216269841269841


	Epoch 395
Training results:
gen_loss: 192.4705
disc_loss: 0.05884581
disc_acc: 0.9977722772277228

Validation results:
gen_loss: 198.94606
disc_loss: 8.067232
disc_acc: 0.9370039682539683


	Epoch 396
Training results:
gen_loss: 196.59805
disc_loss: 0.12469477
disc_acc: 0.99740099009901

Validation results:
gen_loss: 198.1781
disc_loss: 9.50185
disc_acc: 0.9241071428571429


	Epoch 397
Training results:
gen_loss: 220.26216
disc_loss: 0.76450604
disc_acc: 0.986509900990099

Validation results:
gen_loss: 213.07532
disc_loss: 10.8557005
disc_acc: 0.9191468253968254


	Epoch 398
Training results:
gen_loss: 222.72472
disc_loss: 0.27765256
disc_acc: 0.9941831683168317

Validation results:
gen_loss: 220.21199
disc_loss: 12.160094
disc_acc: 0.908234126984127


	Epoch 399
Training results:
gen_loss: 215.59737
disc_loss: 0.26266995
disc_acc: 0.9935643564356436

Validation results:
gen_loss: 205.90976
disc_loss: 10.451609
disc_acc: 0.9181547619047619


	Epoch 400
Training results:
gen_loss: 207.58347
disc_loss: 0.11842812
disc_acc: 0.9972772277227723

Validation results:
gen_loss: 222.5887
disc_loss: 9.260268
disc_acc: 0.9295634920634921


	Epoch 401
Training results:
gen_loss: 214.68822
disc_loss: 0.3357631
disc_acc: 0.9941831683168317

Validation results:
gen_loss: 194.08722
disc_loss: 10.475052
disc_acc: 0.9166666666666666


	Epoch 402
Training results:
gen_loss: 195.16489
disc_loss: 0.11857477
disc_acc: 0.9971534653465347

Validation results:
gen_loss: 210.91289
disc_loss: 9.625143
disc_acc: 0.9181547619047619


	Epoch 403
Training results:
gen_loss: 204.6576
disc_loss: 0.41052216
disc_acc: 0.993440594059406

Validation results:
gen_loss: 231.99515
disc_loss: 10.863873
disc_acc: 0.9166666666666666


	Epoch 404
Training results:
gen_loss: 223.7959
disc_loss: 0.22881229
disc_acc: 0.9931930693069307

Validation results:
gen_loss: 197.8269
disc_loss: 9.135273
disc_acc: 0.9246031746031746


	Epoch 405
Training results:
gen_loss: 207.47528
disc_loss: 0.19891387
disc_acc: 0.9956683168316832

Validation results:
gen_loss: 201.78896
disc_loss: 9.868096
disc_acc: 0.9241071428571429


	Epoch 406
Training results:
gen_loss: 213.91565
disc_loss: 0.24588901
disc_acc: 0.9952970297029703

Validation results:
gen_loss: 226.09477
disc_loss: 9.2406
disc_acc: 0.9206349206349206


	Epoch 407
Training results:
gen_loss: 222.93307
disc_loss: 0.41699997
disc_acc: 0.9920792079207921

Validation results:
gen_loss: 237.55452
disc_loss: 12.858564
disc_acc: 0.9047619047619048


	Epoch 408
Training results:
gen_loss: 230.00099
disc_loss: 0.33931664
disc_acc: 0.9938118811881188

Validation results:
gen_loss: 214.95259
disc_loss: 9.76616
disc_acc: 0.9231150793650794


	Epoch 409
Training results:
gen_loss: 206.29196
disc_loss: 0.17853989
disc_acc: 0.9967821782178218

Validation results:
gen_loss: 221.0295
disc_loss: 12.841839
disc_acc: 0.9032738095238095


	Epoch 410
Training results:
gen_loss: 212.90266
disc_loss: 0.3068465
disc_acc: 0.9945544554455445

Validation results:
gen_loss: 207.21646
disc_loss: 11.176384
disc_acc: 0.9122023809523809


	Epoch 411
Training results:
gen_loss: 195.93816
disc_loss: 0.14516975
disc_acc: 0.9967821782178218

Validation results:
gen_loss: 197.81961
disc_loss: 8.343913
disc_acc: 0.9270833333333334


	Epoch 412
Training results:
gen_loss: 191.5318
disc_loss: 0.35022974
disc_acc: 0.9939356435643565

Validation results:
gen_loss: 196.25041
disc_loss: 10.084861
disc_acc: 0.9236111111111112


	Epoch 413
Training results:
gen_loss: 198.4746
disc_loss: 0.17701842
disc_acc: 0.9967821782178218

Validation results:
gen_loss: 199.05618
disc_loss: 9.876098
disc_acc: 0.9285714285714286


	Epoch 414
Training results:
gen_loss: 203.01466
disc_loss: 0.52331096
disc_acc: 0.9919554455445545

Validation results:
gen_loss: 216.79594
disc_loss: 11.220028
disc_acc: 0.9136904761904762


	Epoch 415
Training results:
gen_loss: 206.60637
disc_loss: 0.22691025
disc_acc: 0.995049504950495

Validation results:
gen_loss: 220.36981
disc_loss: 14.930982
disc_acc: 0.9092261904761905


	Epoch 416
Training results:
gen_loss: 221.64542
disc_loss: 0.34059533
disc_acc: 0.9946782178217822

Validation results:
gen_loss: 216.8088
disc_loss: 10.392987
disc_acc: 0.9141865079365079


	Epoch 417
Training results:
gen_loss: 213.2525
disc_loss: 0.17887013
disc_acc: 0.9966584158415842

Validation results:
gen_loss: 206.30244
disc_loss: 9.239464
disc_acc: 0.9290674603174603


	Epoch 418
Training results:
gen_loss: 211.28835
disc_loss: 0.3506176
disc_acc: 0.994059405940594

Validation results:
gen_loss: 204.36462
disc_loss: 11.21044
disc_acc: 0.9126984126984127


	Epoch 419
Training results:
gen_loss: 211.90244
disc_loss: 0.2860427
disc_acc: 0.9948019801980198

Validation results:
gen_loss: 215.47034
disc_loss: 8.584534
disc_acc: 0.9310515873015873


	Epoch 420
Training results:
gen_loss: 209.24214
disc_loss: 0.031014567
disc_acc: 0.9988861386138614

Validation results:
gen_loss: 210.09973
disc_loss: 9.446764
disc_acc: 0.9246031746031746


	Epoch 421
Training results:
gen_loss: 204.14189
disc_loss: 0.24697174
disc_acc: 0.994430693069307

Validation results:
gen_loss: 199.50389
disc_loss: 9.495218
disc_acc: 0.9186507936507936


	Epoch 422
Training results:
gen_loss: 216.52565
disc_loss: 0.2710288
disc_acc: 0.994059405940594

Validation results:
gen_loss: 222.51056
disc_loss: 11.338685
disc_acc: 0.9186507936507936


	Epoch 423
Training results:
gen_loss: 229.72375
disc_loss: 0.40604028
disc_acc: 0.9941831683168317

Validation results:
gen_loss: 247.81398
disc_loss: 12.07776
disc_acc: 0.90625


	Epoch 424
Training results:
gen_loss: 221.32823
disc_loss: 0.41473797
disc_acc: 0.9941831683168317

Validation results:
gen_loss: 215.91594
disc_loss: 10.356793
disc_acc: 0.9176587301587301


	Epoch 425
Training results:
gen_loss: 211.42227
disc_loss: 0.22906232
disc_acc: 0.9961633663366337

Validation results:
gen_loss: 212.01527
disc_loss: 11.94196
disc_acc: 0.9176587301587301


	Epoch 426
Training results:
gen_loss: 213.39354
disc_loss: 0.09804096
disc_acc: 0.9977722772277228

Validation results:
gen_loss: 217.66852
disc_loss: 9.781911
disc_acc: 0.9260912698412699


	Epoch 427
Training results:
gen_loss: 212.68593
disc_loss: 0.31424695
disc_acc: 0.995049504950495

Validation results:
gen_loss: 210.89583
disc_loss: 13.611214
disc_acc: 0.9136904761904762


	Epoch 428
Training results:
gen_loss: 219.72955
disc_loss: 0.549804
disc_acc: 0.9918316831683168

Validation results:
gen_loss: 229.9156
disc_loss: 11.665456
disc_acc: 0.9166666666666666


	Epoch 429
Training results:
gen_loss: 234.61954
disc_loss: 0.37316766
disc_acc: 0.9935643564356436

Validation results:
gen_loss: 231.94514
disc_loss: 10.5431795
disc_acc: 0.9156746031746031


	Epoch 430
Training results:
gen_loss: 228.02168
disc_loss: 0.23468047
disc_acc: 0.9966584158415842

Validation results:
gen_loss: 212.28171
disc_loss: 12.610695
disc_acc: 0.9037698412698413


	Epoch 431
Training results:
gen_loss: 220.59703
disc_loss: 0.1961585
disc_acc: 0.9965346534653465

Validation results:
gen_loss: 233.07324
disc_loss: 10.00934
disc_acc: 0.9181547619047619


	Epoch 432
Training results:
gen_loss: 218.29596
disc_loss: 0.09640372
disc_acc: 0.9972772277227723

Validation results:
gen_loss: 222.26549
disc_loss: 11.345974
disc_acc: 0.9191468253968254


	Epoch 433
Training results:
gen_loss: 235.18103
disc_loss: 0.3692426
disc_acc: 0.993440594059406

Validation results:
gen_loss: 243.34384
disc_loss: 16.47061
disc_acc: 0.9012896825396826


	Epoch 434
Training results:
gen_loss: 229.36375
disc_loss: 0.571723
disc_acc: 0.9922029702970298

Validation results:
gen_loss: 233.5712
disc_loss: 11.6
disc_acc: 0.9211309523809523


	Epoch 435
Training results:
gen_loss: 230.68683
disc_loss: 0.20439148
disc_acc: 0.997029702970297

Validation results:
gen_loss: 213.61748
disc_loss: 10.617674
disc_acc: 0.9226190476190477


	Epoch 436
Training results:
gen_loss: 219.1682
disc_loss: 0.16686457
disc_acc: 0.9965346534653465

Validation results:
gen_loss: 216.56541
disc_loss: 11.876498
disc_acc: 0.9112103174603174


	Epoch 437
Training results:
gen_loss: 202.6448
disc_loss: 0.3266095
disc_acc: 0.9957920792079208

Validation results:
gen_loss: 208.89899
disc_loss: 10.853148
disc_acc: 0.9141865079365079


	Epoch 438
Training results:
gen_loss: 211.03113
disc_loss: 0.27209055
disc_acc: 0.9935643564356436

Validation results:
gen_loss: 206.00465
disc_loss: 9.520694
disc_acc: 0.9260912698412699


	Epoch 439
Training results:
gen_loss: 215.0751
disc_loss: 0.15573597
disc_acc: 0.996039603960396

Validation results:
gen_loss: 219.71246
disc_loss: 12.789697
disc_acc: 0.9176587301587301


	Epoch 440
Training results:
gen_loss: 219.30338
disc_loss: 0.35868612
disc_acc: 0.9938118811881188

Validation results:
gen_loss: 233.31966
disc_loss: 10.374928
disc_acc: 0.9340277777777778


	Epoch 441
Training results:
gen_loss: 209.12317
disc_loss: 0.15247458
disc_acc: 0.9967821782178218

Validation results:
gen_loss: 207.14337
disc_loss: 12.2793
disc_acc: 0.9176587301587301


	Epoch 442
Training results:
gen_loss: 214.33725
disc_loss: 0.33397165
disc_acc: 0.9951732673267327

Validation results:
gen_loss: 249.21861
disc_loss: 12.097722
disc_acc: 0.9141865079365079


	Epoch 443
Training results:
gen_loss: 242.67387
disc_loss: 0.7080229
disc_acc: 0.9908415841584158

Validation results:
gen_loss: 235.32114
disc_loss: 12.092447
disc_acc: 0.9126984126984127


	Epoch 444
Training results:
gen_loss: 243.65303
disc_loss: 0.22206177
disc_acc: 0.996039603960396

Validation results:
gen_loss: 239.72975
disc_loss: 10.906119
disc_acc: 0.9236111111111112


	Epoch 445
Training results:
gen_loss: 237.10884
disc_loss: 0.09056034
disc_acc: 0.998019801980198

Validation results:
gen_loss: 234.69588
disc_loss: 9.331522
disc_acc: 0.9250992063492064


	Epoch 446
Training results:
gen_loss: 238.81738
disc_loss: 0.1877612
disc_acc: 0.996410891089109

Validation results:
gen_loss: 246.13875
disc_loss: 9.786119
disc_acc: 0.9241071428571429


	Epoch 447
Training results:
gen_loss: 235.39877
disc_loss: 0.31701872
disc_acc: 0.9951732673267327

Validation results:
gen_loss: 226.93782
disc_loss: 9.0614805
disc_acc: 0.9290674603174603


	Epoch 448
Training results:
gen_loss: 230.46008
disc_loss: 0.2798058
disc_acc: 0.994059405940594

Validation results:
gen_loss: 237.27292
disc_loss: 13.542317
disc_acc: 0.9117063492063492


	Epoch 449
Training results:
gen_loss: 251.87587
disc_loss: 0.30996826
disc_acc: 0.9941831683168317

Validation results:
gen_loss: 250.58179
disc_loss: 10.965574
disc_acc: 0.9186507936507936


	Epoch 450
Training results:
gen_loss: 243.18733
disc_loss: 0.1557897
disc_acc: 0.996410891089109

Validation results:
gen_loss: 234.17212
disc_loss: 10.96466
disc_acc: 0.9255952380952381


	Epoch 451
Training results:
gen_loss: 241.91814
disc_loss: 0.17208444
disc_acc: 0.997029702970297

Validation results:
gen_loss: 242.49046
disc_loss: 12.078304
disc_acc: 0.9255952380952381


	Epoch 452
Training results:
gen_loss: 254.74852
disc_loss: 0.34565595
disc_acc: 0.9948019801980198

Validation results:
gen_loss: 262.10095
disc_loss: 14.281419
disc_acc: 0.90625


	Epoch 453
Training results:
gen_loss: 228.50797
disc_loss: 0.40862808
disc_acc: 0.9930693069306931

Validation results:
gen_loss: 220.17677
disc_loss: 11.162003
disc_acc: 0.9206349206349206


	Epoch 454
Training results:
gen_loss: 255.13426
disc_loss: 0.2595337
disc_acc: 0.9951732673267327

Validation results:
gen_loss: 262.85114
disc_loss: 12.436515
disc_acc: 0.9221230158730159


	Epoch 455
Training results:
gen_loss: 249.33672
disc_loss: 0.22482932
disc_acc: 0.9966584158415842

Validation results:
gen_loss: 261.21213
disc_loss: 16.628946
disc_acc: 0.902281746031746


	Epoch 456
Training results:
gen_loss: 246.43822
disc_loss: 0.55045563
disc_acc: 0.9929455445544555

Validation results:
gen_loss: 244.44421
disc_loss: 12.741296
disc_acc: 0.9176587301587301


	Epoch 457
Training results:
gen_loss: 246.11984
disc_loss: 0.16690151
disc_acc: 0.9971534653465347

Validation results:
gen_loss: 261.23373
disc_loss: 11.831136
disc_acc: 0.9250992063492064


	Epoch 458
Training results:
gen_loss: 254.05067
disc_loss: 0.38351533
disc_acc: 0.9943069306930693

Validation results:
gen_loss: 241.06897
disc_loss: 16.986256
disc_acc: 0.8968253968253969


	Epoch 459
Training results:
gen_loss: 259.58264
disc_loss: 0.36025625
disc_acc: 0.9939356435643565

Validation results:
gen_loss: 238.6602
disc_loss: 15.157038
disc_acc: 0.9131944444444444


	Epoch 460
Training results:
gen_loss: 245.82289
disc_loss: 0.23662351
disc_acc: 0.9955445544554455

Validation results:
gen_loss: 247.59589
disc_loss: 13.00116
disc_acc: 0.9226190476190477


	Epoch 461
Training results:
gen_loss: 254.87134
disc_loss: 0.15539296
disc_acc: 0.9972772277227723

Validation results:
gen_loss: 254.02264
disc_loss: 12.635233
disc_acc: 0.9196428571428571


	Epoch 462
Training results:
gen_loss: 241.95158
disc_loss: 0.09533341
disc_acc: 0.9985148514851485

Validation results:
gen_loss: 240.61234
disc_loss: 10.716093
disc_acc: 0.9295634920634921


	Epoch 463
Training results:
gen_loss: 255.87717
disc_loss: 0.34514204
disc_acc: 0.9943069306930693

Validation results:
gen_loss: 255.82187
disc_loss: 17.736921
disc_acc: 0.8953373015873016


	Epoch 464
Training results:
gen_loss: 257.42966
disc_loss: 0.3936072
disc_acc: 0.9930693069306931

Validation results:
gen_loss: 242.39482
disc_loss: 11.754222
disc_acc: 0.9126984126984127


	Epoch 465
Training results:
gen_loss: 250.63094
disc_loss: 0.25679576
disc_acc: 0.9957920792079208

Validation results:
gen_loss: 273.7397
disc_loss: 11.197147
disc_acc: 0.9246031746031746


	Epoch 466
Training results:
gen_loss: 266.807
disc_loss: 0.30347186
disc_acc: 0.9936881188118812

Validation results:
gen_loss: 260.61838
disc_loss: 12.8961
disc_acc: 0.9161706349206349


	Epoch 467
Training results:
gen_loss: 269.9129
disc_loss: 0.26885474
disc_acc: 0.9956683168316832

Validation results:
gen_loss: 271.5888
disc_loss: 11.831532
disc_acc: 0.9250992063492064


	Epoch 468
Training results:
gen_loss: 283.28775
disc_loss: 0.26462147
disc_acc: 0.995049504950495

Validation results:
gen_loss: 297.75803
disc_loss: 15.627441
disc_acc: 0.9166666666666666


	Epoch 469
Training results:
gen_loss: 278.92883
disc_loss: 0.43409458
disc_acc: 0.9948019801980198

Validation results:
gen_loss: 268.3704
disc_loss: 12.784003
disc_acc: 0.9255952380952381


	Epoch 470
Training results:
gen_loss: 275.56146
disc_loss: 0.26475722
disc_acc: 0.9956683168316832

Validation results:
gen_loss: 270.946
disc_loss: 15.339419
disc_acc: 0.9151785714285714


	Epoch 471
Training results:
gen_loss: 270.42435
disc_loss: 0.3128482
disc_acc: 0.9955445544554455

Validation results:
gen_loss: 254.0469
disc_loss: 18.05675
disc_acc: 0.9007936507936508


	Epoch 472
Training results:
gen_loss: 255.12492
disc_loss: 0.25573242
disc_acc: 0.9967821782178218

Validation results:
gen_loss: 251.358
disc_loss: 13.65829
disc_acc: 0.9221230158730159


	Epoch 473
Training results:
gen_loss: 264.5277
disc_loss: 0.30187383
disc_acc: 0.9955445544554455

Validation results:
gen_loss: 268.45923
disc_loss: 13.805785
disc_acc: 0.9186507936507936


	Epoch 474
Training results:
gen_loss: 254.94037
disc_loss: 0.4831925
disc_acc: 0.9931930693069307

Validation results:
gen_loss: 251.72115
disc_loss: 15.2957535
disc_acc: 0.9146825396825397


	Epoch 475
Training results:
gen_loss: 288.08417
disc_loss: 0.45536155
disc_acc: 0.9945544554455445

Validation results:
gen_loss: 291.40515
disc_loss: 15.647431
disc_acc: 0.9206349206349206


	Epoch 476
Training results:
gen_loss: 296.40863
disc_loss: 0.13347183
disc_acc: 0.9982673267326733

Validation results:
gen_loss: 274.0517
disc_loss: 16.892654
disc_acc: 0.9072420634920635


	Epoch 477
Training results:
gen_loss: 262.41476
disc_loss: 0.36399326
disc_acc: 0.9965346534653465

Validation results:
gen_loss: 243.73003
disc_loss: 13.514229
disc_acc: 0.908234126984127


	Epoch 478
Training results:
gen_loss: 261.93933
disc_loss: 0.22040407
disc_acc: 0.9959158415841585

Validation results:
gen_loss: 265.18826
disc_loss: 13.078784
disc_acc: 0.9191468253968254


	Epoch 479
Training results:
gen_loss: 268.49777
disc_loss: 0.22643903
disc_acc: 0.9955445544554455

Validation results:
gen_loss: 290.78867
disc_loss: 17.110592
disc_acc: 0.9107142857142857


	Epoch 480
Training results:
gen_loss: 272.5156
disc_loss: 0.39153534
disc_acc: 0.9948019801980198

Validation results:
gen_loss: 253.14703
disc_loss: 15.472323
disc_acc: 0.9231150793650794


	Epoch 481
Training results:
gen_loss: 262.51193
disc_loss: 0.29643673
disc_acc: 0.995420792079208

Validation results:
gen_loss: 273.17102
disc_loss: 13.940336
disc_acc: 0.9236111111111112


	Epoch 482
Training results:
gen_loss: 264.79474
disc_loss: 0.13438207
disc_acc: 0.9977722772277228

Validation results:
gen_loss: 255.48909
disc_loss: 12.576158
disc_acc: 0.9290674603174603


	Epoch 483
Training results:
gen_loss: 262.03177
disc_loss: 0.2904483
disc_acc: 0.9955445544554455

Validation results:
gen_loss: 264.32394
disc_loss: 16.551638
disc_acc: 0.9151785714285714


	Epoch 484
Training results:
gen_loss: 273.12198
disc_loss: 0.5101551
disc_acc: 0.9928217821782178

Validation results:
gen_loss: 277.60803
disc_loss: 14.063057
disc_acc: 0.9161706349206349


	Epoch 485
Training results:
gen_loss: 277.48718
disc_loss: 0.3064737
disc_acc: 0.995420792079208

Validation results:
gen_loss: 279.46655
disc_loss: 16.479248
disc_acc: 0.9112103174603174


	Epoch 486
Training results:
gen_loss: 274.17798
disc_loss: 0.18985748
disc_acc: 0.99740099009901

Validation results:
gen_loss: 287.51233
disc_loss: 12.350107
disc_acc: 0.9305555555555556


	Epoch 487
Training results:
gen_loss: 282.80145
disc_loss: 0.22546402
disc_acc: 0.997029702970297

Validation results:
gen_loss: 275.90344
disc_loss: 12.776074
disc_acc: 0.9221230158730159


	Epoch 488
Training results:
gen_loss: 284.9733
disc_loss: 0.40897524
disc_acc: 0.9939356435643565

Validation results:
gen_loss: 293.53284
disc_loss: 12.545423
disc_acc: 0.9295634920634921


	Epoch 489
Training results:
gen_loss: 260.19266
disc_loss: 0.3404073
disc_acc: 0.9955445544554455

Validation results:
gen_loss: 247.22014
disc_loss: 11.406876
disc_acc: 0.9231150793650794


	Epoch 490
Training results:
gen_loss: 260.66376
disc_loss: 0.18963258
disc_acc: 0.9967821782178218

Validation results:
gen_loss: 255.57033
disc_loss: 12.736177
disc_acc: 0.9206349206349206


	Epoch 491
Training results:
gen_loss: 264.96826
disc_loss: 0.26367202
disc_acc: 0.9952970297029703

Validation results:
gen_loss: 290.44528
disc_loss: 15.019438
disc_acc: 0.9131944444444444


	Epoch 492
Training results:
gen_loss: 282.66818
disc_loss: 0.3877117
disc_acc: 0.9943069306930693

Validation results:
gen_loss: 273.2068
disc_loss: 16.343145
disc_acc: 0.9206349206349206


	Epoch 493
Training results:
gen_loss: 285.5792
disc_loss: 0.23837058
disc_acc: 0.996039603960396

Validation results:
gen_loss: 294.52603
disc_loss: 17.302189
disc_acc: 0.9107142857142857


	Epoch 494
Training results:
gen_loss: 295.13187
disc_loss: 0.4724956
disc_acc: 0.9933168316831683

Validation results:
gen_loss: 288.37482
disc_loss: 17.68368
disc_acc: 0.9161706349206349


	Epoch 495
Training results:
gen_loss: 280.35455
disc_loss: 0.078755364
disc_acc: 0.9981435643564357

Validation results:
gen_loss: 271.06424
disc_loss: 13.796104
disc_acc: 0.9221230158730159


	Epoch 496
Training results:
gen_loss: 282.6828
disc_loss: 0.2591815
disc_acc: 0.995420792079208

Validation results:
gen_loss: 285.39526
disc_loss: 14.102408
disc_acc: 0.9295634920634921


	Epoch 497
Training results:
gen_loss: 277.14093
disc_loss: 0.2858639
disc_acc: 0.996039603960396

Validation results:
gen_loss: 267.3855
disc_loss: 14.140014
disc_acc: 0.9270833333333334


	Epoch 498
Training results:
gen_loss: 272.9331
disc_loss: 0.19422682
disc_acc: 0.9969059405940595

Validation results:
gen_loss: 269.27664
disc_loss: 12.139987
disc_acc: 0.9246031746031746


	Epoch 499
Training results:
gen_loss: 276.36243
disc_loss: 0.30376482
disc_acc: 0.9957920792079208

Validation results:
gen_loss: 254.38704
disc_loss: 14.027492
disc_acc: 0.9166666666666666


	Epoch 500
Training results:
gen_loss: 277.45642
disc_loss: 0.393338
disc_acc: 0.9948019801980198

Validation results:
gen_loss: 291.2193
disc_loss: 11.90347
disc_acc: 0.935515873015873



Training new discriminator on static trained discriminator.
	Initial performance
Training results:
gen_loss: 0.0038516254
disc_loss: 5.5504932
disc_acc: 0.0

Validation results:
gen_loss: 0.0038174859
disc_loss: 5.557056
disc_acc: 0.0


	Epoch 1
Training results:
gen_loss: 3.4955795
disc_loss: 1.8846432
disc_acc: 0.41955445544554454

Validation results:
gen_loss: 4.389354
disc_loss: 0.77418655
disc_acc: 0.6775793650793651


	Epoch 2
Training results:
gen_loss: 5.233882
disc_loss: 0.7785478
disc_acc: 0.7042079207920792

Validation results:
gen_loss: 5.948767
disc_loss: 0.57918686
disc_acc: 0.7971230158730159


	Epoch 3
Training results:
gen_loss: 6.7768364
disc_loss: 0.5928627
disc_acc: 0.7868811881188119

Validation results:
gen_loss: 6.0051427
disc_loss: 0.60802454
disc_acc: 0.7762896825396826


	Epoch 4
Training results:
gen_loss: 7.726985
disc_loss: 0.55086404
disc_acc: 0.8017326732673268

Validation results:
gen_loss: 9.090755
disc_loss: 0.548505
disc_acc: 0.8125


	Epoch 5
Training results:
gen_loss: 8.519198
disc_loss: 0.50282156
disc_acc: 0.8282178217821782

Validation results:
gen_loss: 8.954978
disc_loss: 0.46589324
disc_acc: 0.8373015873015873


	Epoch 6
Training results:
gen_loss: 8.937818
disc_loss: 0.4615218
disc_acc: 0.8387376237623763

Validation results:
gen_loss: 9.909139
disc_loss: 0.45601496
disc_acc: 0.8432539682539683


	Epoch 7
Training results:
gen_loss: 10.472983
disc_loss: 0.42289194
disc_acc: 0.8532178217821782

Validation results:
gen_loss: 11.447682
disc_loss: 0.8490204
disc_acc: 0.7708333333333334


	Epoch 8
Training results:
gen_loss: 10.203908
disc_loss: 0.41193473
disc_acc: 0.8597772277227723

Validation results:
gen_loss: 10.682827
disc_loss: 0.43015084
disc_acc: 0.8581349206349206


	Epoch 9
Training results:
gen_loss: 10.926107
disc_loss: 0.37643144
disc_acc: 0.8685643564356436

Validation results:
gen_loss: 10.283751
disc_loss: 0.3721653
disc_acc: 0.878968253968254


	Epoch 10
Training results:
gen_loss: 11.152895
disc_loss: 0.35235944
disc_acc: 0.8800742574257425

Validation results:
gen_loss: 11.202745
disc_loss: 0.33789933
disc_acc: 0.8898809523809523


	Epoch 11
Training results:
gen_loss: 11.021084
disc_loss: 0.40105608
disc_acc: 0.8717821782178218

Validation results:
gen_loss: 9.563384
disc_loss: 0.6478141
disc_acc: 0.8229166666666666


	Epoch 12
Training results:
gen_loss: 11.607669
disc_loss: 0.2806097
disc_acc: 0.9023514851485148

Validation results:
gen_loss: 10.121626
disc_loss: 0.40679514
disc_acc: 0.8571428571428571


	Epoch 13
Training results:
gen_loss: 11.571455
disc_loss: 0.27951628
disc_acc: 0.9060643564356435

Validation results:
gen_loss: 11.276258
disc_loss: 0.4212835
disc_acc: 0.8695436507936508


	Epoch 14
Training results:
gen_loss: 12.335113
disc_loss: 0.33108407
disc_acc: 0.8948019801980198

Validation results:
gen_loss: 13.68241
disc_loss: 0.29711553
disc_acc: 0.9012896825396826


	Epoch 15
Training results:
gen_loss: 12.411972
disc_loss: 0.27517006
disc_acc: 0.9016089108910891

Validation results:
gen_loss: 12.022934
disc_loss: 0.3913094
disc_acc: 0.8784722222222222


	Epoch 16
Training results:
gen_loss: 12.327713
disc_loss: 0.26573497
disc_acc: 0.9165841584158416

Validation results:
gen_loss: 13.691089
disc_loss: 0.5498155
disc_acc: 0.8497023809523809


	Epoch 17
Training results:
gen_loss: 12.8865795
disc_loss: 0.31604186
disc_acc: 0.9004950495049505

Validation results:
gen_loss: 14.56576
disc_loss: 0.6638782
disc_acc: 0.8368055555555556


	Epoch 18
Training results:
gen_loss: 12.80415
disc_loss: 0.26020074
disc_acc: 0.9168316831683169

Validation results:
gen_loss: 13.21833
disc_loss: 0.3784698
disc_acc: 0.8983134920634921


	Epoch 19
Training results:
gen_loss: 12.9035635
disc_loss: 0.22118047
disc_acc: 0.925990099009901

Validation results:
gen_loss: 14.591819
disc_loss: 0.3270433
disc_acc: 0.9146825396825397


	Epoch 20
Training results:
gen_loss: 12.862099
disc_loss: 0.24902381
disc_acc: 0.9216584158415841

Validation results:
gen_loss: 15.373465
disc_loss: 0.4534357
disc_acc: 0.8700396825396826


	Epoch 21
Training results:
gen_loss: 13.127277
disc_loss: 0.29426107
disc_acc: 0.9120049504950495

Validation results:
gen_loss: 10.265053
disc_loss: 0.4793974
disc_acc: 0.8621031746031746


	Epoch 22
Training results:
gen_loss: 13.912535
disc_loss: 0.23314302
disc_acc: 0.925

Validation results:
gen_loss: 14.706151
disc_loss: 0.42367822
disc_acc: 0.8913690476190477


	Epoch 23
Training results:
gen_loss: 13.885501
disc_loss: 0.20501527
disc_acc: 0.9346534653465347

Validation results:
gen_loss: 15.418181
disc_loss: 0.46252233
disc_acc: 0.8918650793650794


	Epoch 24
Training results:
gen_loss: 14.7916765
disc_loss: 0.2612049
disc_acc: 0.9198019801980198

Validation results:
gen_loss: 14.506501
disc_loss: 0.5762483
disc_acc: 0.8611111111111112


	Epoch 25
Training results:
gen_loss: 14.760675
disc_loss: 0.23144723
disc_acc: 0.9263613861386139

Validation results:
gen_loss: 16.578693
disc_loss: 0.5715774
disc_acc: 0.8635912698412699


	Epoch 26
Training results:
gen_loss: 14.796593
disc_loss: 0.19316804
disc_acc: 0.9403465346534653

Validation results:
gen_loss: 14.028945
disc_loss: 0.5515111
disc_acc: 0.8720238095238095


	Epoch 27
Training results:
gen_loss: 15.026253
disc_loss: 0.21649516
disc_acc: 0.9388613861386138

Validation results:
gen_loss: 15.141663
disc_loss: 0.35977355
disc_acc: 0.8978174603174603


	Epoch 28
Training results:
gen_loss: 14.466298
disc_loss: 0.2332575
disc_acc: 0.9318069306930693

Validation results:
gen_loss: 15.254534
disc_loss: 0.45947197
disc_acc: 0.8804563492063492


	Epoch 29
Training results:
gen_loss: 14.481441
disc_loss: 0.18281567
disc_acc: 0.9455445544554455

Validation results:
gen_loss: 14.657062
disc_loss: 0.35176778
disc_acc: 0.9092261904761905


	Epoch 30
Training results:
gen_loss: 15.544963
disc_loss: 0.2472149
disc_acc: 0.9294554455445545

Validation results:
gen_loss: 15.382908
disc_loss: 0.62334245
disc_acc: 0.8700396825396826


	Epoch 31
Training results:
gen_loss: 15.619453
disc_loss: 0.19007783
disc_acc: 0.9481435643564357

Validation results:
gen_loss: 14.545729
disc_loss: 0.5098863
disc_acc: 0.8700396825396826


	Epoch 32
Training results:
gen_loss: 14.939503
disc_loss: 0.14872295
disc_acc: 0.9565594059405941

Validation results:
gen_loss: 15.093616
disc_loss: 0.41258472
disc_acc: 0.8948412698412699


	Epoch 33
Training results:
gen_loss: 15.310648
disc_loss: 0.3004306
disc_acc: 0.9271039603960396

Validation results:
gen_loss: 15.681787
disc_loss: 0.38384804
disc_acc: 0.9012896825396826


	Epoch 34
Training results:
gen_loss: 14.673719
disc_loss: 0.14204963
disc_acc: 0.9551980198019802

Validation results:
gen_loss: 15.459922
disc_loss: 0.2803642
disc_acc: 0.9171626984126984


	Epoch 35
Training results:
gen_loss: 15.634398
disc_loss: 0.10725258
disc_acc: 0.968069306930693

Validation results:
gen_loss: 14.502415
disc_loss: 0.356408
disc_acc: 0.9206349206349206


	Epoch 36
Training results:
gen_loss: 15.885609
disc_loss: 0.14647248
disc_acc: 0.9542079207920792

Validation results:
gen_loss: 14.722109
disc_loss: 1.0657762
disc_acc: 0.8174603174603174


	Epoch 37
Training results:
gen_loss: 16.567368
disc_loss: 0.33515757
disc_acc: 0.9214108910891089

Validation results:
gen_loss: 17.330938
disc_loss: 0.41457295
disc_acc: 0.9112103174603174


	Epoch 38
Training results:
gen_loss: 15.959015
disc_loss: 0.08491772
disc_acc: 0.9737623762376237

Validation results:
gen_loss: 15.578716
disc_loss: 0.24446617
disc_acc: 0.9330357142857143


	Epoch 39
Training results:
gen_loss: 16.211695
disc_loss: 0.11277292
disc_acc: 0.9660891089108911

Validation results:
gen_loss: 14.867256
disc_loss: 0.4445243
disc_acc: 0.904265873015873


	Epoch 40
Training results:
gen_loss: 17.116129
disc_loss: 0.39148456
disc_acc: 0.9246287128712871

Validation results:
gen_loss: 17.543186
disc_loss: 0.6084134
disc_acc: 0.8819444444444444


	Epoch 41
Training results:
gen_loss: 17.214148
disc_loss: 0.155874
disc_acc: 0.9596534653465346

Validation results:
gen_loss: 19.179003
disc_loss: 0.50960356
disc_acc: 0.9052579365079365


	Epoch 42
Training results:
gen_loss: 17.740582
disc_loss: 0.08540989
disc_acc: 0.9737623762376237

Validation results:
gen_loss: 16.839163
disc_loss: 0.36467305
disc_acc: 0.9112103174603174


	Epoch 43
Training results:
gen_loss: 17.761768
disc_loss: 0.09683484
disc_acc: 0.9731435643564357

Validation results:
gen_loss: 17.269072
disc_loss: 0.3882699
disc_acc: 0.9201388888888888


	Epoch 44
Training results:
gen_loss: 17.674238
disc_loss: 0.15497293
disc_acc: 0.9584158415841584

Validation results:
gen_loss: 15.444126
disc_loss: 0.31266215
disc_acc: 0.9122023809523809


	Epoch 45
Training results:
gen_loss: 16.607765
disc_loss: 0.14446184
disc_acc: 0.960519801980198

Validation results:
gen_loss: 15.903817
disc_loss: 0.30976677
disc_acc: 0.9345238095238095


	Epoch 46
Training results:
gen_loss: 17.770988
disc_loss: 0.20937875
disc_acc: 0.9495049504950495

Validation results:
gen_loss: 15.522021
disc_loss: 1.640822
disc_acc: 0.7366071428571429


	Epoch 47
Training results:
gen_loss: 17.662163
disc_loss: 0.17157209
disc_acc: 0.9566831683168316

Validation results:
gen_loss: 17.91637
disc_loss: 0.3859121
disc_acc: 0.9250992063492064


	Epoch 48
Training results:
gen_loss: 19.04044
disc_loss: 0.12783843
disc_acc: 0.9655940594059406

Validation results:
gen_loss: 19.73553
disc_loss: 0.48678088
disc_acc: 0.8983134920634921


	Epoch 49
Training results:
gen_loss: 18.970676
disc_loss: 0.11080294
disc_acc: 0.968440594059406

Validation results:
gen_loss: 18.757505
disc_loss: 2.0814078
disc_acc: 0.8025793650793651


	Epoch 50
Training results:
gen_loss: 20.340237
disc_loss: 0.24825482
disc_acc: 0.9517326732673267

Validation results:
gen_loss: 19.009329
disc_loss: 0.5820198
disc_acc: 0.9007936507936508


	Epoch 51
Training results:
gen_loss: 19.13148
disc_loss: 0.12981258
disc_acc: 0.9706683168316832

Validation results:
gen_loss: 17.909866
disc_loss: 0.6898487
disc_acc: 0.8814484126984127


	Epoch 52
Training results:
gen_loss: 17.505276
disc_loss: 0.073302604
disc_acc: 0.9790841584158416

Validation results:
gen_loss: 16.432348
disc_loss: 0.45081946
disc_acc: 0.9156746031746031


	Epoch 53
Training results:
gen_loss: 19.286066
disc_loss: 0.28936738
disc_acc: 0.944059405940594

Validation results:
gen_loss: 19.833878
disc_loss: 0.6336517
disc_acc: 0.8973214285714286


	Epoch 54
Training results:
gen_loss: 19.14756
disc_loss: 0.121674776
disc_acc: 0.9738861386138614

Validation results:
gen_loss: 16.600283
disc_loss: 0.51323426
disc_acc: 0.9012896825396826


	Epoch 55
Training results:
gen_loss: 17.921623
disc_loss: 0.096592456
disc_acc: 0.9753712871287129

Validation results:
gen_loss: 17.728573
disc_loss: 0.571003
disc_acc: 0.9151785714285714


	Epoch 56
Training results:
gen_loss: 18.213425
disc_loss: 0.12692182
disc_acc: 0.969430693069307

Validation results:
gen_loss: 20.50206
disc_loss: 0.5998073
disc_acc: 0.8973214285714286


	Epoch 57
Training results:
gen_loss: 19.075909
disc_loss: 0.13512498
disc_acc: 0.968069306930693

Validation results:
gen_loss: 18.045675
disc_loss: 0.5339108
disc_acc: 0.9146825396825397


	Epoch 58
Training results:
gen_loss: 18.723879
disc_loss: 0.11841521
disc_acc: 0.9693069306930693

Validation results:
gen_loss: 21.060673
disc_loss: 0.7144231
disc_acc: 0.8953373015873016


	Epoch 59
Training results:
gen_loss: 21.235262
disc_loss: 0.2533639
disc_acc: 0.9488861386138614

Validation results:
gen_loss: 23.338533
disc_loss: 0.88207084
disc_acc: 0.8948412698412699


	Epoch 60
Training results:
gen_loss: 22.816624
disc_loss: 0.16979355
disc_acc: 0.9612623762376238

Validation results:
gen_loss: 19.580545
disc_loss: 0.6316439
disc_acc: 0.90625


	Epoch 61
Training results:
gen_loss: 20.98987
disc_loss: 0.10199224
disc_acc: 0.9754950495049505

Validation results:
gen_loss: 20.955471
disc_loss: 0.45893887
disc_acc: 0.9236111111111112


	Epoch 62
Training results:
gen_loss: 21.320482
disc_loss: 0.13082089
disc_acc: 0.9707920792079208

Validation results:
gen_loss: 17.251175
disc_loss: 2.5205288
disc_acc: 0.7316468253968254


	Epoch 63
Training results:
gen_loss: 21.42833
disc_loss: 0.289229
disc_acc: 0.9475247524752475

Validation results:
gen_loss: 19.501612
disc_loss: 0.61212504
disc_acc: 0.910218253968254


	Epoch 64
Training results:
gen_loss: 21.120989
disc_loss: 0.07670258
disc_acc: 0.9800742574257426

Validation results:
gen_loss: 21.701649
disc_loss: 0.51336414
disc_acc: 0.9295634920634921


	Epoch 65
Training results:
gen_loss: 20.287476
disc_loss: 0.103967994
disc_acc: 0.9792079207920792

Validation results:
gen_loss: 17.890554
disc_loss: 0.6597574
disc_acc: 0.9037698412698413


	Epoch 66
Training results:
gen_loss: 19.825853
disc_loss: 0.18839127
disc_acc: 0.9622524752475248

Validation results:
gen_loss: 20.370884
disc_loss: 0.90073764
disc_acc: 0.8938492063492064


	Epoch 67
Training results:
gen_loss: 20.601362
disc_loss: 0.15977763
disc_acc: 0.9648514851485148

Validation results:
gen_loss: 22.078037
disc_loss: 0.5297371
disc_acc: 0.9250992063492064


	Epoch 68
Training results:
gen_loss: 21.843534
disc_loss: 0.11313685
disc_acc: 0.9724009900990099

Validation results:
gen_loss: 20.141134
disc_loss: 0.9716242
disc_acc: 0.8978174603174603


	Epoch 69
Training results:
gen_loss: 20.678682
disc_loss: 0.12803423
disc_acc: 0.9738861386138614

Validation results:
gen_loss: 21.84345
disc_loss: 0.61647695
disc_acc: 0.9067460317460317


	Epoch 70
Training results:
gen_loss: 20.955988
disc_loss: 0.1506781
disc_acc: 0.9678217821782178

Validation results:
gen_loss: 21.89151
disc_loss: 0.5617897
disc_acc: 0.9171626984126984


	Epoch 71
Training results:
gen_loss: 23.180313
disc_loss: 0.14905918
disc_acc: 0.9714108910891089

Validation results:
gen_loss: 25.410864
disc_loss: 0.62209564
disc_acc: 0.9176587301587301


	Epoch 72
Training results:
gen_loss: 23.05021
disc_loss: 0.14372109
disc_acc: 0.9699257425742575

Validation results:
gen_loss: 22.254442
disc_loss: 0.70779204
disc_acc: 0.8993055555555556


	Epoch 73
Training results:
gen_loss: 24.123875
disc_loss: 0.17013188
disc_acc: 0.9689356435643565

Validation results:
gen_loss: 22.619946
disc_loss: 0.5579609
disc_acc: 0.9290674603174603


	Epoch 74
Training results:
gen_loss: 22.66629
disc_loss: 0.1089488
disc_acc: 0.9753712871287129

Validation results:
gen_loss: 22.2978
disc_loss: 0.482001
disc_acc: 0.9300595238095238


	Epoch 75
Training results:
gen_loss: 23.770058
disc_loss: 0.12908706
disc_acc: 0.973019801980198

Validation results:
gen_loss: 22.014427
disc_loss: 0.99107534
disc_acc: 0.8948412698412699


	Epoch 76
Training results:
gen_loss: 24.53617
disc_loss: 0.1442332
disc_acc: 0.9724009900990099

Validation results:
gen_loss: 25.42551
disc_loss: 0.8403749
disc_acc: 0.904265873015873


	Epoch 77
Training results:
gen_loss: 24.358793
disc_loss: 0.21604878
disc_acc: 0.966460396039604

Validation results:
gen_loss: 23.74829
disc_loss: 0.76608944
disc_acc: 0.9122023809523809


	Epoch 78
Training results:
gen_loss: 23.71453
disc_loss: 0.10237533
disc_acc: 0.9797029702970297

Validation results:
gen_loss: 22.18819
disc_loss: 0.9169639
disc_acc: 0.8978174603174603


	Epoch 79
Training results:
gen_loss: 24.538097
disc_loss: 0.118940145
disc_acc: 0.9758663366336634

Validation results:
gen_loss: 23.349005
disc_loss: 0.5751073
disc_acc: 0.9285714285714286


	Epoch 80
Training results:
gen_loss: 23.655403
disc_loss: 0.08596313
disc_acc: 0.9834158415841584

Validation results:
gen_loss: 23.360907
disc_loss: 0.7620099
disc_acc: 0.9052579365079365


	Epoch 81
Training results:
gen_loss: 23.995728
disc_loss: 0.25954694
disc_acc: 0.9596534653465346

Validation results:
gen_loss: 28.55817
disc_loss: 1.1611314
disc_acc: 0.8958333333333334


	Epoch 82
Training results:
gen_loss: 26.763113
disc_loss: 0.18258652
disc_acc: 0.9662128712871287

Validation results:
gen_loss: 23.92578
disc_loss: 0.7065268
disc_acc: 0.9107142857142857


	Epoch 83
Training results:
gen_loss: 26.25521
disc_loss: 0.0635206
disc_acc: 0.9857673267326733

Validation results:
gen_loss: 24.358831
disc_loss: 0.64310014
disc_acc: 0.9305555555555556


	Epoch 84
Training results:
gen_loss: 25.596786
disc_loss: 0.10915888
disc_acc: 0.9793316831683169

Validation results:
gen_loss: 26.763664
disc_loss: 0.73317426
disc_acc: 0.9196428571428571


	Epoch 85
Training results:
gen_loss: 24.664967
disc_loss: 0.16560638
disc_acc: 0.9701732673267327

Validation results:
gen_loss: 24.526371
disc_loss: 0.9041121
disc_acc: 0.9107142857142857


	Epoch 86
Training results:
gen_loss: 24.942724
disc_loss: 0.13655378
disc_acc: 0.9766089108910891

Validation results:
gen_loss: 24.559101
disc_loss: 1.0712243
disc_acc: 0.9037698412698413


	Epoch 87
Training results:
gen_loss: 25.780792
disc_loss: 0.13580269
disc_acc: 0.9762376237623762

Validation results:
gen_loss: 24.378263
disc_loss: 1.4127713
disc_acc: 0.8650793650793651


	Epoch 88
Training results:
gen_loss: 27.007195
disc_loss: 0.1538699
disc_acc: 0.9748762376237624

Validation results:
gen_loss: 28.717333
disc_loss: 0.7402087
disc_acc: 0.9151785714285714


	Epoch 89
Training results:
gen_loss: 27.348417
disc_loss: 0.1345536
disc_acc: 0.9757425742574257

Validation results:
gen_loss: 26.91193
disc_loss: 0.8631798
disc_acc: 0.9097222222222222


	Epoch 90
Training results:
gen_loss: 26.797218
disc_loss: 0.095855065
disc_acc: 0.9804455445544554

Validation results:
gen_loss: 27.033785
disc_loss: 0.8475452
disc_acc: 0.9226190476190477


	Epoch 91
Training results:
gen_loss: 27.666016
disc_loss: 0.16598377
disc_acc: 0.972029702970297

Validation results:
gen_loss: 31.359201
disc_loss: 1.9674925
disc_acc: 0.847718253968254


	Epoch 92
Training results:
gen_loss: 28.800493
disc_loss: 0.20338824
disc_acc: 0.9696782178217822

Validation results:
gen_loss: 30.53688
disc_loss: 0.9522595
disc_acc: 0.902281746031746


	Epoch 93
Training results:
gen_loss: 28.378407
disc_loss: 0.12561765
disc_acc: 0.9803217821782179

Validation results:
gen_loss: 26.32176
disc_loss: 0.62233055
disc_acc: 0.9310515873015873


	Epoch 94
Training results:
gen_loss: 26.742369
disc_loss: 0.122785285
disc_acc: 0.9805693069306931

Validation results:
gen_loss: 26.010832
disc_loss: 0.66678536
disc_acc: 0.9280753968253969


	Epoch 95
Training results:
gen_loss: 26.59784
disc_loss: 0.08403205
disc_acc: 0.9846534653465346

Validation results:
gen_loss: 29.990713
disc_loss: 0.91873634
disc_acc: 0.9171626984126984


	Epoch 96
Training results:
gen_loss: 30.549593
disc_loss: 0.35685793
disc_acc: 0.9600247524752475

Validation results:
gen_loss: 32.543324
disc_loss: 4.127765
disc_acc: 0.7867063492063492


	Epoch 97
Training results:
gen_loss: 35.286648
disc_loss: 0.1998062
disc_acc: 0.9747524752475247

Validation results:
gen_loss: 33.68189
disc_loss: 1.0154082
disc_acc: 0.9166666666666666


	Epoch 98
Training results:
gen_loss: 34.39313
disc_loss: 0.07513138
disc_acc: 0.9868811881188119

Validation results:
gen_loss: 33.021442
disc_loss: 1.0439405
disc_acc: 0.910218253968254


	Epoch 99
Training results:
gen_loss: 32.586586
disc_loss: 0.110990055
disc_acc: 0.9825495049504951

Validation results:
gen_loss: 34.180763
disc_loss: 0.80196714
disc_acc: 0.9285714285714286


	Epoch 100
Training results:
gen_loss: 29.937593
disc_loss: 0.14902583
disc_acc: 0.9780940594059406

Validation results:
gen_loss: 30.319593
disc_loss: 0.750072
disc_acc: 0.9226190476190477


	Epoch 101
Training results:
gen_loss: 28.21052
disc_loss: 0.115601696
disc_acc: 0.9820544554455446

Validation results:
gen_loss: 27.42011
disc_loss: 0.7949071
disc_acc: 0.9201388888888888


	Epoch 102
Training results:
gen_loss: 27.567984
disc_loss: 0.0838472
disc_acc: 0.9846534653465346

Validation results:
gen_loss: 26.256418
disc_loss: 1.2582773
disc_acc: 0.8888888888888888


	Epoch 103
Training results:
gen_loss: 30.566816
disc_loss: 0.19338769
disc_acc: 0.9717821782178218

Validation results:
gen_loss: 32.08036
disc_loss: 0.987676
disc_acc: 0.9176587301587301


	Epoch 104
Training results:
gen_loss: 32.274025
disc_loss: 0.17389254
disc_acc: 0.9754950495049505

Validation results:
gen_loss: 37.067886
disc_loss: 2.014347
disc_acc: 0.8576388888888888


	Epoch 105
Training results:
gen_loss: 32.45449
disc_loss: 0.16504738
disc_acc: 0.9775990099009901

Validation results:
gen_loss: 30.447176
disc_loss: 0.7387977
disc_acc: 0.9275793650793651


	Epoch 106
Training results:
gen_loss: 29.954208
disc_loss: 0.10061668
disc_acc: 0.9846534653465346

Validation results:
gen_loss: 28.673777
disc_loss: 0.8789858
disc_acc: 0.9181547619047619


	Epoch 107
Training results:
gen_loss: 31.44292
disc_loss: 0.17995502
disc_acc: 0.9743811881188119

Validation results:
gen_loss: 33.259686
disc_loss: 1.5645497
disc_acc: 0.8794642857142857


	Epoch 108
Training results:
gen_loss: 33.711525
disc_loss: 0.18715058
disc_acc: 0.9757425742574257

Validation results:
gen_loss: 34.37735
disc_loss: 1.0902567
disc_acc: 0.9112103174603174


	Epoch 109
Training results:
gen_loss: 33.806824
disc_loss: 0.12751003
disc_acc: 0.9808168316831684

Validation results:
gen_loss: 34.131783
disc_loss: 0.8879123
disc_acc: 0.9246031746031746


	Epoch 110
Training results:
gen_loss: 32.76797
disc_loss: 0.083635546
disc_acc: 0.9867574257425743

Validation results:
gen_loss: 34.49531
disc_loss: 0.76589745
disc_acc: 0.939484126984127


	Epoch 111
Training results:
gen_loss: 30.657194
disc_loss: 0.12919836
disc_acc: 0.9834158415841584

Validation results:
gen_loss: 32.771095
disc_loss: 0.78094065
disc_acc: 0.9260912698412699


	Epoch 112
Training results:
gen_loss: 34.89381
disc_loss: 0.34849292
disc_acc: 0.9662128712871287

Validation results:
gen_loss: 34.279507
disc_loss: 1.861119
disc_acc: 0.8799603174603174


	Epoch 113
Training results:
gen_loss: 33.26971
disc_loss: 0.17246106
disc_acc: 0.9787128712871287

Validation results:
gen_loss: 32.488388
disc_loss: 1.0919539
disc_acc: 0.9161706349206349


	Epoch 114
Training results:
gen_loss: 31.630198
disc_loss: 0.045949586
disc_acc: 0.9912128712871288

Validation results:
gen_loss: 33.122536
disc_loss: 0.93243235
disc_acc: 0.9300595238095238


	Epoch 115
Training results:
gen_loss: 32.676006
disc_loss: 0.12555544
disc_acc: 0.9831683168316832

Validation results:
gen_loss: 34.95338
disc_loss: 1.3380157
disc_acc: 0.9092261904761905


	Epoch 116
Training results:
gen_loss: 33.716476
disc_loss: 0.24607629
disc_acc: 0.9754950495049505

Validation results:
gen_loss: 27.312857
disc_loss: 1.6610087
disc_acc: 0.8799603174603174


	Epoch 117
Training results:
gen_loss: 31.830809
disc_loss: 0.15550503
disc_acc: 0.9784653465346534

Validation results:
gen_loss: 33.472603
disc_loss: 1.0370384
disc_acc: 0.9196428571428571


	Epoch 118
Training results:
gen_loss: 31.475136
disc_loss: 0.102736264
disc_acc: 0.9850247524752476

Validation results:
gen_loss: 32.50993
disc_loss: 0.90631366
disc_acc: 0.9260912698412699


	Epoch 119
Training results:
gen_loss: 35.308933
disc_loss: 0.07883338
disc_acc: 0.9867574257425743

Validation results:
gen_loss: 32.95245
disc_loss: 1.1698148
disc_acc: 0.9176587301587301


	Epoch 120
Training results:
gen_loss: 33.791954
disc_loss: 0.17086738
disc_acc: 0.9787128712871287

Validation results:
gen_loss: 34.363415
disc_loss: 1.5353471
disc_acc: 0.8993055555555556


	Epoch 121
Training results:
gen_loss: 33.31767
disc_loss: 0.113757126
disc_acc: 0.9857673267326733

Validation results:
gen_loss: 39.37396
disc_loss: 1.6625667
disc_acc: 0.8933531746031746


	Epoch 122
Training results:
gen_loss: 37.49087
disc_loss: 0.28366148
disc_acc: 0.9715346534653465

Validation results:
gen_loss: 38.14588
disc_loss: 1.3899263
disc_acc: 0.908234126984127


	Epoch 123
Training results:
gen_loss: 35.240875
disc_loss: 0.13289315
disc_acc: 0.9834158415841584

Validation results:
gen_loss: 35.39743
disc_loss: 1.2040371
disc_acc: 0.9241071428571429


	Epoch 124
Training results:
gen_loss: 36.071774
disc_loss: 0.071250245
disc_acc: 0.9892326732673268

Validation results:
gen_loss: 34.485
disc_loss: 1.2829093
disc_acc: 0.9117063492063492


	Epoch 125
Training results:
gen_loss: 35.85259
disc_loss: 0.08155294
disc_acc: 0.9893564356435643

Validation results:
gen_loss: 34.317226
disc_loss: 1.652931
disc_acc: 0.8998015873015873


	Epoch 126
Training results:
gen_loss: 35.399815
disc_loss: 0.25096014
disc_acc: 0.9717821782178218

Validation results:
gen_loss: 35.81212
disc_loss: 1.6731554
disc_acc: 0.8978174603174603


	Epoch 127
Training results:
gen_loss: 36.302338
disc_loss: 0.16651413
disc_acc: 0.9819306930693069

Validation results:
gen_loss: 35.06632
disc_loss: 1.0361683
disc_acc: 0.9300595238095238


	Epoch 128
Training results:
gen_loss: 35.244244
disc_loss: 0.11861998
disc_acc: 0.9851485148514851

Validation results:
gen_loss: 33.657658
disc_loss: 1.5070338
disc_acc: 0.910218253968254


	Epoch 129
Training results:
gen_loss: 36.19847
disc_loss: 0.120136194
disc_acc: 0.9851485148514851

Validation results:
gen_loss: 34.809853
disc_loss: 1.5160563
disc_acc: 0.9057539682539683


	Epoch 130
Training results:
gen_loss: 34.73549
disc_loss: 0.18055257
disc_acc: 0.9788366336633664

Validation results:
gen_loss: 33.52691
disc_loss: 2.168475
disc_acc: 0.8680555555555556


	Epoch 131
Training results:
gen_loss: 40.390057
disc_loss: 0.27401412
disc_acc: 0.9745049504950495

Validation results:
gen_loss: 42.369736
disc_loss: 1.4816574
disc_acc: 0.9181547619047619


	Epoch 132
Training results:
gen_loss: 39.668346
disc_loss: 0.1296604
disc_acc: 0.9853960396039604

Validation results:
gen_loss: 36.605453
disc_loss: 1.8949814
disc_acc: 0.8918650793650794


	Epoch 133
Training results:
gen_loss: 39.78075
disc_loss: 0.12172535
disc_acc: 0.9850247524752476

Validation results:
gen_loss: 39.35738
disc_loss: 1.6804732
disc_acc: 0.8883928571428571


	Epoch 134
Training results:
gen_loss: 38.31514
disc_loss: 0.11651368
disc_acc: 0.9852722772277228

Validation results:
gen_loss: 39.315395
disc_loss: 1.2966412
disc_acc: 0.9226190476190477


	Epoch 135
Training results:
gen_loss: 35.82039
disc_loss: 0.15589805
disc_acc: 0.9818069306930693

Validation results:
gen_loss: 37.858013
disc_loss: 1.4280262
disc_acc: 0.9176587301587301


	Epoch 136
Training results:
gen_loss: 37.05625
disc_loss: 0.151831
disc_acc: 0.9835396039603961

Validation results:
gen_loss: 41.57758
disc_loss: 1.7379209
disc_acc: 0.8968253968253969


	Epoch 137
Training results:
gen_loss: 38.586872
disc_loss: 0.17711297
disc_acc: 0.9799504950495049

Validation results:
gen_loss: 44.284813
disc_loss: 3.4518757
disc_acc: 0.8387896825396826


	Epoch 138
Training results:
gen_loss: 39.90236
disc_loss: 0.23445706
disc_acc: 0.9778465346534654

Validation results:
gen_loss: 40.20434
disc_loss: 2.7673025
disc_acc: 0.8834325396825397


	Epoch 139
Training results:
gen_loss: 39.27377
disc_loss: 0.10066016
disc_acc: 0.9872524752475248

Validation results:
gen_loss: 38.663284
disc_loss: 2.436047
disc_acc: 0.875


	Epoch 140
Training results:
gen_loss: 40.55426
disc_loss: 0.13554029
disc_acc: 0.9862623762376238

Validation results:
gen_loss: 38.22642
disc_loss: 1.6352017
disc_acc: 0.9002976190476191


	Epoch 141
Training results:
gen_loss: 40.568943
disc_loss: 0.21039072
disc_acc: 0.9772277227722772

Validation results:
gen_loss: 42.19865
disc_loss: 2.200757
disc_acc: 0.8943452380952381


	Epoch 142
Training results:
gen_loss: 38.8913
disc_loss: 0.12202377
disc_acc: 0.9850247524752476

Validation results:
gen_loss: 35.938793
disc_loss: 1.5002953
disc_acc: 0.9226190476190477


	Epoch 143
Training results:
gen_loss: 36.608353
disc_loss: 0.16183083
disc_acc: 0.9839108910891089

Validation results:
gen_loss: 36.116306
disc_loss: 2.4611666
disc_acc: 0.8834325396825397


	Epoch 144
Training results:
gen_loss: 39.603004
disc_loss: 0.1293401
disc_acc: 0.9850247524752476

Validation results:
gen_loss: 38.357273
disc_loss: 1.3320394
disc_acc: 0.9231150793650794


	Epoch 145
Training results:
gen_loss: 39.28147
disc_loss: 0.117558725
disc_acc: 0.9863861386138614

Validation results:
gen_loss: 41.589073
disc_loss: 1.5621825
disc_acc: 0.9201388888888888


	Epoch 146
Training results:
gen_loss: 39.651802
disc_loss: 0.17281948
disc_acc: 0.9821782178217822

Validation results:
gen_loss: 38.153137
disc_loss: 2.1693306
disc_acc: 0.8908730158730159


	Epoch 147
Training results:
gen_loss: 41.094738
disc_loss: 0.15687694
disc_acc: 0.9846534653465346

Validation results:
gen_loss: 41.26571
disc_loss: 1.9375602
disc_acc: 0.9027777777777778


	Epoch 148
Training results:
gen_loss: 40.350216
disc_loss: 0.18448162
disc_acc: 0.9823019801980198

Validation results:
gen_loss: 39.898914
disc_loss: 1.5766964
disc_acc: 0.9126984126984127


	Epoch 149
Training results:
gen_loss: 42.452644
disc_loss: 0.15223114
disc_acc: 0.9844059405940594

Validation results:
gen_loss: 46.3771
disc_loss: 2.3007617
disc_acc: 0.9107142857142857


	Epoch 150
Training results:
gen_loss: 44.130684
disc_loss: 0.24176623
disc_acc: 0.9768564356435644

Validation results:
gen_loss: 42.579777
disc_loss: 1.8925283
disc_acc: 0.9057539682539683


	Epoch 151
Training results:
gen_loss: 43.47411
disc_loss: 0.10483085
disc_acc: 0.9882425742574258

Validation results:
gen_loss: 43.883926
disc_loss: 1.3497466
disc_acc: 0.9310515873015873


	Epoch 152
Training results:
gen_loss: 43.780815
disc_loss: 0.06614457
disc_acc: 0.9922029702970298

Validation results:
gen_loss: 43.420147
disc_loss: 1.5438923
disc_acc: 0.9161706349206349


	Epoch 153
Training results:
gen_loss: 45.82267
disc_loss: 0.27065375
disc_acc: 0.9763613861386139

Validation results:
gen_loss: 48.318928
disc_loss: 1.8980658
disc_acc: 0.9077380952380952


	Epoch 154
Training results:
gen_loss: 45.104485
disc_loss: 0.16183034
disc_acc: 0.986509900990099

Validation results:
gen_loss: 44.58485
disc_loss: 2.5762358
disc_acc: 0.8953373015873016


	Epoch 155
Training results:
gen_loss: 45.274952
disc_loss: 0.12443716
disc_acc: 0.9862623762376238

Validation results:
gen_loss: 48.98796
disc_loss: 1.8436583
disc_acc: 0.9112103174603174


	Epoch 156
Training results:
gen_loss: 46.69115
disc_loss: 0.07475976
disc_acc: 0.9920792079207921

Validation results:
gen_loss: 50.0025
disc_loss: 2.3222115
disc_acc: 0.908234126984127


	Epoch 157
Training results:
gen_loss: 44.615704
disc_loss: 0.15056235
disc_acc: 0.9853960396039604

Validation results:
gen_loss: 42.091293
disc_loss: 2.493784
disc_acc: 0.8923611111111112


	Epoch 158
Training results:
gen_loss: 43.674522
disc_loss: 0.17390545
disc_acc: 0.9830445544554456

Validation results:
gen_loss: 41.026306
disc_loss: 1.8163502
disc_acc: 0.9087301587301587


	Epoch 159
Training results:
gen_loss: 42.032696
disc_loss: 0.05195186
disc_acc: 0.9933168316831683

Validation results:
gen_loss: 42.764687
disc_loss: 1.2759616
disc_acc: 0.9305555555555556


	Epoch 160
Training results:
gen_loss: 43.16127
disc_loss: 0.16409783
disc_acc: 0.9851485148514851

Validation results:
gen_loss: 48.290295
disc_loss: 3.1842835
disc_acc: 0.8625992063492064


	Epoch 161
Training results:
gen_loss: 48.81165
disc_loss: 0.25699323
disc_acc: 0.9785891089108911

Validation results:
gen_loss: 50.307297
disc_loss: 1.9057369
disc_acc: 0.9161706349206349


	Epoch 162
Training results:
gen_loss: 49.339596
disc_loss: 0.13676998
disc_acc: 0.9873762376237624

Validation results:
gen_loss: 51.562664
disc_loss: 2.2613404
disc_acc: 0.8973214285714286


	Epoch 163
Training results:
gen_loss: 48.48442
disc_loss: 0.11392365
disc_acc: 0.9883663366336634

Validation results:
gen_loss: 44.83718
disc_loss: 1.997732
disc_acc: 0.9126984126984127


	Epoch 164
Training results:
gen_loss: 47.488945
disc_loss: 0.18190281
disc_acc: 0.9844059405940594

Validation results:
gen_loss: 46.516895
disc_loss: 1.8694358
disc_acc: 0.9126984126984127


	Epoch 165
Training results:
gen_loss: 50.018204
disc_loss: 0.21665177
disc_acc: 0.9840346534653466

Validation results:
gen_loss: 53.694077
disc_loss: 2.5533345
disc_acc: 0.9012896825396826


	Epoch 166
Training results:
gen_loss: 51.734608
disc_loss: 0.2337329
disc_acc: 0.9815594059405941

Validation results:
gen_loss: 55.99772
disc_loss: 2.1568935
disc_acc: 0.9201388888888888


	Epoch 167
Training results:
gen_loss: 52.326054
disc_loss: 0.12591499
disc_acc: 0.989480198019802

Validation results:
gen_loss: 48.545578
disc_loss: 1.5447079
disc_acc: 0.9280753968253969


	Epoch 168
Training results:
gen_loss: 49.618633
disc_loss: 0.11605143
disc_acc: 0.9898514851485148

Validation results:
gen_loss: 49.843533
disc_loss: 1.7065902
disc_acc: 0.9295634920634921


	Epoch 169
Training results:
gen_loss: 48.82594
disc_loss: 0.22270817
disc_acc: 0.9855198019801981

Validation results:
gen_loss: 42.315853
disc_loss: 3.9740484
disc_acc: 0.8407738095238095


	Epoch 170
Training results:
gen_loss: 48.50402
disc_loss: 0.15150881
disc_acc: 0.9877475247524753

Validation results:
gen_loss: 42.368187
disc_loss: 2.6716404
disc_acc: 0.8963293650793651


	Epoch 171
Training results:
gen_loss: 52.58818
disc_loss: 0.25631806
disc_acc: 0.9790841584158416

Validation results:
gen_loss: 58.100044
disc_loss: 2.2811713
disc_acc: 0.9107142857142857


	Epoch 172
Training results:
gen_loss: 54.07005
disc_loss: 0.2249686
disc_acc: 0.9821782178217822

Validation results:
gen_loss: 52.120323
disc_loss: 1.8334036
disc_acc: 0.9221230158730159


	Epoch 173
Training results:
gen_loss: 51.480415
disc_loss: 0.101877846
disc_acc: 0.9918316831683168

Validation results:
gen_loss: 50.443584
disc_loss: 1.846853
disc_acc: 0.9246031746031746


	Epoch 174
Training results:
gen_loss: 49.132614
disc_loss: 0.10542971
disc_acc: 0.9891089108910891

Validation results:
gen_loss: 60.42197
disc_loss: 2.403504
disc_acc: 0.9017857142857143


	Epoch 175
Training results:
gen_loss: 50.209877
disc_loss: 0.18436706
disc_acc: 0.9851485148514851

Validation results:
gen_loss: 47.62494
disc_loss: 2.1219132
disc_acc: 0.9072420634920635


	Epoch 176
Training results:
gen_loss: 53.885532
disc_loss: 0.31347567
disc_acc: 0.9810643564356436

Validation results:
gen_loss: 70.315994
disc_loss: 3.0395231
disc_acc: 0.9007936507936508


	Epoch 177
Training results:
gen_loss: 65.327675
disc_loss: 0.21549344
disc_acc: 0.9840346534653466

Validation results:
gen_loss: 58.688118
disc_loss: 1.7438682
disc_acc: 0.9221230158730159


	Epoch 178
Training results:
gen_loss: 58.52488
disc_loss: 0.13495353
disc_acc: 0.9877475247524753

Validation results:
gen_loss: 59.754612
disc_loss: 2.0468907
disc_acc: 0.9260912698412699


	Epoch 179
Training results:
gen_loss: 55.46734
disc_loss: 0.11352308
disc_acc: 0.9918316831683168

Validation results:
gen_loss: 54.470886
disc_loss: 2.0513732
disc_acc: 0.9166666666666666


	Epoch 180
Training results:
gen_loss: 55.756668
disc_loss: 0.14697562
disc_acc: 0.9883663366336634

Validation results:
gen_loss: 50.58705
disc_loss: 2.1678522
disc_acc: 0.9122023809523809


	Epoch 181
Training results:
gen_loss: 53.92453
disc_loss: 0.19072989
disc_acc: 0.9856435643564356

Validation results:
gen_loss: 57.108303
disc_loss: 5.1193924
disc_acc: 0.8467261904761905


	Epoch 182
Training results:
gen_loss: 53.384377
disc_loss: 0.17506342
disc_acc: 0.9868811881188119

Validation results:
gen_loss: 54.59205
disc_loss: 2.5433364
disc_acc: 0.9077380952380952


	Epoch 183
Training results:
gen_loss: 54.511086
disc_loss: 0.14036971
disc_acc: 0.9891089108910891

Validation results:
gen_loss: 64.50735
disc_loss: 1.9512678
disc_acc: 0.9216269841269841


	Epoch 184
Training results:
gen_loss: 57.77265
disc_loss: 0.16739765
disc_acc: 0.9876237623762376

Validation results:
gen_loss: 49.914864
disc_loss: 2.9579966
disc_acc: 0.8888888888888888


	Epoch 185
Training results:
gen_loss: 53.283787
disc_loss: 0.2237815
disc_acc: 0.9851485148514851

Validation results:
gen_loss: 51.271286
disc_loss: 2.4560695
disc_acc: 0.9057539682539683


	Epoch 186
Training results:
gen_loss: 51.60242
disc_loss: 0.19006409
disc_acc: 0.9847772277227723

Validation results:
gen_loss: 53.05993
disc_loss: 2.185567
disc_acc: 0.9146825396825397


	Epoch 187
Training results:
gen_loss: 57.36556
disc_loss: 0.17839925
disc_acc: 0.9861386138613861

Validation results:
gen_loss: 56.85103
disc_loss: 2.0974865
disc_acc: 0.9201388888888888


	Epoch 188
Training results:
gen_loss: 57.94096
disc_loss: 0.090202466
disc_acc: 0.9929455445544555

Validation results:
gen_loss: 55.097855
disc_loss: 2.075539
disc_acc: 0.9231150793650794


	Epoch 189
Training results:
gen_loss: 57.394802
disc_loss: 0.22422136
disc_acc: 0.9852722772277228

Validation results:
gen_loss: 59.593796
disc_loss: 3.4311705
disc_acc: 0.8963293650793651


	Epoch 190
Training results:
gen_loss: 59.36076
disc_loss: 0.21469307
disc_acc: 0.9857673267326733

Validation results:
gen_loss: 56.132957
disc_loss: 1.6092967
disc_acc: 0.9320436507936508


	Epoch 191
Training results:
gen_loss: 58.711464
disc_loss: 0.11048945
disc_acc: 0.9919554455445545

Validation results:
gen_loss: 58.492447
disc_loss: 1.9683759
disc_acc: 0.9300595238095238


	Epoch 192
Training results:
gen_loss: 56.67951
disc_loss: 0.23142469
disc_acc: 0.9832920792079208

Validation results:
gen_loss: 57.938217
disc_loss: 2.9352572
disc_acc: 0.9112103174603174


	Epoch 193
Training results:
gen_loss: 57.55968
disc_loss: 0.19523787
disc_acc: 0.9866336633663366

Validation results:
gen_loss: 52.766586
disc_loss: 3.6670132
disc_acc: 0.873015873015873


	Epoch 194
Training results:
gen_loss: 63.684147
disc_loss: 0.15632191
disc_acc: 0.9876237623762376

Validation results:
gen_loss: 63.91288
disc_loss: 3.2433643
disc_acc: 0.8918650793650794


	Epoch 195
Training results:
gen_loss: 64.71288
disc_loss: 0.21120708
disc_acc: 0.9846534653465346

Validation results:
gen_loss: 60.782536
disc_loss: 3.0833263
disc_acc: 0.9052579365079365


	Epoch 196
Training results:
gen_loss: 62.596447
disc_loss: 0.14369972
disc_acc: 0.9905940594059406

Validation results:
gen_loss: 59.90129
disc_loss: 2.353162
disc_acc: 0.9265873015873016


	Epoch 197
Training results:
gen_loss: 61.606335
disc_loss: 0.2301242
disc_acc: 0.9851485148514851

Validation results:
gen_loss: 58.048096
disc_loss: 4.284019
disc_acc: 0.8735119047619048


	Epoch 198
Training results:
gen_loss: 65.9577
disc_loss: 0.12787554
disc_acc: 0.9899752475247525

Validation results:
gen_loss: 60.61494
disc_loss: 2.8897693
disc_acc: 0.9122023809523809


	Epoch 199
Training results:
gen_loss: 62.196133
disc_loss: 0.19159065
disc_acc: 0.9875

Validation results:
gen_loss: 64.99399
disc_loss: 2.573864
disc_acc: 0.9107142857142857


	Epoch 200
Training results:
gen_loss: 61.736076
disc_loss: 0.1690991
disc_acc: 0.9871287128712871

Validation results:
gen_loss: 62.01886
disc_loss: 2.8517606
disc_acc: 0.9141865079365079


	Epoch 201
Training results:
gen_loss: 65.14824
disc_loss: 0.17149423
disc_acc: 0.9886138613861386

Validation results:
gen_loss: 69.92313
disc_loss: 2.1397855
disc_acc: 0.9216269841269841


	Epoch 202
Training results:
gen_loss: 66.46352
disc_loss: 0.19583884
disc_acc: 0.9852722772277228

Validation results:
gen_loss: 65.94943
disc_loss: 2.4807198
disc_acc: 0.9260912698412699


	Epoch 203
Training results:
gen_loss: 64.95706
disc_loss: 0.10022906
disc_acc: 0.9923267326732673

Validation results:
gen_loss: 67.63927
disc_loss: 2.4090416
disc_acc: 0.9285714285714286


	Epoch 204
Training results:
gen_loss: 63.22101
disc_loss: 0.18788904
disc_acc: 0.9889851485148515

Validation results:
gen_loss: 62.601982
disc_loss: 2.2846007
disc_acc: 0.9211309523809523


	Epoch 205
Training results:
gen_loss: 63.042477
disc_loss: 0.13340712
disc_acc: 0.9909653465346535

Validation results:
gen_loss: 65.22625
disc_loss: 2.8228495
disc_acc: 0.9156746031746031


	Epoch 206
Training results:
gen_loss: 65.59634
disc_loss: 0.32082647
disc_acc: 0.9827970297029703

Validation results:
gen_loss: 68.55452
disc_loss: 2.5147407
disc_acc: 0.9231150793650794


	Epoch 207
Training results:
gen_loss: 67.50974
disc_loss: 0.21651115
disc_acc: 0.988490099009901

Validation results:
gen_loss: 64.544044
disc_loss: 4.3468213
disc_acc: 0.8824404761904762


	Epoch 208
Training results:
gen_loss: 66.335464
disc_loss: 0.07246439
disc_acc: 0.9929455445544555

Validation results:
gen_loss: 65.56823
disc_loss: 2.5757174
disc_acc: 0.9285714285714286


	Epoch 209
Training results:
gen_loss: 66.66238
disc_loss: 0.21106851
disc_acc: 0.9870049504950495

Validation results:
gen_loss: 69.4877
disc_loss: 2.3359852
disc_acc: 0.9226190476190477


	Epoch 210
Training results:
gen_loss: 66.62562
disc_loss: 0.1766367
disc_acc: 0.989480198019802

Validation results:
gen_loss: 62.9569
disc_loss: 2.2567408
disc_acc: 0.9201388888888888


	Epoch 211
Training results:
gen_loss: 66.65098
disc_loss: 0.20803124
disc_acc: 0.989480198019802

Validation results:
gen_loss: 66.25984
disc_loss: 2.5506759
disc_acc: 0.9136904761904762


	Epoch 212
Training results:
gen_loss: 69.209564
disc_loss: 0.3273527
disc_acc: 0.9832920792079208

Validation results:
gen_loss: 66.04977
disc_loss: 2.138182
disc_acc: 0.9241071428571429


	Epoch 213
Training results:
gen_loss: 68.12256
disc_loss: 0.12474433
disc_acc: 0.993440594059406

Validation results:
gen_loss: 71.01496
disc_loss: 2.371737
disc_acc: 0.9300595238095238


	Epoch 214
Training results:
gen_loss: 70.204124
disc_loss: 0.12634163
disc_acc: 0.9918316831683168

Validation results:
gen_loss: 76.50034
disc_loss: 2.4554753
disc_acc: 0.9176587301587301


	Epoch 215
Training results:
gen_loss: 70.70234
disc_loss: 0.17600189
disc_acc: 0.988490099009901

Validation results:
gen_loss: 73.04461
disc_loss: 4.542796
disc_acc: 0.8849206349206349


	Epoch 216
Training results:
gen_loss: 69.93292
disc_loss: 0.19701955
disc_acc: 0.9883663366336634

Validation results:
gen_loss: 68.87538
disc_loss: 2.4865842
disc_acc: 0.9221230158730159


	Epoch 217
Training results:
gen_loss: 72.520226
disc_loss: 0.13939133
disc_acc: 0.9925742574257426

Validation results:
gen_loss: 68.75318
disc_loss: 3.624895
disc_acc: 0.8938492063492064


	Epoch 218
Training results:
gen_loss: 70.75742
disc_loss: 0.2718968
disc_acc: 0.9862623762376238

Validation results:
gen_loss: 69.06284
disc_loss: 3.36458
disc_acc: 0.9097222222222222


	Epoch 219
Training results:
gen_loss: 68.59804
disc_loss: 0.16531481
disc_acc: 0.9893564356435643

Validation results:
gen_loss: 75.63212
disc_loss: 2.842485
disc_acc: 0.9221230158730159


	Epoch 220
Training results:
gen_loss: 73.791725
disc_loss: 0.07462111
disc_acc: 0.9939356435643565

Validation results:
gen_loss: 74.77877
disc_loss: 2.5484557
disc_acc: 0.9250992063492064


	Epoch 221
Training results:
gen_loss: 72.235725
disc_loss: 0.2926434
disc_acc: 0.9853960396039604

Validation results:
gen_loss: 82.30996
disc_loss: 4.6677647
disc_acc: 0.8913690476190477


	Epoch 222
Training results:
gen_loss: 77.9975
disc_loss: 0.2152906
disc_acc: 0.9868811881188119

Validation results:
gen_loss: 74.10998
disc_loss: 2.861906
disc_acc: 0.9226190476190477


	Epoch 223
Training results:
gen_loss: 71.54493
disc_loss: 0.16769986
disc_acc: 0.9886138613861386

Validation results:
gen_loss: 78.436264
disc_loss: 3.3936157
disc_acc: 0.9166666666666666


	Epoch 224
Training results:
gen_loss: 73.72856
disc_loss: 0.16370016
disc_acc: 0.9886138613861386

Validation results:
gen_loss: 70.20279
disc_loss: 2.7303061
disc_acc: 0.9166666666666666


	Epoch 225
Training results:
gen_loss: 74.81603
disc_loss: 0.17478028
disc_acc: 0.9912128712871288

Validation results:
gen_loss: 71.917984
disc_loss: 3.1373968
disc_acc: 0.9141865079365079


	Epoch 226
Training results:
gen_loss: 74.27835
disc_loss: 0.15976901
disc_acc: 0.9897277227722773

Validation results:
gen_loss: 73.09425
disc_loss: 2.9612217
disc_acc: 0.9241071428571429


	Epoch 227
Training results:
gen_loss: 72.07464
disc_loss: 0.2900656
disc_acc: 0.9872524752475248

Validation results:
gen_loss: 77.55512
disc_loss: 5.3050275
disc_acc: 0.8844246031746031


	Epoch 228
Training results:
gen_loss: 79.139496
disc_loss: 0.18607421
disc_acc: 0.9905940594059406

Validation results:
gen_loss: 77.463554
disc_loss: 2.889996
disc_acc: 0.9255952380952381


	Epoch 229
Training results:
gen_loss: 79.309654
disc_loss: 0.11135126
disc_acc: 0.992450495049505

Validation results:
gen_loss: 78.85343
disc_loss: 4.2502785
disc_acc: 0.9032738095238095


	Epoch 230
Training results:
gen_loss: 79.958496
disc_loss: 0.17592216
disc_acc: 0.9905940594059406

Validation results:
gen_loss: 81.04416
disc_loss: 2.9834065
disc_acc: 0.9211309523809523


	Epoch 231
Training results:
gen_loss: 82.885704
disc_loss: 0.31765646
disc_acc: 0.9855198019801981

Validation results:
gen_loss: 79.66569
disc_loss: 2.8523557
disc_acc: 0.9280753968253969


	Epoch 232
Training results:
gen_loss: 76.9121
disc_loss: 0.1220798
disc_acc: 0.993440594059406

Validation results:
gen_loss: 73.55676
disc_loss: 2.4221704
disc_acc: 0.939484126984127


	Epoch 233
Training results:
gen_loss: 76.66192
disc_loss: 0.19776392
disc_acc: 0.990470297029703

Validation results:
gen_loss: 75.00911
disc_loss: 5.2492094
disc_acc: 0.8764880952380952


	Epoch 234
Training results:
gen_loss: 81.25133
disc_loss: 0.16809523
disc_acc: 0.9898514851485148

Validation results:
gen_loss: 79.81162
disc_loss: 4.4353237
disc_acc: 0.8998015873015873


	Epoch 235
Training results:
gen_loss: 79.81717
disc_loss: 0.27690142
disc_acc: 0.986509900990099

Validation results:
gen_loss: 84.4801
disc_loss: 2.8589447
disc_acc: 0.9221230158730159


	Epoch 236
Training results:
gen_loss: 78.56063
disc_loss: 0.24832469
disc_acc: 0.9886138613861386

Validation results:
gen_loss: 75.72455
disc_loss: 2.6363575
disc_acc: 0.9255952380952381


	Epoch 237
Training results:
gen_loss: 80.78436
disc_loss: 0.14993156
disc_acc: 0.991460396039604

Validation results:
gen_loss: 83.59122
disc_loss: 3.1983669
disc_acc: 0.9156746031746031


	Epoch 238
Training results:
gen_loss: 84.52072
disc_loss: 0.20186147
disc_acc: 0.9909653465346535

Validation results:
gen_loss: 82.526054
disc_loss: 5.0484595
disc_acc: 0.8888888888888888


	Epoch 239
Training results:
gen_loss: 82.79007
disc_loss: 0.22274223
disc_acc: 0.9888613861386139

Validation results:
gen_loss: 85.76081
disc_loss: 3.967311
disc_acc: 0.9112103174603174


	Epoch 240
Training results:
gen_loss: 81.93571
disc_loss: 0.111553326
disc_acc: 0.9929455445544555

Validation results:
gen_loss: 85.21707
disc_loss: 7.966497
disc_acc: 0.8298611111111112


	Epoch 241
Training results:
gen_loss: 77.31065
disc_loss: 0.1887067
disc_acc: 0.9900990099009901

Validation results:
gen_loss: 73.129364
disc_loss: 3.2338884
disc_acc: 0.9131944444444444


	Epoch 242
Training results:
gen_loss: 75.28234
disc_loss: 0.13351229
disc_acc: 0.9910891089108911

Validation results:
gen_loss: 84.305916
disc_loss: 3.3239882
disc_acc: 0.9122023809523809


	Epoch 243
Training results:
gen_loss: 82.55491
disc_loss: 0.2751743
disc_acc: 0.9852722772277228

Validation results:
gen_loss: 84.106514
disc_loss: 4.676158
disc_acc: 0.8983134920634921


	Epoch 244
Training results:
gen_loss: 83.97717
disc_loss: 0.28302854
disc_acc: 0.9887376237623763

Validation results:
gen_loss: 86.55484
disc_loss: 4.6794314
disc_acc: 0.9047619047619048


	Epoch 245
Training results:
gen_loss: 86.88496
disc_loss: 0.12164367
disc_acc: 0.9922029702970298

Validation results:
gen_loss: 81.970795
disc_loss: 2.9724154
disc_acc: 0.9300595238095238


	Epoch 246
Training results:
gen_loss: 82.37642
disc_loss: 0.10133035
disc_acc: 0.9939356435643565

Validation results:
gen_loss: 78.61159
disc_loss: 3.6806479
disc_acc: 0.9122023809523809


	Epoch 247
Training results:
gen_loss: 82.05633
disc_loss: 0.19054218
disc_acc: 0.9920792079207921

Validation results:
gen_loss: 82.078705
disc_loss: 4.391997
disc_acc: 0.8928571428571429


	Epoch 248
Training results:
gen_loss: 84.433014
disc_loss: 0.26510248
disc_acc: 0.9876237623762376

Validation results:
gen_loss: 78.69013
disc_loss: 5.1410403
disc_acc: 0.8814484126984127


	Epoch 249
Training results:
gen_loss: 87.93354
disc_loss: 0.3199212
disc_acc: 0.9860148514851486

Validation results:
gen_loss: 81.49822
disc_loss: 4.3474846
disc_acc: 0.9007936507936508


	Epoch 250
Training results:
gen_loss: 88.23328
disc_loss: 0.12136139
disc_acc: 0.9918316831683168

Validation results:
gen_loss: 93.20436
disc_loss: 3.0811129
disc_acc: 0.9231150793650794


	Epoch 251
Training results:
gen_loss: 87.36346
disc_loss: 0.12470131
disc_acc: 0.9931930693069307

Validation results:
gen_loss: 93.1659
disc_loss: 4.895758
disc_acc: 0.8963293650793651


	Epoch 252
Training results:
gen_loss: 87.97898
disc_loss: 0.14433576
disc_acc: 0.9922029702970298

Validation results:
gen_loss: 92.71429
disc_loss: 3.4566967
disc_acc: 0.9156746031746031


	Epoch 253
Training results:
gen_loss: 85.763084
disc_loss: 0.24846955
disc_acc: 0.9881188118811881

Validation results:
gen_loss: 88.95642
disc_loss: 4.9544244
disc_acc: 0.8913690476190477


	Epoch 254
Training results:
gen_loss: 91.43138
disc_loss: 0.23258172
disc_acc: 0.9899752475247525

Validation results:
gen_loss: 90.279976
disc_loss: 3.412483
disc_acc: 0.9186507936507936


	Epoch 255
Training results:
gen_loss: 89.740326
disc_loss: 0.12006118
disc_acc: 0.9931930693069307

Validation results:
gen_loss: 85.213776
disc_loss: 3.2413516
disc_acc: 0.9186507936507936


	Epoch 256
Training results:
gen_loss: 87.94275
disc_loss: 0.1984048
disc_acc: 0.9907178217821783

Validation results:
gen_loss: 79.69921
disc_loss: 7.9799476
disc_acc: 0.8670634920634921


	Epoch 257
Training results:
gen_loss: 86.89729
disc_loss: 0.17914909
disc_acc: 0.9917079207920793

Validation results:
gen_loss: 83.24979
disc_loss: 3.4099681
disc_acc: 0.9191468253968254


	Epoch 258
Training results:
gen_loss: 86.81268
disc_loss: 0.20460798
disc_acc: 0.9913366336633663

Validation results:
gen_loss: 90.86949
disc_loss: 4.221029
disc_acc: 0.9126984126984127


	Epoch 259
Training results:
gen_loss: 95.75362
disc_loss: 0.27238086
disc_acc: 0.9882425742574258

Validation results:
gen_loss: 94.2715
disc_loss: 3.0039985
disc_acc: 0.9290674603174603


	Epoch 260
Training results:
gen_loss: 94.90135
disc_loss: 0.16020758
disc_acc: 0.9918316831683168

Validation results:
gen_loss: 88.41144
disc_loss: 3.8074076
disc_acc: 0.9246031746031746


	Epoch 261
Training results:
gen_loss: 88.36303
disc_loss: 0.18354015
disc_acc: 0.9930693069306931

Validation results:
gen_loss: 86.78793
disc_loss: 3.5743005
disc_acc: 0.9330357142857143


	Epoch 262
Training results:
gen_loss: 87.450035
disc_loss: 0.2284753
disc_acc: 0.9893564356435643

Validation results:
gen_loss: 88.54446
disc_loss: 3.5781105
disc_acc: 0.9211309523809523


	Epoch 263
Training results:
gen_loss: 88.23913
disc_loss: 0.214653
disc_acc: 0.990470297029703

Validation results:
gen_loss: 88.23815
disc_loss: 5.1577654
disc_acc: 0.908234126984127


	Epoch 264
Training results:
gen_loss: 96.83402
disc_loss: 0.4041722
disc_acc: 0.9856435643564356

Validation results:
gen_loss: 118.41244
disc_loss: 6.0060935
disc_acc: 0.8958333333333334


	Epoch 265
Training results:
gen_loss: 105.588165
disc_loss: 0.24476
disc_acc: 0.9909653465346535

Validation results:
gen_loss: 101.741325
disc_loss: 6.001373
disc_acc: 0.9067460317460317


	Epoch 266
Training results:
gen_loss: 103.191444
disc_loss: 0.15098514
disc_acc: 0.9926980198019802

Validation results:
gen_loss: 106.053
disc_loss: 5.5230446
disc_acc: 0.9126984126984127


	Epoch 267
Training results:
gen_loss: 99.388954
disc_loss: 0.2130742
disc_acc: 0.9918316831683168

Validation results:
gen_loss: 97.734604
disc_loss: 4.426999
disc_acc: 0.910218253968254


	Epoch 268
Training results:
gen_loss: 98.56149
disc_loss: 0.13891324
disc_acc: 0.9917079207920793

Validation results:
gen_loss: 97.6257
disc_loss: 4.1618075
disc_acc: 0.9231150793650794


	Epoch 269
Training results:
gen_loss: 99.64725
disc_loss: 0.15814199
disc_acc: 0.9922029702970298

Validation results:
gen_loss: 97.29707
disc_loss: 6.2310343
disc_acc: 0.9002976190476191


	Epoch 270
Training results:
gen_loss: 96.01924
disc_loss: 0.17628437
disc_acc: 0.9920792079207921

Validation results:
gen_loss: 93.637825
disc_loss: 3.3347
disc_acc: 0.9270833333333334


	Epoch 271
Training results:
gen_loss: 104.38322
disc_loss: 0.28673345
disc_acc: 0.9872524752475248

Validation results:
gen_loss: 103.56528
disc_loss: 3.3052912
disc_acc: 0.9280753968253969


	Epoch 272
Training results:
gen_loss: 99.1952
disc_loss: 0.17874508
disc_acc: 0.9929455445544555

Validation results:
gen_loss: 96.51882
disc_loss: 5.955835
disc_acc: 0.9126984126984127


	Epoch 273
Training results:
gen_loss: 93.73111
disc_loss: 0.11841373
disc_acc: 0.9939356435643565

Validation results:
gen_loss: 89.72025
disc_loss: 3.15816
disc_acc: 0.9260912698412699


	Epoch 274
Training results:
gen_loss: 91.33125
disc_loss: 0.13088757
disc_acc: 0.993440594059406

Validation results:
gen_loss: 97.22849
disc_loss: 4.8242145
disc_acc: 0.9092261904761905


	Epoch 275
Training results:
gen_loss: 96.70947
disc_loss: 0.14668289
disc_acc: 0.9929455445544555

Validation results:
gen_loss: 92.23956
disc_loss: 4.020975
disc_acc: 0.9241071428571429


	Epoch 276
Training results:
gen_loss: 97.070946
disc_loss: 0.18691875
disc_acc: 0.9918316831683168

Validation results:
gen_loss: 105.317276
disc_loss: 5.5108514
disc_acc: 0.9007936507936508


	Epoch 277
Training results:
gen_loss: 98.209435
disc_loss: 0.17120805
disc_acc: 0.9925742574257426

Validation results:
gen_loss: 106.345245
disc_loss: 3.8864982
disc_acc: 0.9255952380952381


	Epoch 278
Training results:
gen_loss: 101.77905
disc_loss: 0.40772602
disc_acc: 0.9856435643564356

Validation results:
gen_loss: 110.603134
disc_loss: 6.4274035
disc_acc: 0.9072420634920635


	Epoch 279
Training results:
gen_loss: 106.85474
disc_loss: 0.24365628
disc_acc: 0.9917079207920793

Validation results:
gen_loss: 107.964806
disc_loss: 4.6686583
disc_acc: 0.9211309523809523


	Epoch 280
Training results:
gen_loss: 105.66227
disc_loss: 0.22291656
disc_acc: 0.991460396039604

Validation results:
gen_loss: 108.14046
disc_loss: 5.373592
disc_acc: 0.9181547619047619


	Epoch 281
Training results:
gen_loss: 103.24666
disc_loss: 0.08902292
disc_acc: 0.996410891089109

Validation results:
gen_loss: 103.199356
disc_loss: 4.8643622
disc_acc: 0.9246031746031746


	Epoch 282
Training results:
gen_loss: 98.152534
disc_loss: 0.21405265
disc_acc: 0.9913366336633663

Validation results:
gen_loss: 106.73441
disc_loss: 5.839581
disc_acc: 0.908234126984127


	Epoch 283
Training results:
gen_loss: 98.47652
disc_loss: 0.29555744
disc_acc: 0.9893564356435643

Validation results:
gen_loss: 98.75613
disc_loss: 4.6050982
disc_acc: 0.9092261904761905


	Epoch 284
Training results:
gen_loss: 102.03616
disc_loss: 0.2522434
disc_acc: 0.9897277227722773

Validation results:
gen_loss: 108.607506
disc_loss: 4.265063
disc_acc: 0.9280753968253969


	Epoch 285
Training results:
gen_loss: 100.91167
disc_loss: 0.122677274
disc_acc: 0.9936881188118812

Validation results:
gen_loss: 100.69884
disc_loss: 4.8935995
disc_acc: 0.9161706349206349


	Epoch 286
Training results:
gen_loss: 104.20347
disc_loss: 0.18163696
disc_acc: 0.992450495049505

Validation results:
gen_loss: 105.37211
disc_loss: 4.0490456
disc_acc: 0.9315476190476191


	Epoch 287
Training results:
gen_loss: 107.16105
disc_loss: 0.15454935
disc_acc: 0.993440594059406

Validation results:
gen_loss: 105.014824
disc_loss: 5.6697745
disc_acc: 0.9181547619047619


	Epoch 288
Training results:
gen_loss: 106.844536
disc_loss: 0.4759812
disc_acc: 0.9857673267326733

Validation results:
gen_loss: 102.836494
disc_loss: 5.319422
disc_acc: 0.902281746031746


	Epoch 289
Training results:
gen_loss: 98.970314
disc_loss: 0.1807234
disc_acc: 0.9926980198019802

Validation results:
gen_loss: 93.31818
disc_loss: 4.421227
disc_acc: 0.9186507936507936


	Epoch 290
Training results:
gen_loss: 99.57076
disc_loss: 0.09463864
disc_acc: 0.995420792079208

Validation results:
gen_loss: 102.79934
disc_loss: 4.465689
disc_acc: 0.9236111111111112


	Epoch 291
Training results:
gen_loss: 100.21658
disc_loss: 0.1279802
disc_acc: 0.9951732673267327

Validation results:
gen_loss: 101.2031
disc_loss: 4.178317
disc_acc: 0.9211309523809523


	Epoch 292
Training results:
gen_loss: 106.37041
disc_loss: 0.33103603
disc_acc: 0.9876237623762376

Validation results:
gen_loss: 115.28715
disc_loss: 5.230526
disc_acc: 0.9156746031746031


	Epoch 293
Training results:
gen_loss: 115.91011
disc_loss: 0.19398037
disc_acc: 0.9933168316831683

Validation results:
gen_loss: 111.90079
disc_loss: 4.5938373
disc_acc: 0.9206349206349206


	Epoch 294
Training results:
gen_loss: 107.63053
disc_loss: 0.12745687
disc_acc: 0.9935643564356436

Validation results:
gen_loss: 102.877625
disc_loss: 5.3866663
disc_acc: 0.9067460317460317


	Epoch 295
Training results:
gen_loss: 107.929985
disc_loss: 0.2242217
disc_acc: 0.9909653465346535

Validation results:
gen_loss: 112.4629
disc_loss: 6.0437117
disc_acc: 0.9171626984126984


	Epoch 296
Training results:
gen_loss: 107.30942
disc_loss: 0.03828347
disc_acc: 0.9972772277227723

Validation results:
gen_loss: 109.00294
disc_loss: 3.467769
disc_acc: 0.9310515873015873


	Epoch 297
Training results:
gen_loss: 114.98422
disc_loss: 0.33601058
disc_acc: 0.9883663366336634

Validation results:
gen_loss: 104.60136
disc_loss: 6.235191
disc_acc: 0.8928571428571429


	Epoch 298
Training results:
gen_loss: 114.088554
disc_loss: 0.29278293
disc_acc: 0.9889851485148515

Validation results:
gen_loss: 115.08626
disc_loss: 5.044262
disc_acc: 0.9206349206349206


	Epoch 299
Training results:
gen_loss: 111.79081
disc_loss: 0.23874864
disc_acc: 0.9928217821782178

Validation results:
gen_loss: 113.74525
disc_loss: 4.157054
disc_acc: 0.9305555555555556


	Epoch 300
Training results:
gen_loss: 116.115776
disc_loss: 0.1918289
disc_acc: 0.9929455445544555

Validation results:
gen_loss: 117.138336
disc_loss: 3.7351565
disc_acc: 0.9295634920634921


	Epoch 301
Training results:
gen_loss: 112.82982
disc_loss: 0.13355264
disc_acc: 0.9949257425742575

Validation results:
gen_loss: 107.51071
disc_loss: 4.1146607
disc_acc: 0.9275793650793651


	Epoch 302
Training results:
gen_loss: 113.555084
disc_loss: 0.25386286
disc_acc: 0.9907178217821783

Validation results:
gen_loss: 120.5567
disc_loss: 4.8116674
disc_acc: 0.9186507936507936


	Epoch 303
Training results:
gen_loss: 123.69337
disc_loss: 0.32840192
disc_acc: 0.9899752475247525

Validation results:
gen_loss: 122.547714
disc_loss: 6.6531744
disc_acc: 0.9057539682539683


	Epoch 304
Training results:
gen_loss: 118.948265
disc_loss: 0.16953292
disc_acc: 0.9923267326732673

Validation results:
gen_loss: 112.74902
disc_loss: 4.284238
disc_acc: 0.9255952380952381


	Epoch 305
Training results:
gen_loss: 115.11502
disc_loss: 0.11230734
disc_acc: 0.995049504950495

Validation results:
gen_loss: 116.666115
disc_loss: 5.4369054
disc_acc: 0.910218253968254


	Epoch 306
Training results:
gen_loss: 117.4257
disc_loss: 0.26701182
disc_acc: 0.9910891089108911

Validation results:
gen_loss: 117.78054
disc_loss: 4.053629
disc_acc: 0.9320436507936508


	Epoch 307
Training results:
gen_loss: 123.94703
disc_loss: 0.17332256
disc_acc: 0.993440594059406

Validation results:
gen_loss: 117.609726
disc_loss: 6.03367
disc_acc: 0.9067460317460317


	Epoch 308
Training results:
gen_loss: 122.54193
disc_loss: 0.36713153
disc_acc: 0.9887376237623763

Validation results:
gen_loss: 123.93428
disc_loss: 6.259283
disc_acc: 0.9117063492063492


	Epoch 309
Training results:
gen_loss: 119.170906
disc_loss: 0.31007954
disc_acc: 0.9931930693069307

Validation results:
gen_loss: 125.4401
disc_loss: 5.9701605
disc_acc: 0.9206349206349206


	Epoch 310
Training results:
gen_loss: 115.08986
disc_loss: 0.26850435
disc_acc: 0.9919554455445545

Validation results:
gen_loss: 115.706604
disc_loss: 4.639933
disc_acc: 0.9201388888888888


	Epoch 311
Training results:
gen_loss: 114.61088
disc_loss: 0.12184284
disc_acc: 0.9946782178217822

Validation results:
gen_loss: 111.17106
disc_loss: 3.8040128
disc_acc: 0.9360119047619048


	Epoch 312
Training results:
gen_loss: 109.09601
disc_loss: 0.09453514
disc_acc: 0.996039603960396

Validation results:
gen_loss: 105.033104
disc_loss: 4.484313
disc_acc: 0.9136904761904762


	Epoch 313
Training results:
gen_loss: 114.144196
disc_loss: 0.246093
disc_acc: 0.9915841584158416

Validation results:
gen_loss: 103.6315
disc_loss: 6.747723
disc_acc: 0.8923611111111112


	Epoch 314
Training results:
gen_loss: 112.15934
disc_loss: 0.27189693
disc_acc: 0.9909653465346535

Validation results:
gen_loss: 114.10437
disc_loss: 5.997655
disc_acc: 0.9067460317460317


	Epoch 315
Training results:
gen_loss: 121.13665
disc_loss: 0.37953922
disc_acc: 0.9876237623762376

Validation results:
gen_loss: 130.28084
disc_loss: 5.8302016
disc_acc: 0.9196428571428571


	Epoch 316
Training results:
gen_loss: 121.0365
disc_loss: 0.11305511
disc_acc: 0.9956683168316832

Validation results:
gen_loss: 119.39872
disc_loss: 6.303351
disc_acc: 0.9077380952380952


	Epoch 317
Training results:
gen_loss: 119.03502
disc_loss: 0.18146785
disc_acc: 0.9938118811881188

Validation results:
gen_loss: 124.329094
disc_loss: 8.848699
disc_acc: 0.8938492063492064


	Epoch 318
Training results:
gen_loss: 120.14249
disc_loss: 0.22644717
disc_acc: 0.9930693069306931

Validation results:
gen_loss: 124.135704
disc_loss: 4.92435
disc_acc: 0.9191468253968254


	Epoch 319
Training results:
gen_loss: 123.9843
disc_loss: 0.24768622
disc_acc: 0.9918316831683168

Validation results:
gen_loss: 130.16447
disc_loss: 4.898667
disc_acc: 0.9280753968253969


	Epoch 320
Training results:
gen_loss: 120.77547
disc_loss: 0.27987263
disc_acc: 0.990470297029703

Validation results:
gen_loss: 129.3131
disc_loss: 11.91368
disc_acc: 0.8705357142857143


	Epoch 321
Training results:
gen_loss: 124.12116
disc_loss: 0.15382527
disc_acc: 0.9945544554455445

Validation results:
gen_loss: 126.00627
disc_loss: 4.8435917
disc_acc: 0.9345238095238095


	Epoch 322
Training results:
gen_loss: 120.66435
disc_loss: 0.1158067
disc_acc: 0.995420792079208

Validation results:
gen_loss: 114.55488
disc_loss: 5.2327003
disc_acc: 0.9231150793650794


	Epoch 323
Training results:
gen_loss: 125.39412
disc_loss: 0.32070115
disc_acc: 0.989480198019802

Validation results:
gen_loss: 134.18495
disc_loss: 5.9906106
disc_acc: 0.9241071428571429


	Epoch 324
Training results:
gen_loss: 123.277306
disc_loss: 0.15386279
disc_acc: 0.9941831683168317

Validation results:
gen_loss: 131.28102
disc_loss: 5.8593965
disc_acc: 0.9181547619047619


	Epoch 325
Training results:
gen_loss: 130.3029
disc_loss: 0.25028294
disc_acc: 0.9919554455445545

Validation results:
gen_loss: 132.0943
disc_loss: 4.8557353
disc_acc: 0.9290674603174603


	Epoch 326
Training results:
gen_loss: 128.29073
disc_loss: 0.17583235
disc_acc: 0.9936881188118812

Validation results:
gen_loss: 127.45656
disc_loss: 8.212649
disc_acc: 0.9072420634920635


	Epoch 327
Training results:
gen_loss: 131.86609
disc_loss: 0.27372488
disc_acc: 0.9909653465346535

Validation results:
gen_loss: 132.75258
disc_loss: 5.826162
disc_acc: 0.9226190476190477


	Epoch 328
Training results:
gen_loss: 141.37021
disc_loss: 0.24223515
disc_acc: 0.9908415841584158

Validation results:
gen_loss: 150.93741
disc_loss: 7.193409
disc_acc: 0.9191468253968254


	Epoch 329
Training results:
gen_loss: 140.06184
disc_loss: 0.3086163
disc_acc: 0.9928217821782178

Validation results:
gen_loss: 129.59142
disc_loss: 6.327472
disc_acc: 0.9156746031746031


	Epoch 330
Training results:
gen_loss: 132.48862
disc_loss: 0.20075409
disc_acc: 0.9941831683168317

Validation results:
gen_loss: 139.10211
disc_loss: 6.89826
disc_acc: 0.9122023809523809


	Epoch 331
Training results:
gen_loss: 137.09459
disc_loss: 0.25823003
disc_acc: 0.9920792079207921

Validation results:
gen_loss: 133.50056
disc_loss: 6.445279
disc_acc: 0.9146825396825397


	Epoch 332
Training results:
gen_loss: 133.95866
disc_loss: 0.18284129
disc_acc: 0.994059405940594

Validation results:
gen_loss: 137.6222
disc_loss: 6.77766
disc_acc: 0.9156746031746031


	Epoch 333
Training results:
gen_loss: 140.85928
disc_loss: 0.1637533
disc_acc: 0.994430693069307

Validation results:
gen_loss: 143.45233
disc_loss: 5.410072
disc_acc: 0.9295634920634921


	Epoch 334
Training results:
gen_loss: 140.19035
disc_loss: 0.31152606
disc_acc: 0.9915841584158416

Validation results:
gen_loss: 136.31651
disc_loss: 5.0150743
disc_acc: 0.9260912698412699


	Epoch 335
Training results:
gen_loss: 138.19987
disc_loss: 0.11573818
disc_acc: 0.9965346534653465

Validation results:
gen_loss: 134.95232
disc_loss: 5.64059
disc_acc: 0.9241071428571429


	Epoch 336
Training results:
gen_loss: 128.9476
disc_loss: 0.30154812
disc_acc: 0.9920792079207921

Validation results:
gen_loss: 130.23952
disc_loss: 5.3429775
disc_acc: 0.9176587301587301


	Epoch 337
Training results:
gen_loss: 135.22977
disc_loss: 0.46920407
disc_acc: 0.988490099009901

Validation results:
gen_loss: 135.74092
disc_loss: 6.2559953
disc_acc: 0.9161706349206349


	Epoch 338
Training results:
gen_loss: 133.64072
disc_loss: 0.1318481
disc_acc: 0.9949257425742575

Validation results:
gen_loss: 125.34373
disc_loss: 6.5228157
disc_acc: 0.9161706349206349


	Epoch 339
Training results:
gen_loss: 131.00488
disc_loss: 0.20867237
disc_acc: 0.9936881188118812

Validation results:
gen_loss: 137.12877
disc_loss: 10.567216
disc_acc: 0.8943452380952381


	Epoch 340
Training results:
gen_loss: 132.51648
disc_loss: 0.23069274
disc_acc: 0.9933168316831683

Validation results:
gen_loss: 134.46402
disc_loss: 5.835641
disc_acc: 0.9285714285714286


	Epoch 341
Training results:
gen_loss: 135.26414
disc_loss: 0.20746464
disc_acc: 0.9928217821782178

Validation results:
gen_loss: 142.94745
disc_loss: 6.2327714
disc_acc: 0.9226190476190477


	Epoch 342
Training results:
gen_loss: 136.08936
disc_loss: 0.35698667
disc_acc: 0.9902227722772278

Validation results:
gen_loss: 134.91212
disc_loss: 5.0674334
disc_acc: 0.9290674603174603


	Epoch 343
Training results:
gen_loss: 141.08199
disc_loss: 0.14580691
disc_acc: 0.995049504950495

Validation results:
gen_loss: 141.10187
disc_loss: 7.8452396
disc_acc: 0.8943452380952381


	Epoch 344
Training results:
gen_loss: 133.59103
disc_loss: 0.44423303
disc_acc: 0.9881188118811881

Validation results:
gen_loss: 130.52232
disc_loss: 5.7612247
disc_acc: 0.9186507936507936


	Epoch 345
Training results:
gen_loss: 131.12321
disc_loss: 0.13525282
disc_acc: 0.9957920792079208

Validation results:
gen_loss: 132.98323
disc_loss: 5.7608867
disc_acc: 0.9265873015873016


	Epoch 346
Training results:
gen_loss: 137.67992
disc_loss: 0.1451917
disc_acc: 0.9948019801980198

Validation results:
gen_loss: 138.6272
disc_loss: 5.3598332
disc_acc: 0.9305555555555556


	Epoch 347
Training results:
gen_loss: 138.75711
disc_loss: 0.1841245
disc_acc: 0.9941831683168317

Validation results:
gen_loss: 145.00546
disc_loss: 5.880486
disc_acc: 0.9226190476190477


	Epoch 348
Training results:
gen_loss: 136.13036
disc_loss: 0.3209617
disc_acc: 0.9903465346534653

Validation results:
gen_loss: 139.20737
disc_loss: 6.0412006
disc_acc: 0.9181547619047619


	Epoch 349
Training results:
gen_loss: 139.2371
disc_loss: 0.17342488
disc_acc: 0.993440594059406

Validation results:
gen_loss: 138.32486
disc_loss: 5.659454
disc_acc: 0.9260912698412699


	Epoch 350
Training results:
gen_loss: 138.14647
disc_loss: 0.18974066
disc_acc: 0.9936881188118812

Validation results:
gen_loss: 146.10997
disc_loss: 5.679879
disc_acc: 0.9325396825396826


	Epoch 351
Training results:
gen_loss: 144.37437
disc_loss: 0.06342106
disc_acc: 0.9982673267326733

Validation results:
gen_loss: 142.47688
disc_loss: 8.030655
disc_acc: 0.9156746031746031


	Epoch 352
Training results:
gen_loss: 140.76166
disc_loss: 0.32415074
disc_acc: 0.9918316831683168

Validation results:
gen_loss: 146.97795
disc_loss: 7.3562284
disc_acc: 0.9146825396825397


	Epoch 353
Training results:
gen_loss: 156.79521
disc_loss: 0.4786672
disc_acc: 0.9893564356435643

Validation results:
gen_loss: 160.29442
disc_loss: 5.6856384
disc_acc: 0.9226190476190477


	Epoch 354
Training results:
gen_loss: 156.13051
disc_loss: 0.21374227
disc_acc: 0.9931930693069307

Validation results:
gen_loss: 158.40941
disc_loss: 6.4178123
disc_acc: 0.9241071428571429


	Epoch 355
Training results:
gen_loss: 154.70215
disc_loss: 0.26121154
disc_acc: 0.9930693069306931

Validation results:
gen_loss: 156.3569
disc_loss: 6.2135777
disc_acc: 0.9315476190476191


	Epoch 356
Training results:
gen_loss: 148.77791
disc_loss: 0.104786165
disc_acc: 0.9971534653465347

Validation results:
gen_loss: 150.03732
disc_loss: 6.896981
disc_acc: 0.9146825396825397


	Epoch 357
Training results:
gen_loss: 143.37294
disc_loss: 0.23989339
disc_acc: 0.9935643564356436

Validation results:
gen_loss: 135.41139
disc_loss: 6.750686
disc_acc: 0.9171626984126984


	Epoch 358
Training results:
gen_loss: 147.64542
disc_loss: 0.26587144
disc_acc: 0.9941831683168317

Validation results:
gen_loss: 146.94913
disc_loss: 6.0892677
disc_acc: 0.9325396825396826


	Epoch 359
Training results:
gen_loss: 146.92862
disc_loss: 0.37075925
disc_acc: 0.991460396039604

Validation results:
gen_loss: 139.36697
disc_loss: 7.0807405
disc_acc: 0.9171626984126984


	Epoch 360
Training results:
gen_loss: 148.16473
disc_loss: 0.2013707
disc_acc: 0.994430693069307

Validation results:
gen_loss: 158.32127
disc_loss: 5.9704256
disc_acc: 0.9300595238095238


	Epoch 361
Training results:
gen_loss: 160.3146
disc_loss: 0.16066456
disc_acc: 0.995420792079208

Validation results:
gen_loss: 158.43095
disc_loss: 9.877379
disc_acc: 0.9007936507936508


	Epoch 362
Training results:
gen_loss: 155.4863
disc_loss: 0.31318855
disc_acc: 0.9907178217821783

Validation results:
gen_loss: 154.7606
disc_loss: 6.747122
disc_acc: 0.9280753968253969


	Epoch 363
Training results:
gen_loss: 160.07394
disc_loss: 0.21097331
disc_acc: 0.9938118811881188

Validation results:
gen_loss: 142.89682
disc_loss: 9.741617
disc_acc: 0.8878968253968254


	Epoch 364
Training results:
gen_loss: 155.66693
disc_loss: 0.27545837
disc_acc: 0.9929455445544555

Validation results:
gen_loss: 166.4249
disc_loss: 7.7225604
disc_acc: 0.9146825396825397


	Epoch 365
Training results:
gen_loss: 154.87137
disc_loss: 0.23784119
disc_acc: 0.9945544554455445

Validation results:
gen_loss: 153.227
disc_loss: 7.4883113
disc_acc: 0.9241071428571429


	Epoch 366
Training results:
gen_loss: 155.14226
disc_loss: 0.12191895
disc_acc: 0.9961633663366337

Validation results:
gen_loss: 151.01927
disc_loss: 7.3556685
disc_acc: 0.9156746031746031


	Epoch 367
Training results:
gen_loss: 157.95474
disc_loss: 0.23767546
disc_acc: 0.9925742574257426

Validation results:
gen_loss: 144.03209
disc_loss: 10.570458
disc_acc: 0.8849206349206349


	Epoch 368
Training results:
gen_loss: 164.75587
disc_loss: 0.39130902
disc_acc: 0.9900990099009901

Validation results:
gen_loss: 166.59456
disc_loss: 7.693676
disc_acc: 0.9176587301587301


	Epoch 369
Training results:
gen_loss: 162.58592
disc_loss: 0.16195925
disc_acc: 0.9962871287128713

Validation results:
gen_loss: 161.46414
disc_loss: 7.823699
disc_acc: 0.9186507936507936


	Epoch 370
Training results:
gen_loss: 157.60312
disc_loss: 0.20002317
disc_acc: 0.9952970297029703

Validation results:
gen_loss: 161.88115
disc_loss: 7.983587
disc_acc: 0.904265873015873


	Epoch 371
Training results:
gen_loss: 152.98663
disc_loss: 0.32297418
disc_acc: 0.9929455445544555

Validation results:
gen_loss: 172.67337
disc_loss: 8.713321
disc_acc: 0.9131944444444444


	Epoch 372
Training results:
gen_loss: 165.84471
disc_loss: 0.29434
disc_acc: 0.9929455445544555

Validation results:
gen_loss: 151.84528
disc_loss: 8.989606
disc_acc: 0.8943452380952381


	Epoch 373
Training results:
gen_loss: 159.8702
disc_loss: 0.3354969
disc_acc: 0.991460396039604

Validation results:
gen_loss: 164.112
disc_loss: 9.241663
disc_acc: 0.910218253968254


	Epoch 374
Training results:
gen_loss: 158.62126
disc_loss: 0.23812631
disc_acc: 0.9936881188118812

Validation results:
gen_loss: 151.80762
disc_loss: 7.589858
disc_acc: 0.9226190476190477


	Epoch 375
Training results:
gen_loss: 159.06895
disc_loss: 0.27647606
disc_acc: 0.9936881188118812

Validation results:
gen_loss: 161.82806
disc_loss: 7.5420785
disc_acc: 0.9181547619047619


	Epoch 376
Training results:
gen_loss: 163.25095
disc_loss: 0.17654687
disc_acc: 0.994430693069307

Validation results:
gen_loss: 163.50127
disc_loss: 8.469137
disc_acc: 0.9265873015873016


	Epoch 377
Training results:
gen_loss: 163.04868
disc_loss: 0.19096895
disc_acc: 0.9948019801980198

Validation results:
gen_loss: 172.54526
disc_loss: 9.619211
disc_acc: 0.9151785714285714


	Epoch 378
Training results:
gen_loss: 167.71458
disc_loss: 0.21542935
disc_acc: 0.9948019801980198

Validation results:
gen_loss: 170.07674
disc_loss: 7.473199
disc_acc: 0.9166666666666666


	Epoch 379
Training results:
gen_loss: 170.79477
disc_loss: 0.42007542
disc_acc: 0.9897277227722773

Validation results:
gen_loss: 170.94437
disc_loss: 10.060779
disc_acc: 0.9092261904761905


	Epoch 380
Training results:
gen_loss: 170.12236
disc_loss: 0.20676947
disc_acc: 0.9949257425742575

Validation results:
gen_loss: 182.87946
disc_loss: 8.030053
disc_acc: 0.9201388888888888


	Epoch 381
Training results:
gen_loss: 169.45891
disc_loss: 0.2678447
disc_acc: 0.9933168316831683

Validation results:
gen_loss: 162.79153
disc_loss: 7.3896093
disc_acc: 0.9156746031746031


	Epoch 382
Training results:
gen_loss: 167.88016
disc_loss: 0.16737363
disc_acc: 0.996039603960396

Validation results:
gen_loss: 166.38077
disc_loss: 11.143968
disc_acc: 0.9047619047619048


	Epoch 383
Training results:
gen_loss: 175.6818
disc_loss: 0.21635757
disc_acc: 0.9946782178217822

Validation results:
gen_loss: 188.68832
disc_loss: 8.600138
disc_acc: 0.9156746031746031


	Epoch 384
Training results:
gen_loss: 172.8915
disc_loss: 0.37875065
disc_acc: 0.9929455445544555

Validation results:
gen_loss: 165.6524
disc_loss: 6.721271
disc_acc: 0.9231150793650794


	Epoch 385
Training results:
gen_loss: 163.8112
disc_loss: 0.25749958
disc_acc: 0.9946782178217822

Validation results:
gen_loss: 164.73528
disc_loss: 7.642903
disc_acc: 0.9156746031746031


	Epoch 386
Training results:
gen_loss: 165.71776
disc_loss: 0.24143887
disc_acc: 0.9931930693069307

Validation results:
gen_loss: 167.28636
disc_loss: 9.840191
disc_acc: 0.9057539682539683


	Epoch 387
Training results:
gen_loss: 167.56352
disc_loss: 0.32065797
disc_acc: 0.9939356435643565

Validation results:
gen_loss: 163.33174
disc_loss: 8.0058975
disc_acc: 0.9176587301587301


	Epoch 388
Training results:
gen_loss: 168.2358
disc_loss: 0.15107906
disc_acc: 0.9959158415841585

Validation results:
gen_loss: 166.48619
disc_loss: 7.9675803
disc_acc: 0.9171626984126984


	Epoch 389
Training results:
gen_loss: 159.96011
disc_loss: 0.28300938
disc_acc: 0.9935643564356436

Validation results:
gen_loss: 165.01831
disc_loss: 7.5857806
disc_acc: 0.9260912698412699


	Epoch 390
Training results:
gen_loss: 164.24512
disc_loss: 0.257072
disc_acc: 0.994059405940594

Validation results:
gen_loss: 169.11197
disc_loss: 8.404517
disc_acc: 0.9260912698412699


	Epoch 391
Training results:
gen_loss: 163.66125
disc_loss: 0.3298611
disc_acc: 0.9917079207920793

Validation results:
gen_loss: 163.72974
disc_loss: 7.859863
disc_acc: 0.9250992063492064


	Epoch 392
Training results:
gen_loss: 168.3438
disc_loss: 0.18193413
disc_acc: 0.9956683168316832

Validation results:
gen_loss: 170.74109
disc_loss: 9.329836
disc_acc: 0.9265873015873016


	Epoch 393
Training results:
gen_loss: 170.74483
disc_loss: 0.18552606
disc_acc: 0.9951732673267327

Validation results:
gen_loss: 165.0322
disc_loss: 8.055905
disc_acc: 0.9171626984126984


	Epoch 394
Training results:
gen_loss: 174.65558
disc_loss: 0.23488703
disc_acc: 0.9945544554455445

Validation results:
gen_loss: 171.22656
disc_loss: 8.585104
disc_acc: 0.9280753968253969


	Epoch 395
Training results:
gen_loss: 180.98065
disc_loss: 0.5215743
disc_acc: 0.9902227722772278

Validation results:
gen_loss: 187.29071
disc_loss: 9.312755
disc_acc: 0.9241071428571429


	Epoch 396
Training results:
gen_loss: 181.29602
disc_loss: 0.13019457
disc_acc: 0.9969059405940595

Validation results:
gen_loss: 179.65895
disc_loss: 8.532584
disc_acc: 0.9236111111111112


	Epoch 397
Training results:
gen_loss: 186.66682
disc_loss: 0.20154838
disc_acc: 0.9946782178217822

Validation results:
gen_loss: 196.42198
disc_loss: 10.083679
disc_acc: 0.9126984126984127


	Epoch 398
Training results:
gen_loss: 185.35149
disc_loss: 0.35809964
disc_acc: 0.9936881188118812

Validation results:
gen_loss: 180.97986
disc_loss: 9.880305
disc_acc: 0.9146825396825397


	Epoch 399
Training results:
gen_loss: 182.38385
disc_loss: 0.29927236
disc_acc: 0.993440594059406

Validation results:
gen_loss: 183.8942
disc_loss: 8.992527
disc_acc: 0.9146825396825397


	Epoch 400
Training results:
gen_loss: 178.60017
disc_loss: 0.18047844
disc_acc: 0.9962871287128713

Validation results:
gen_loss: 176.69539
disc_loss: 9.400182
disc_acc: 0.9097222222222222


	Epoch 401
Training results:
gen_loss: 180.07532
disc_loss: 0.3478386
disc_acc: 0.9935643564356436

Validation results:
gen_loss: 183.33841
disc_loss: 6.9164066
disc_acc: 0.9345238095238095


	Epoch 402
Training results:
gen_loss: 188.1343
disc_loss: 0.17569983
disc_acc: 0.994059405940594

Validation results:
gen_loss: 188.34607
disc_loss: 8.650134
disc_acc: 0.9236111111111112


	Epoch 403
Training results:
gen_loss: 187.08246
disc_loss: 0.23963931
disc_acc: 0.9949257425742575

Validation results:
gen_loss: 187.41556
disc_loss: 7.860915
disc_acc: 0.9345238095238095


	Epoch 404
Training results:
gen_loss: 183.85027
disc_loss: 0.18053672
disc_acc: 0.995049504950495

Validation results:
gen_loss: 174.54057
disc_loss: 9.793296
disc_acc: 0.9151785714285714


	Epoch 405
Training results:
gen_loss: 182.04553
disc_loss: 0.38488907
disc_acc: 0.9931930693069307

Validation results:
gen_loss: 180.21236
disc_loss: 8.6478
disc_acc: 0.9285714285714286


	Epoch 406
Training results:
gen_loss: 181.17287
disc_loss: 0.36854947
disc_acc: 0.9925742574257426

Validation results:
gen_loss: 172.30319
disc_loss: 7.289265
disc_acc: 0.9246031746031746


	Epoch 407
Training results:
gen_loss: 182.16582
disc_loss: 0.17607008
disc_acc: 0.9952970297029703

Validation results:
gen_loss: 191.10118
disc_loss: 8.460469
disc_acc: 0.9300595238095238


	Epoch 408
Training results:
gen_loss: 185.97289
disc_loss: 0.21214552
disc_acc: 0.995049504950495

Validation results:
gen_loss: 171.9708
disc_loss: 10.093581
disc_acc: 0.9136904761904762


	Epoch 409
Training results:
gen_loss: 188.23892
disc_loss: 0.19618866
disc_acc: 0.995420792079208

Validation results:
gen_loss: 197.06277
disc_loss: 11.898513
disc_acc: 0.90625


	Epoch 410
Training results:
gen_loss: 180.9286
disc_loss: 0.3900287
disc_acc: 0.9912128712871288

Validation results:
gen_loss: 167.86043
disc_loss: 8.253452
disc_acc: 0.9131944444444444


	Epoch 411
Training results:
gen_loss: 187.53552
disc_loss: 0.36747825
disc_acc: 0.9923267326732673

Validation results:
gen_loss: 182.7451
disc_loss: 8.788581
disc_acc: 0.9275793650793651


	Epoch 412
Training results:
gen_loss: 193.28291
disc_loss: 0.11077625
disc_acc: 0.9966584158415842

Validation results:
gen_loss: 201.04228
disc_loss: 11.2082
disc_acc: 0.908234126984127


	Epoch 413
Training results:
gen_loss: 201.5224
disc_loss: 0.37739354
disc_acc: 0.992450495049505

Validation results:
gen_loss: 212.35844
disc_loss: 10.802805
disc_acc: 0.9186507936507936


	Epoch 414
Training results:
gen_loss: 208.6807
disc_loss: 0.12242228
disc_acc: 0.9967821782178218

Validation results:
gen_loss: 199.3296
disc_loss: 9.034604
disc_acc: 0.9231150793650794


	Epoch 415
Training results:
gen_loss: 202.63
disc_loss: 0.24415924
disc_acc: 0.9935643564356436

Validation results:
gen_loss: 207.65433
disc_loss: 9.73249
disc_acc: 0.9265873015873016


	Epoch 416
Training results:
gen_loss: 204.3361
disc_loss: 0.30828384
disc_acc: 0.9946782178217822

Validation results:
gen_loss: 197.23787
disc_loss: 9.787774
disc_acc: 0.9211309523809523


	Epoch 417
Training results:
gen_loss: 192.96582
disc_loss: 0.14582926
disc_acc: 0.9965346534653465

Validation results:
gen_loss: 186.0779
disc_loss: 10.748146
disc_acc: 0.9072420634920635


	Epoch 418
Training results:
gen_loss: 193.46269
disc_loss: 0.31259698
disc_acc: 0.9922029702970298

Validation results:
gen_loss: 204.05956
disc_loss: 12.369763
disc_acc: 0.90625


	Epoch 419
Training results:
gen_loss: 192.84697
disc_loss: 0.28241226
disc_acc: 0.9936881188118812

Validation results:
gen_loss: 199.39479
disc_loss: 7.9374614
disc_acc: 0.9260912698412699


	Epoch 420
Training results:
gen_loss: 201.96109
disc_loss: 0.22218682
disc_acc: 0.9962871287128713

Validation results:
gen_loss: 193.30879
disc_loss: 8.300827
disc_acc: 0.9295634920634921


	Epoch 421
Training results:
gen_loss: 196.39096
disc_loss: 0.16687706
disc_acc: 0.9967821782178218

Validation results:
gen_loss: 198.94032
disc_loss: 10.342098
disc_acc: 0.9191468253968254


	Epoch 422
Training results:
gen_loss: 196.5262
disc_loss: 0.35387778
disc_acc: 0.9920792079207921

Validation results:
gen_loss: 192.89365
disc_loss: 10.759742
disc_acc: 0.9156746031746031


	Epoch 423
Training results:
gen_loss: 198.11134
disc_loss: 0.2834817
disc_acc: 0.9939356435643565

Validation results:
gen_loss: 202.13815
disc_loss: 12.310276
disc_acc: 0.9092261904761905


	Epoch 424
Training results:
gen_loss: 197.72733
disc_loss: 0.29483357
disc_acc: 0.994059405940594

Validation results:
gen_loss: 200.00977
disc_loss: 10.413574
disc_acc: 0.9221230158730159


	Epoch 425
Training results:
gen_loss: 201.13533
disc_loss: 0.09132109
disc_acc: 0.9982673267326733

Validation results:
gen_loss: 209.53131
disc_loss: 9.5609865
disc_acc: 0.9181547619047619


	Epoch 426
Training results:
gen_loss: 199.47069
disc_loss: 0.39148942
disc_acc: 0.9936881188118812

Validation results:
gen_loss: 198.40257
disc_loss: 11.863828
disc_acc: 0.9107142857142857


	Epoch 427
Training results:
gen_loss: 201.25139
disc_loss: 0.47427154
disc_acc: 0.9912128712871288

Validation results:
gen_loss: 224.02737
disc_loss: 15.555205
disc_acc: 0.8923611111111112


	Epoch 428
Training results:
gen_loss: 206.42505
disc_loss: 0.2925139
disc_acc: 0.9938118811881188

Validation results:
gen_loss: 208.70735
disc_loss: 11.438952
disc_acc: 0.9141865079365079


	Epoch 429
Training results:
gen_loss: 202.20761
disc_loss: 0.187647
disc_acc: 0.996410891089109

Validation results:
gen_loss: 193.04218
disc_loss: 8.409741
disc_acc: 0.9246031746031746


	Epoch 430
Training results:
gen_loss: 197.6926
disc_loss: 0.16972354
disc_acc: 0.9976485148514852

Validation results:
gen_loss: 189.175
disc_loss: 7.4164667
disc_acc: 0.9340277777777778


	Epoch 431
Training results:
gen_loss: 195.71881
disc_loss: 0.20536655
disc_acc: 0.9969059405940595

Validation results:
gen_loss: 189.7177
disc_loss: 10.156127
disc_acc: 0.9122023809523809


	Epoch 432
Training results:
gen_loss: 198.51564
disc_loss: 0.5168
disc_acc: 0.9917079207920793

Validation results:
gen_loss: 218.49367
disc_loss: 14.985262
disc_acc: 0.8958333333333334


	Epoch 433
Training results:
gen_loss: 204.34227
disc_loss: 0.39375576
disc_acc: 0.9926980198019802

Validation results:
gen_loss: 200.46034
disc_loss: 9.74642
disc_acc: 0.9241071428571429


	Epoch 434
Training results:
gen_loss: 203.90353
disc_loss: 0.08551882
disc_acc: 0.9972772277227723

Validation results:
gen_loss: 213.12466
disc_loss: 8.4922285
disc_acc: 0.9265873015873016


	Epoch 435
Training results:
gen_loss: 214.81726
disc_loss: 0.18630394
disc_acc: 0.9959158415841585

Validation results:
gen_loss: 208.7233
disc_loss: 9.960098
disc_acc: 0.9181547619047619


	Epoch 436
Training results:
gen_loss: 215.42964
disc_loss: 0.34245116
disc_acc: 0.9935643564356436

Validation results:
gen_loss: 214.44234
disc_loss: 12.773959
disc_acc: 0.9047619047619048


	Epoch 437
Training results:
gen_loss: 210.47754
disc_loss: 0.38593018
disc_acc: 0.9935643564356436

Validation results:
gen_loss: 210.8774
disc_loss: 12.4286
disc_acc: 0.902281746031746


	Epoch 438
Training results:
gen_loss: 208.68951
disc_loss: 0.44449422
disc_acc: 0.9933168316831683

Validation results:
gen_loss: 215.58333
disc_loss: 9.304929
disc_acc: 0.9275793650793651


	Epoch 439
Training results:
gen_loss: 212.44864
disc_loss: 0.15887341
disc_acc: 0.9967821782178218

Validation results:
gen_loss: 226.04242
disc_loss: 10.276235
disc_acc: 0.9290674603174603


	Epoch 440
Training results:
gen_loss: 215.73206
disc_loss: 0.35972464
disc_acc: 0.9930693069306931

Validation results:
gen_loss: 212.12181
disc_loss: 8.145298
disc_acc: 0.9345238095238095


	Epoch 441
Training results:
gen_loss: 214.72324
disc_loss: 0.11303389
disc_acc: 0.9972772277227723

Validation results:
gen_loss: 220.76176
disc_loss: 8.35358
disc_acc: 0.9305555555555556


	Epoch 442
Training results:
gen_loss: 216.72624
disc_loss: 0.25633538
disc_acc: 0.9952970297029703

Validation results:
gen_loss: 227.96718
disc_loss: 11.053792
disc_acc: 0.9181547619047619


	Epoch 443
Training results:
gen_loss: 217.70866
disc_loss: 0.46950385
disc_acc: 0.9925742574257426

Validation results:
gen_loss: 222.8187
disc_loss: 12.252936
disc_acc: 0.9126984126984127


	Epoch 444
Training results:
gen_loss: 223.32909
disc_loss: 0.24904364
disc_acc: 0.996039603960396

Validation results:
gen_loss: 237.1437
disc_loss: 14.767834
disc_acc: 0.910218253968254


	Epoch 445
Training results:
gen_loss: 220.13516
disc_loss: 0.15093601
disc_acc: 0.9961633663366337

Validation results:
gen_loss: 220.46675
disc_loss: 10.394206
disc_acc: 0.9176587301587301


	Epoch 446
Training results:
gen_loss: 203.78714
disc_loss: 0.3173987
disc_acc: 0.994430693069307

Validation results:
gen_loss: 208.09076
disc_loss: 8.686387
disc_acc: 0.9370039682539683


	Epoch 447
Training results:
gen_loss: 215.9717
disc_loss: 0.23964708
disc_acc: 0.9957920792079208

Validation results:
gen_loss: 214.17284
disc_loss: 9.393904
disc_acc: 0.9290674603174603


	Epoch 448
Training results:
gen_loss: 214.79776
disc_loss: 0.2213653
disc_acc: 0.996039603960396

Validation results:
gen_loss: 207.42566
disc_loss: 10.935731
disc_acc: 0.9161706349206349


	Epoch 449
Training results:
gen_loss: 216.84673
disc_loss: 0.31440103
disc_acc: 0.9943069306930693

Validation results:
gen_loss: 219.70413
disc_loss: 8.29371
disc_acc: 0.9389880952380952


	Epoch 450
Training results:
gen_loss: 223.5465
disc_loss: 0.33994925
disc_acc: 0.9935643564356436

Validation results:
gen_loss: 234.26971
disc_loss: 15.85042
disc_acc: 0.9037698412698413


	Epoch 451
Training results:
gen_loss: 215.98839
disc_loss: 0.2712448
disc_acc: 0.9943069306930693

Validation results:
gen_loss: 217.90686
disc_loss: 9.703153
disc_acc: 0.9275793650793651


	Epoch 452
Training results:
gen_loss: 218.48946
disc_loss: 0.17312367
disc_acc: 0.9957920792079208

Validation results:
gen_loss: 218.58626
disc_loss: 7.2662115
disc_acc: 0.9345238095238095


	Epoch 453
Training results:
gen_loss: 221.56987
disc_loss: 0.373691
disc_acc: 0.9939356435643565

Validation results:
gen_loss: 234.15877
disc_loss: 12.780338
disc_acc: 0.9206349206349206


	Epoch 454
Training results:
gen_loss: 227.69328
disc_loss: 0.25005528
disc_acc: 0.995049504950495

Validation results:
gen_loss: 214.51988
disc_loss: 9.41216
disc_acc: 0.9280753968253969


	Epoch 455
Training results:
gen_loss: 227.56613
disc_loss: 0.21910831
disc_acc: 0.9959158415841585

Validation results:
gen_loss: 228.8646
disc_loss: 9.557358
disc_acc: 0.9260912698412699


	Epoch 456
Training results:
gen_loss: 222.09361
disc_loss: 0.23688574
disc_acc: 0.9946782178217822

Validation results:
gen_loss: 221.57347
disc_loss: 10.123588
disc_acc: 0.9255952380952381


	Epoch 457
Training results:
gen_loss: 220.07263
disc_loss: 0.32012045
disc_acc: 0.994059405940594

Validation results:
gen_loss: 223.6187
disc_loss: 8.933081
disc_acc: 0.9325396825396826


	Epoch 458
Training results:
gen_loss: 222.39803
disc_loss: 0.27211574
disc_acc: 0.9951732673267327

Validation results:
gen_loss: 241.13913
disc_loss: 16.935097
disc_acc: 0.8948412698412699


	Epoch 459
Training results:
gen_loss: 216.97131
disc_loss: 0.3806887
disc_acc: 0.9936881188118812

Validation results:
gen_loss: 211.95685
disc_loss: 9.3073
disc_acc: 0.9255952380952381


	Epoch 460
Training results:
gen_loss: 221.53645
disc_loss: 0.17028081
disc_acc: 0.9967821782178218

Validation results:
gen_loss: 223.068
disc_loss: 14.681076
disc_acc: 0.9191468253968254


	Epoch 461
Training results:
gen_loss: 223.56346
disc_loss: 0.25234425
disc_acc: 0.9957920792079208

Validation results:
gen_loss: 229.04329
disc_loss: 9.56047
disc_acc: 0.9305555555555556


	Epoch 462
Training results:
gen_loss: 234.29768
disc_loss: 0.5616955
disc_acc: 0.9915841584158416

Validation results:
gen_loss: 236.97414
disc_loss: 12.93751
disc_acc: 0.9236111111111112


	Epoch 463
Training results:
gen_loss: 239.17819
disc_loss: 0.30644557
disc_acc: 0.9955445544554455

Validation results:
gen_loss: 228.3941
disc_loss: 10.059008
disc_acc: 0.9236111111111112


	Epoch 464
Training results:
gen_loss: 231.37154
disc_loss: 0.26944688
disc_acc: 0.9966584158415842

Validation results:
gen_loss: 218.72151
disc_loss: 8.091336
disc_acc: 0.9384920634920635


	Epoch 465
Training results:
gen_loss: 238.59721
disc_loss: 0.18969294
disc_acc: 0.9961633663366337

Validation results:
gen_loss: 248.38257
disc_loss: 10.589004
disc_acc: 0.9285714285714286


	Epoch 466
Training results:
gen_loss: 238.99866
disc_loss: 0.277205
disc_acc: 0.995420792079208

Validation results:
gen_loss: 242.21318
disc_loss: 11.83263
disc_acc: 0.9141865079365079


	Epoch 467
Training results:
gen_loss: 238.74533
disc_loss: 0.3126149
disc_acc: 0.994430693069307

Validation results:
gen_loss: 241.04782
disc_loss: 9.074482
disc_acc: 0.9325396825396826


	Epoch 468
Training results:
gen_loss: 236.75993
disc_loss: 0.118077904
disc_acc: 0.9967821782178218

Validation results:
gen_loss: 249.15073
disc_loss: 18.358414
disc_acc: 0.90625


	Epoch 469
Training results:
gen_loss: 240.9816
disc_loss: 0.3404967
disc_acc: 0.994430693069307

Validation results:
gen_loss: 254.41972
disc_loss: 11.900548
disc_acc: 0.9141865079365079


	Epoch 470
Training results:
gen_loss: 245.60313
disc_loss: 0.49014115
disc_acc: 0.9922029702970298

Validation results:
gen_loss: 234.4034
disc_loss: 10.880144
disc_acc: 0.9176587301587301


	Epoch 471
Training results:
gen_loss: 240.91751
disc_loss: 0.18856274
disc_acc: 0.995420792079208

Validation results:
gen_loss: 239.36044
disc_loss: 11.689816
disc_acc: 0.9226190476190477


	Epoch 472
Training results:
gen_loss: 234.38078
disc_loss: 0.14941183
disc_acc: 0.9969059405940595

Validation results:
gen_loss: 238.83502
disc_loss: 15.613918
disc_acc: 0.9032738095238095


	Epoch 473
Training results:
gen_loss: 240.44995
disc_loss: 0.44439745
disc_acc: 0.992450495049505

Validation results:
gen_loss: 227.81688
disc_loss: 13.126496
disc_acc: 0.9136904761904762


	Epoch 474
Training results:
gen_loss: 244.47218
disc_loss: 0.16766155
disc_acc: 0.996410891089109

Validation results:
gen_loss: 245.40007
disc_loss: 11.739843
disc_acc: 0.9255952380952381


	Epoch 475
Training results:
gen_loss: 241.75992
disc_loss: 0.3281819
disc_acc: 0.9965346534653465

Validation results:
gen_loss: 230.93823
disc_loss: 11.698459
disc_acc: 0.9216269841269841


	Epoch 476
Training results:
gen_loss: 238.2778
disc_loss: 0.46792585
disc_acc: 0.9939356435643565

Validation results:
gen_loss: 244.09395
disc_loss: 11.981954
disc_acc: 0.9181547619047619


	Epoch 477
Training results:
gen_loss: 236.63828
disc_loss: 0.2700804
disc_acc: 0.9959158415841585

Validation results:
gen_loss: 223.37798
disc_loss: 11.189849
disc_acc: 0.9166666666666666


	Epoch 478
Training results:
gen_loss: 230.25842
disc_loss: 0.1407855
disc_acc: 0.9965346534653465

Validation results:
gen_loss: 261.5237
disc_loss: 17.83788
disc_acc: 0.8978174603174603


	Epoch 479
Training results:
gen_loss: 248.79468
disc_loss: 0.25307715
disc_acc: 0.9952970297029703

Validation results:
gen_loss: 257.2855
disc_loss: 10.988367
disc_acc: 0.9255952380952381


	Epoch 480
Training results:
gen_loss: 250.24687
disc_loss: 0.21039143
disc_acc: 0.9965346534653465

Validation results:
gen_loss: 242.38985
disc_loss: 10.537496
disc_acc: 0.9246031746031746


	Epoch 481
Training results:
gen_loss: 254.47481
disc_loss: 0.3354411
disc_acc: 0.9938118811881188

Validation results:
gen_loss: 266.57242
disc_loss: 11.635481
disc_acc: 0.9250992063492064


	Epoch 482
Training results:
gen_loss: 253.19565
disc_loss: 0.4404427
disc_acc: 0.9945544554455445

Validation results:
gen_loss: 246.00134
disc_loss: 9.757371
disc_acc: 0.9340277777777778


	Epoch 483
Training results:
gen_loss: 247.2827
disc_loss: 0.43217534
disc_acc: 0.9943069306930693

Validation results:
gen_loss: 233.43625
disc_loss: 9.31293
disc_acc: 0.9365079365079365


	Epoch 484
Training results:
gen_loss: 242.44342
disc_loss: 0.14998402
disc_acc: 0.996410891089109

Validation results:
gen_loss: 256.98923
disc_loss: 11.547264
disc_acc: 0.9166666666666666


	Epoch 485
Training results:
gen_loss: 242.95316
disc_loss: 0.22118065
disc_acc: 0.9956683168316832

Validation results:
gen_loss: 247.15182
disc_loss: 13.261576
disc_acc: 0.9216269841269841


	Epoch 486
Training results:
gen_loss: 244.61699
disc_loss: 0.3953589
disc_acc: 0.9943069306930693

Validation results:
gen_loss: 235.86789
disc_loss: 16.295397
disc_acc: 0.8993055555555556


	Epoch 487
Training results:
gen_loss: 246.66347
disc_loss: 0.28093952
disc_acc: 0.995420792079208

Validation results:
gen_loss: 248.12039
disc_loss: 14.576361
disc_acc: 0.9097222222222222


	Epoch 488
Training results:
gen_loss: 242.0967
disc_loss: 0.4222058
disc_acc: 0.9941831683168317

Validation results:
gen_loss: 235.80742
disc_loss: 10.73971
disc_acc: 0.9260912698412699


	Epoch 489
Training results:
gen_loss: 241.45282
disc_loss: 0.07812342
disc_acc: 0.9978960396039604

Validation results:
gen_loss: 242.10692
disc_loss: 11.951349
disc_acc: 0.9221230158730159


	Epoch 490
Training results:
gen_loss: 241.92308
disc_loss: 0.39613175
disc_acc: 0.9949257425742575

Validation results:
gen_loss: 244.6888
disc_loss: 11.044151
disc_acc: 0.9231150793650794


	Epoch 491
Training results:
gen_loss: 243.89496
disc_loss: 0.5117432
disc_acc: 0.9936881188118812

Validation results:
gen_loss: 228.0368
disc_loss: 15.425241
disc_acc: 0.8988095238095238


	Epoch 492
Training results:
gen_loss: 241.91086
disc_loss: 0.2684518
disc_acc: 0.995420792079208

Validation results:
gen_loss: 247.83794
disc_loss: 12.029635
disc_acc: 0.9226190476190477


	Epoch 493
Training results:
gen_loss: 244.5781
disc_loss: 0.32932335
disc_acc: 0.995420792079208

Validation results:
gen_loss: 230.92479
disc_loss: 12.043542
disc_acc: 0.9181547619047619


	Epoch 494
Training results:
gen_loss: 243.0828
disc_loss: 0.14178322
disc_acc: 0.9967821782178218

Validation results:
gen_loss: 233.1941
disc_loss: 10.500347
disc_acc: 0.9196428571428571


	Epoch 495
Training results:
gen_loss: 242.22304
disc_loss: 0.29681614
disc_acc: 0.9956683168316832

Validation results:
gen_loss: 243.02902
disc_loss: 11.258056
disc_acc: 0.9255952380952381


	Epoch 496
Training results:
gen_loss: 249.09259
disc_loss: 0.451425
disc_acc: 0.9926980198019802

Validation results:
gen_loss: 254.0253
disc_loss: 14.225451
disc_acc: 0.9146825396825397


	Epoch 497
Training results:
gen_loss: 251.58911
disc_loss: 0.38882893
disc_acc: 0.9943069306930693

Validation results:
gen_loss: 251.3054
disc_loss: 13.056415
disc_acc: 0.9236111111111112


	Epoch 498
Training results:
gen_loss: 257.9755
disc_loss: 0.39395013
disc_acc: 0.9952970297029703

Validation results:
gen_loss: 239.8144
disc_loss: 15.423847
disc_acc: 0.9201388888888888


	Epoch 499
Training results:
gen_loss: 244.71095
disc_loss: 0.20253399
disc_acc: 0.9959158415841585

Validation results:
gen_loss: 242.11234
disc_loss: 12.742605
disc_acc: 0.9236111111111112


	Epoch 500
Training results:
gen_loss: 249.34273
disc_loss: 0.22255185
disc_acc: 0.9956683168316832

Validation results:
gen_loss: 254.81708
disc_loss: 9.595782
disc_acc: 0.9345238095238095



gen_train_loss: 0.004578718, 2.5651238, 3.5892026, 4.7076006, 5.843518, 6.1834497, 6.272717, 7.5607276, 7.410952, 6.979266, 7.717928, 8.110597, 9.228245, 10.095384, 9.881261, 11.510379, 11.226344, 11.738682, 11.143848, 11.908234, 12.817752, 11.872915, 12.05871, 12.311412, 12.686986, 12.27307, 12.99272, 13.489879, 13.556956, 14.643663, 14.14284, 13.238492, 13.908578, 14.154787, 14.596844, 14.338892, 14.146249, 16.616966, 16.987904, 15.422368, 16.859474, 17.490332, 16.799658, 15.89764, 16.302618, 17.951763, 17.626558, 17.204819, 16.440832, 15.942344, 17.985064, 17.412888, 18.164778, 19.186796, 18.54165, 17.674782, 18.65085, 21.306843, 19.116848, 18.935822, 19.357975, 19.176222, 19.52319, 18.365185, 17.138529, 21.795008, 20.996029, 20.612656, 20.22754, 20.21915, 20.085922, 22.44509, 20.85006, 21.18055, 22.46395, 24.425003, 22.038939, 21.60666, 23.623863, 22.128323, 23.155777, 24.315485, 26.057554, 24.448832, 23.86689, 22.745321, 24.439257, 24.360386, 23.88806, 23.964376, 26.72589, 26.148336, 26.561726, 26.123165, 24.46243, 27.364079, 27.827713, 25.873726, 27.120472, 27.06861, 26.317057, 27.121426, 28.697887, 29.03082, 33.632977, 33.194168, 32.232933, 33.637066, 31.56563, 31.85164, 32.381134, 32.819138, 33.618927, 34.696583, 35.869404, 32.313644, 35.436558, 33.97291, 32.614315, 32.6155, 37.730568, 40.99229, 36.179363, 36.546898, 37.52475, 39.006954, 39.43487, 39.994186, 39.65716, 42.394413, 40.73594, 37.811176, 38.297806, 39.592697, 38.65277, 40.07924, 41.303684, 43.389736, 42.923283, 42.931824, 43.779766, 46.430977, 43.93157, 41.70554, 43.636868, 41.303963, 41.31527, 47.45457, 45.31187, 43.43176, 44.037468, 44.82108, 44.413437, 44.825348, 50.711937, 49.117844, 49.973465, 50.419308, 49.673878, 46.600433, 47.627247, 48.587646, 49.13013, 57.96227, 56.65953, 57.21084, 55.721172, 52.909298, 57.69874, 60.907207, 58.82321, 59.094547, 55.96271, 54.675117, 55.203255, 57.13376, 57.468517, 58.442165, 57.0169, 56.82382, 60.172077, 58.09423, 58.466785, 60.337616, 66.38998, 68.02371, 63.704567, 65.550064, 64.432144, 64.941055, 64.53006, 62.925697, 65.67921, 66.77316, 64.28085, 64.81736, 66.26733, 68.804794, 71.37667, 74.56085, 66.70086, 69.11, 69.91295, 72.13482, 74.84406, 70.4685, 70.971565, 69.113495, 70.670525, 70.048035, 68.394, 68.61558, 70.29433, 75.04748, 73.19864, 76.32879, 72.42203, 72.85906, 80.34652, 78.271416, 74.03639, 81.10518, 77.833435, 79.35743, 82.573715, 77.8888, 81.62551, 80.096985, 81.593376, 85.79515, 85.289055, 84.340996, 83.07376, 87.37652, 84.20302, 81.598915, 86.17848, 84.142685, 74.59823, 81.87814, 90.63888, 83.19039, 87.54268, 88.862114, 96.79949, 94.06875, 91.663864, 93.10928, 93.67408, 95.00633, 91.68729, 93.815025, 89.85908, 92.37194, 101.48547, 95.593376, 91.18767, 99.05228, 94.15285, 98.45373, 95.205696, 94.56795, 99.10206, 95.95259, 104.39177, 104.19074, 100.388565, 100.15136, 95.073235, 98.803215, 101.00128, 99.81611, 98.341255, 103.11493, 106.22604, 103.34065, 112.379654, 108.96934, 108.878136, 102.6513, 102.30419, 107.730804, 116.684616, 115.067375, 116.42619, 119.72319, 116.12265, 115.44225, 110.2082, 122.574, 113.00065, 118.08654, 125.17031, 124.11339, 123.83681, 124.88763, 124.17175, 115.02591, 116.80981, 114.522964, 135.91284, 130.03523, 123.52836, 131.1651, 129.74747, 119.823616, 128.26323, 134.83174, 131.38689, 125.654724, 133.68289, 148.0733, 141.76146, 150.83041, 142.42741, 132.88768, 139.10739, 131.42754, 135.25789, 131.54988, 129.21239, 130.30121, 142.70018, 133.56186, 128.45316, 132.4262, 150.5451, 156.28168, 149.33818, 148.89195, 154.46184, 153.15929, 158.93344, 147.54062, 134.26872, 145.59483, 144.76567, 152.00513, 159.63556, 156.15228, 158.40028, 156.8113, 158.6518, 155.51488, 152.27257, 145.6526, 158.76779, 161.33202, 172.63673, 163.4457, 168.19868, 169.72466, 164.6142, 166.82265, 164.4927, 150.40918, 166.36983, 166.17361, 161.22227, 158.6283, 163.58289, 158.57831, 169.96576, 173.25894, 173.13394, 171.47655, 162.90727, 183.50508, 175.0771, 185.481, 179.38412, 179.46712, 182.17479, 192.06773, 187.50848, 178.84915, 178.08998, 177.48172, 178.59549, 187.55739, 189.63815, 181.6248, 182.9785, 192.69656, 187.69656, 191.10696, 202.23155, 199.11308, 184.53394, 187.83018, 185.5621, 180.47705, 189.59677, 200.4388, 198.1332, 192.4705, 196.59805, 220.26216, 222.72472, 215.59737, 207.58347, 214.68822, 195.16489, 204.6576, 223.7959, 207.47528, 213.91565, 222.93307, 230.00099, 206.29196, 212.90266, 195.93816, 191.5318, 198.4746, 203.01466, 206.60637, 221.64542, 213.2525, 211.28835, 211.90244, 209.24214, 204.14189, 216.52565, 229.72375, 221.32823, 211.42227, 213.39354, 212.68593, 219.72955, 234.61954, 228.02168, 220.59703, 218.29596, 235.18103, 229.36375, 230.68683, 219.1682, 202.6448, 211.03113, 215.0751, 219.30338, 209.12317, 214.33725, 242.67387, 243.65303, 237.10884, 238.81738, 235.39877, 230.46008, 251.87587, 243.18733, 241.91814, 254.74852, 228.50797, 255.13426, 249.33672, 246.43822, 246.11984, 254.05067, 259.58264, 245.82289, 254.87134, 241.95158, 255.87717, 257.42966, 250.63094, 266.807, 269.9129, 283.28775, 278.92883, 275.56146, 270.42435, 255.12492, 264.5277, 254.94037, 288.08417, 296.40863, 262.41476, 261.93933, 268.49777, 272.5156, 262.51193, 264.79474, 262.03177, 273.12198, 277.48718, 274.17798, 282.80145, 284.9733, 260.19266, 260.66376, 264.96826, 282.66818, 285.5792, 295.13187, 280.35455, 282.6828, 277.14093, 272.9331, 276.36243, 277.45642, 0.0038516254, 3.4955795, 5.233882, 6.7768364, 7.726985, 8.519198, 8.937818, 10.472983, 10.203908, 10.926107, 11.152895, 11.021084, 11.607669, 11.571455, 12.335113, 12.411972, 12.327713, 12.8865795, 12.80415, 12.9035635, 12.862099, 13.127277, 13.912535, 13.885501, 14.7916765, 14.760675, 14.796593, 15.026253, 14.466298, 14.481441, 15.544963, 15.619453, 14.939503, 15.310648, 14.673719, 15.634398, 15.885609, 16.567368, 15.959015, 16.211695, 17.116129, 17.214148, 17.740582, 17.761768, 17.674238, 16.607765, 17.770988, 17.662163, 19.04044, 18.970676, 20.340237, 19.13148, 17.505276, 19.286066, 19.14756, 17.921623, 18.213425, 19.075909, 18.723879, 21.235262, 22.816624, 20.98987, 21.320482, 21.42833, 21.120989, 20.287476, 19.825853, 20.601362, 21.843534, 20.678682, 20.955988, 23.180313, 23.05021, 24.123875, 22.66629, 23.770058, 24.53617, 24.358793, 23.71453, 24.538097, 23.655403, 23.995728, 26.763113, 26.25521, 25.596786, 24.664967, 24.942724, 25.780792, 27.007195, 27.348417, 26.797218, 27.666016, 28.800493, 28.378407, 26.742369, 26.59784, 30.549593, 35.286648, 34.39313, 32.586586, 29.937593, 28.21052, 27.567984, 30.566816, 32.274025, 32.45449, 29.954208, 31.44292, 33.711525, 33.806824, 32.76797, 30.657194, 34.89381, 33.26971, 31.630198, 32.676006, 33.716476, 31.830809, 31.475136, 35.308933, 33.791954, 33.31767, 37.49087, 35.240875, 36.071774, 35.85259, 35.399815, 36.302338, 35.244244, 36.19847, 34.73549, 40.390057, 39.668346, 39.78075, 38.31514, 35.82039, 37.05625, 38.586872, 39.90236, 39.27377, 40.55426, 40.568943, 38.8913, 36.608353, 39.603004, 39.28147, 39.651802, 41.094738, 40.350216, 42.452644, 44.130684, 43.47411, 43.780815, 45.82267, 45.104485, 45.274952, 46.69115, 44.615704, 43.674522, 42.032696, 43.16127, 48.81165, 49.339596, 48.48442, 47.488945, 50.018204, 51.734608, 52.326054, 49.618633, 48.82594, 48.50402, 52.58818, 54.07005, 51.480415, 49.132614, 50.209877, 53.885532, 65.327675, 58.52488, 55.46734, 55.756668, 53.92453, 53.384377, 54.511086, 57.77265, 53.283787, 51.60242, 57.36556, 57.94096, 57.394802, 59.36076, 58.711464, 56.67951, 57.55968, 63.684147, 64.71288, 62.596447, 61.606335, 65.9577, 62.196133, 61.736076, 65.14824, 66.46352, 64.95706, 63.22101, 63.042477, 65.59634, 67.50974, 66.335464, 66.66238, 66.62562, 66.65098, 69.209564, 68.12256, 70.204124, 70.70234, 69.93292, 72.520226, 70.75742, 68.59804, 73.791725, 72.235725, 77.9975, 71.54493, 73.72856, 74.81603, 74.27835, 72.07464, 79.139496, 79.309654, 79.958496, 82.885704, 76.9121, 76.66192, 81.25133, 79.81717, 78.56063, 80.78436, 84.52072, 82.79007, 81.93571, 77.31065, 75.28234, 82.55491, 83.97717, 86.88496, 82.37642, 82.05633, 84.433014, 87.93354, 88.23328, 87.36346, 87.97898, 85.763084, 91.43138, 89.740326, 87.94275, 86.89729, 86.81268, 95.75362, 94.90135, 88.36303, 87.450035, 88.23913, 96.83402, 105.588165, 103.191444, 99.388954, 98.56149, 99.64725, 96.01924, 104.38322, 99.1952, 93.73111, 91.33125, 96.70947, 97.070946, 98.209435, 101.77905, 106.85474, 105.66227, 103.24666, 98.152534, 98.47652, 102.03616, 100.91167, 104.20347, 107.16105, 106.844536, 98.970314, 99.57076, 100.21658, 106.37041, 115.91011, 107.63053, 107.929985, 107.30942, 114.98422, 114.088554, 111.79081, 116.115776, 112.82982, 113.555084, 123.69337, 118.948265, 115.11502, 117.4257, 123.94703, 122.54193, 119.170906, 115.08986, 114.61088, 109.09601, 114.144196, 112.15934, 121.13665, 121.0365, 119.03502, 120.14249, 123.9843, 120.77547, 124.12116, 120.66435, 125.39412, 123.277306, 130.3029, 128.29073, 131.86609, 141.37021, 140.06184, 132.48862, 137.09459, 133.95866, 140.85928, 140.19035, 138.19987, 128.9476, 135.22977, 133.64072, 131.00488, 132.51648, 135.26414, 136.08936, 141.08199, 133.59103, 131.12321, 137.67992, 138.75711, 136.13036, 139.2371, 138.14647, 144.37437, 140.76166, 156.79521, 156.13051, 154.70215, 148.77791, 143.37294, 147.64542, 146.92862, 148.16473, 160.3146, 155.4863, 160.07394, 155.66693, 154.87137, 155.14226, 157.95474, 164.75587, 162.58592, 157.60312, 152.98663, 165.84471, 159.8702, 158.62126, 159.06895, 163.25095, 163.04868, 167.71458, 170.79477, 170.12236, 169.45891, 167.88016, 175.6818, 172.8915, 163.8112, 165.71776, 167.56352, 168.2358, 159.96011, 164.24512, 163.66125, 168.3438, 170.74483, 174.65558, 180.98065, 181.29602, 186.66682, 185.35149, 182.38385, 178.60017, 180.07532, 188.1343, 187.08246, 183.85027, 182.04553, 181.17287, 182.16582, 185.97289, 188.23892, 180.9286, 187.53552, 193.28291, 201.5224, 208.6807, 202.63, 204.3361, 192.96582, 193.46269, 192.84697, 201.96109, 196.39096, 196.5262, 198.11134, 197.72733, 201.13533, 199.47069, 201.25139, 206.42505, 202.20761, 197.6926, 195.71881, 198.51564, 204.34227, 203.90353, 214.81726, 215.42964, 210.47754, 208.68951, 212.44864, 215.73206, 214.72324, 216.72624, 217.70866, 223.32909, 220.13516, 203.78714, 215.9717, 214.79776, 216.84673, 223.5465, 215.98839, 218.48946, 221.56987, 227.69328, 227.56613, 222.09361, 220.07263, 222.39803, 216.97131, 221.53645, 223.56346, 234.29768, 239.17819, 231.37154, 238.59721, 238.99866, 238.74533, 236.75993, 240.9816, 245.60313, 240.91751, 234.38078, 240.44995, 244.47218, 241.75992, 238.2778, 236.63828, 230.25842, 248.79468, 250.24687, 254.47481, 253.19565, 247.2827, 242.44342, 242.95316, 244.61699, 246.66347, 242.0967, 241.45282, 241.92308, 243.89496, 241.91086, 244.5781, 243.0828, 242.22304, 249.09259, 251.58911, 257.9755, 244.71095, 249.34273
disc_train_loss: 5.5712934, 2.1467245, 0.94879866, 0.7655655, 0.5810537, 0.55227727, 0.4836948, 0.43686226, 0.43814793, 0.3721705, 0.4028062, 0.33961436, 0.38082865, 0.35541853, 0.27477947, 0.3048587, 0.34341612, 0.25468165, 0.24350905, 0.2568004, 0.36082673, 0.21173073, 0.24576665, 0.20501523, 0.23693956, 0.21543144, 0.27984312, 0.19142492, 0.17332183, 0.3432081, 0.16098589, 0.13365684, 0.16776586, 0.17546238, 0.35388786, 0.122719735, 0.15481964, 0.276421, 0.18287328, 0.10213003, 0.15408702, 0.27720964, 0.12786835, 0.14018257, 0.15345559, 0.20691611, 0.17454913, 0.14753808, 0.1229962, 0.21000406, 0.1395294, 0.14250945, 0.22293189, 0.16329168, 0.1374026, 0.11087063, 0.12949887, 0.38278997, 0.117612995, 0.08018649, 0.08905206, 0.15528338, 0.16335273, 0.14870891, 0.099745825, 0.27527016, 0.1199277, 0.14613731, 0.15641226, 0.0941073, 0.15870506, 0.26488647, 0.1092701, 0.13384101, 0.10521779, 0.3425074, 0.06842329, 0.10252624, 0.18752223, 0.099785455, 0.150795, 0.15765013, 0.19932142, 0.16407223, 0.124254495, 0.06362894, 0.20489988, 0.12841845, 0.11079035, 0.15576686, 0.20100978, 0.10271662, 0.14193772, 0.10814766, 0.14897579, 0.20320603, 0.15469025, 0.09349855, 0.17513153, 0.20851417, 0.155739, 0.120935164, 0.07584647, 0.14748609, 0.32381055, 0.10373082, 0.088322654, 0.13887767, 0.15736507, 0.1310015, 0.14339884, 0.15346384, 0.30178586, 0.13112852, 0.08190545, 0.105248116, 0.3109813, 0.07662007, 0.065210074, 0.11568504, 0.28280112, 0.12918179, 0.1183164, 0.102749385, 0.18134452, 0.17626588, 0.12722051, 0.16090237, 0.21045789, 0.18857309, 0.08527855, 0.15841345, 0.112094074, 0.23474276, 0.11875836, 0.15586458, 0.14712358, 0.24121404, 0.11462865, 0.12059114, 0.20034815, 0.2143516, 0.13611628, 0.1277142, 0.17092822, 0.17456865, 0.12207262, 0.3321025, 0.14208916, 0.065450355, 0.16479157, 0.14422995, 0.14964361, 0.11244858, 0.37605265, 0.13776107, 0.09651178, 0.09380509, 0.18526739, 0.21583714, 0.10690557, 0.093928315, 0.18525729, 0.30237168, 0.10392551, 0.19534142, 0.08051419, 0.1539314, 0.32191852, 0.17713632, 0.11047892, 0.0854171, 0.15557344, 0.094083905, 0.22599366, 0.1747259, 0.2830666, 0.16896908, 0.060770553, 0.228162, 0.15768431, 0.22479506, 0.10612938, 0.24047577, 0.14129981, 0.1670205, 0.18262583, 0.17779554, 0.103911355, 0.24413647, 0.11875813, 0.20592892, 0.122338004, 0.20890194, 0.16060963, 0.27045986, 0.18455723, 0.1373424, 0.14072435, 0.23361991, 0.17310008, 0.17917922, 0.15742692, 0.38592896, 0.202259, 0.11184179, 0.23880155, 0.098632134, 0.15903725, 0.13566338, 0.2509919, 0.15333766, 0.14813894, 0.26539692, 0.199788, 0.2264987, 0.17745367, 0.034940407, 0.23347557, 0.31538135, 0.25219706, 0.17709927, 0.08178974, 0.27153772, 0.22543985, 0.12304145, 0.25593635, 0.13684474, 0.26382565, 0.12349579, 0.19037879, 0.2185197, 0.2052889, 0.14138073, 0.17749684, 0.21002066, 0.1909538, 0.15466577, 0.12713936, 0.18002413, 0.2806054, 0.17906655, 0.10935332, 0.3547432, 0.13659701, 0.21359044, 0.19592647, 0.21449591, 0.36852518, 0.17583755, 0.11171544, 0.16518018, 0.13216615, 0.15102226, 0.2804781, 0.20717412, 0.16213413, 0.2592153, 0.1482212, 0.26703128, 0.19875835, 0.1064684, 0.17076641, 0.28494477, 0.28002572, 0.1918003, 0.18499777, 0.24559896, 0.14749406, 0.23313111, 0.19219473, 0.21976969, 0.11434694, 0.3990224, 0.3518832, 0.15808001, 0.12858663, 0.18041581, 0.19031635, 0.21576521, 0.07388631, 0.15409023, 0.64601266, 0.2144941, 0.13878769, 0.19865096, 0.14859287, 0.13921034, 0.26019222, 0.27734894, 0.16815305, 0.18917191, 0.16651507, 0.4292161, 0.1399517, 0.23128507, 0.26088083, 0.2384328, 0.10624727, 0.15986675, 0.4076051, 0.2785793, 0.13098036, 0.1030106, 0.18557219, 0.28622347, 0.23263513, 0.24250923, 0.19747734, 0.20119081, 0.28353488, 0.33981878, 0.21873257, 0.19425392, 0.25088692, 0.3186635, 0.25024968, 0.22394457, 0.14491853, 0.28949344, 0.17465052, 0.17599067, 0.37841162, 0.24462609, 0.14948216, 0.2491921, 0.26338208, 0.15355143, 0.11162773, 0.39618802, 0.2663833, 0.21523446, 0.15372117, 0.15325531, 0.30641904, 0.27418715, 0.18699262, 0.44617638, 0.21245258, 0.12876303, 0.31954795, 0.4178163, 0.26571465, 0.117635384, 0.1749557, 0.08473944, 0.31616116, 0.6208124, 0.39501646, 0.20874101, 0.12236376, 0.29730526, 0.35031193, 0.12798578, 0.20197137, 0.1781503, 0.49764016, 0.1796766, 0.13718961, 0.41599578, 0.19997308, 0.21288712, 0.27901033, 0.25993243, 0.25526297, 0.1872234, 0.14980294, 0.37273523, 0.3107298, 0.24970177, 0.16122083, 0.2308494, 0.18310282, 0.3035779, 0.3915026, 0.18373181, 0.28172383, 0.34450924, 0.2027386, 0.42999244, 0.042918168, 0.124770716, 0.73835367, 0.19634023, 0.15181513, 0.25480023, 0.13065058, 0.3842389, 0.48758525, 0.36854598, 0.17473918, 0.15009148, 0.23584807, 0.4105625, 0.21987115, 0.05884581, 0.12469477, 0.76450604, 0.27765256, 0.26266995, 0.11842812, 0.3357631, 0.11857477, 0.41052216, 0.22881229, 0.19891387, 0.24588901, 0.41699997, 0.33931664, 0.17853989, 0.3068465, 0.14516975, 0.35022974, 0.17701842, 0.52331096, 0.22691025, 0.34059533, 0.17887013, 0.3506176, 0.2860427, 0.031014567, 0.24697174, 0.2710288, 0.40604028, 0.41473797, 0.22906232, 0.09804096, 0.31424695, 0.549804, 0.37316766, 0.23468047, 0.1961585, 0.09640372, 0.3692426, 0.571723, 0.20439148, 0.16686457, 0.3266095, 0.27209055, 0.15573597, 0.35868612, 0.15247458, 0.33397165, 0.7080229, 0.22206177, 0.09056034, 0.1877612, 0.31701872, 0.2798058, 0.30996826, 0.1557897, 0.17208444, 0.34565595, 0.40862808, 0.2595337, 0.22482932, 0.55045563, 0.16690151, 0.38351533, 0.36025625, 0.23662351, 0.15539296, 0.09533341, 0.34514204, 0.3936072, 0.25679576, 0.30347186, 0.26885474, 0.26462147, 0.43409458, 0.26475722, 0.3128482, 0.25573242, 0.30187383, 0.4831925, 0.45536155, 0.13347183, 0.36399326, 0.22040407, 0.22643903, 0.39153534, 0.29643673, 0.13438207, 0.2904483, 0.5101551, 0.3064737, 0.18985748, 0.22546402, 0.40897524, 0.3404073, 0.18963258, 0.26367202, 0.3877117, 0.23837058, 0.4724956, 0.078755364, 0.2591815, 0.2858639, 0.19422682, 0.30376482, 0.393338, 5.5504932, 1.8846432, 0.7785478, 0.5928627, 0.55086404, 0.50282156, 0.4615218, 0.42289194, 0.41193473, 0.37643144, 0.35235944, 0.40105608, 0.2806097, 0.27951628, 0.33108407, 0.27517006, 0.26573497, 0.31604186, 0.26020074, 0.22118047, 0.24902381, 0.29426107, 0.23314302, 0.20501527, 0.2612049, 0.23144723, 0.19316804, 0.21649516, 0.2332575, 0.18281567, 0.2472149, 0.19007783, 0.14872295, 0.3004306, 0.14204963, 0.10725258, 0.14647248, 0.33515757, 0.08491772, 0.11277292, 0.39148456, 0.155874, 0.08540989, 0.09683484, 0.15497293, 0.14446184, 0.20937875, 0.17157209, 0.12783843, 0.11080294, 0.24825482, 0.12981258, 0.073302604, 0.28936738, 0.121674776, 0.096592456, 0.12692182, 0.13512498, 0.11841521, 0.2533639, 0.16979355, 0.10199224, 0.13082089, 0.289229, 0.07670258, 0.103967994, 0.18839127, 0.15977763, 0.11313685, 0.12803423, 0.1506781, 0.14905918, 0.14372109, 0.17013188, 0.1089488, 0.12908706, 0.1442332, 0.21604878, 0.10237533, 0.118940145, 0.08596313, 0.25954694, 0.18258652, 0.0635206, 0.10915888, 0.16560638, 0.13655378, 0.13580269, 0.1538699, 0.1345536, 0.095855065, 0.16598377, 0.20338824, 0.12561765, 0.122785285, 0.08403205, 0.35685793, 0.1998062, 0.07513138, 0.110990055, 0.14902583, 0.115601696, 0.0838472, 0.19338769, 0.17389254, 0.16504738, 0.10061668, 0.17995502, 0.18715058, 0.12751003, 0.083635546, 0.12919836, 0.34849292, 0.17246106, 0.045949586, 0.12555544, 0.24607629, 0.15550503, 0.102736264, 0.07883338, 0.17086738, 0.113757126, 0.28366148, 0.13289315, 0.071250245, 0.08155294, 0.25096014, 0.16651413, 0.11861998, 0.120136194, 0.18055257, 0.27401412, 0.1296604, 0.12172535, 0.11651368, 0.15589805, 0.151831, 0.17711297, 0.23445706, 0.10066016, 0.13554029, 0.21039072, 0.12202377, 0.16183083, 0.1293401, 0.117558725, 0.17281948, 0.15687694, 0.18448162, 0.15223114, 0.24176623, 0.10483085, 0.06614457, 0.27065375, 0.16183034, 0.12443716, 0.07475976, 0.15056235, 0.17390545, 0.05195186, 0.16409783, 0.25699323, 0.13676998, 0.11392365, 0.18190281, 0.21665177, 0.2337329, 0.12591499, 0.11605143, 0.22270817, 0.15150881, 0.25631806, 0.2249686, 0.101877846, 0.10542971, 0.18436706, 0.31347567, 0.21549344, 0.13495353, 0.11352308, 0.14697562, 0.19072989, 0.17506342, 0.14036971, 0.16739765, 0.2237815, 0.19006409, 0.17839925, 0.090202466, 0.22422136, 0.21469307, 0.11048945, 0.23142469, 0.19523787, 0.15632191, 0.21120708, 0.14369972, 0.2301242, 0.12787554, 0.19159065, 0.1690991, 0.17149423, 0.19583884, 0.10022906, 0.18788904, 0.13340712, 0.32082647, 0.21651115, 0.07246439, 0.21106851, 0.1766367, 0.20803124, 0.3273527, 0.12474433, 0.12634163, 0.17600189, 0.19701955, 0.13939133, 0.2718968, 0.16531481, 0.07462111, 0.2926434, 0.2152906, 0.16769986, 0.16370016, 0.17478028, 0.15976901, 0.2900656, 0.18607421, 0.11135126, 0.17592216, 0.31765646, 0.1220798, 0.19776392, 0.16809523, 0.27690142, 0.24832469, 0.14993156, 0.20186147, 0.22274223, 0.111553326, 0.1887067, 0.13351229, 0.2751743, 0.28302854, 0.12164367, 0.10133035, 0.19054218, 0.26510248, 0.3199212, 0.12136139, 0.12470131, 0.14433576, 0.24846955, 0.23258172, 0.12006118, 0.1984048, 0.17914909, 0.20460798, 0.27238086, 0.16020758, 0.18354015, 0.2284753, 0.214653, 0.4041722, 0.24476, 0.15098514, 0.2130742, 0.13891324, 0.15814199, 0.17628437, 0.28673345, 0.17874508, 0.11841373, 0.13088757, 0.14668289, 0.18691875, 0.17120805, 0.40772602, 0.24365628, 0.22291656, 0.08902292, 0.21405265, 0.29555744, 0.2522434, 0.122677274, 0.18163696, 0.15454935, 0.4759812, 0.1807234, 0.09463864, 0.1279802, 0.33103603, 0.19398037, 0.12745687, 0.2242217, 0.03828347, 0.33601058, 0.29278293, 0.23874864, 0.1918289, 0.13355264, 0.25386286, 0.32840192, 0.16953292, 0.11230734, 0.26701182, 0.17332256, 0.36713153, 0.31007954, 0.26850435, 0.12184284, 0.09453514, 0.246093, 0.27189693, 0.37953922, 0.11305511, 0.18146785, 0.22644717, 0.24768622, 0.27987263, 0.15382527, 0.1158067, 0.32070115, 0.15386279, 0.25028294, 0.17583235, 0.27372488, 0.24223515, 0.3086163, 0.20075409, 0.25823003, 0.18284129, 0.1637533, 0.31152606, 0.11573818, 0.30154812, 0.46920407, 0.1318481, 0.20867237, 0.23069274, 0.20746464, 0.35698667, 0.14580691, 0.44423303, 0.13525282, 0.1451917, 0.1841245, 0.3209617, 0.17342488, 0.18974066, 0.06342106, 0.32415074, 0.4786672, 0.21374227, 0.26121154, 0.104786165, 0.23989339, 0.26587144, 0.37075925, 0.2013707, 0.16066456, 0.31318855, 0.21097331, 0.27545837, 0.23784119, 0.12191895, 0.23767546, 0.39130902, 0.16195925, 0.20002317, 0.32297418, 0.29434, 0.3354969, 0.23812631, 0.27647606, 0.17654687, 0.19096895, 0.21542935, 0.42007542, 0.20676947, 0.2678447, 0.16737363, 0.21635757, 0.37875065, 0.25749958, 0.24143887, 0.32065797, 0.15107906, 0.28300938, 0.257072, 0.3298611, 0.18193413, 0.18552606, 0.23488703, 0.5215743, 0.13019457, 0.20154838, 0.35809964, 0.29927236, 0.18047844, 0.3478386, 0.17569983, 0.23963931, 0.18053672, 0.38488907, 0.36854947, 0.17607008, 0.21214552, 0.19618866, 0.3900287, 0.36747825, 0.11077625, 0.37739354, 0.12242228, 0.24415924, 0.30828384, 0.14582926, 0.31259698, 0.28241226, 0.22218682, 0.16687706, 0.35387778, 0.2834817, 0.29483357, 0.09132109, 0.39148942, 0.47427154, 0.2925139, 0.187647, 0.16972354, 0.20536655, 0.5168, 0.39375576, 0.08551882, 0.18630394, 0.34245116, 0.38593018, 0.44449422, 0.15887341, 0.35972464, 0.11303389, 0.25633538, 0.46950385, 0.24904364, 0.15093601, 0.3173987, 0.23964708, 0.2213653, 0.31440103, 0.33994925, 0.2712448, 0.17312367, 0.373691, 0.25005528, 0.21910831, 0.23688574, 0.32012045, 0.27211574, 0.3806887, 0.17028081, 0.25234425, 0.5616955, 0.30644557, 0.26944688, 0.18969294, 0.277205, 0.3126149, 0.118077904, 0.3404967, 0.49014115, 0.18856274, 0.14941183, 0.44439745, 0.16766155, 0.3281819, 0.46792585, 0.2700804, 0.1407855, 0.25307715, 0.21039143, 0.3354411, 0.4404427, 0.43217534, 0.14998402, 0.22118065, 0.3953589, 0.28093952, 0.4222058, 0.07812342, 0.39613175, 0.5117432, 0.2684518, 0.32932335, 0.14178322, 0.29681614, 0.451425, 0.38882893, 0.39395013, 0.20253399, 0.22255185
disc_train_acc: 0.0, 0.42264851485148514, 0.6491336633663366, 0.7206683168316832, 0.7930693069306931, 0.8077970297029703, 0.8299504950495049, 0.848019801980198, 0.8485148514851485, 0.8705445544554455, 0.8650990099009901, 0.8797029702970297, 0.871410891089109, 0.8836633663366337, 0.9092821782178218, 0.8997524752475248, 0.8925742574257426, 0.9162128712871287, 0.9191831683168317, 0.9210396039603961, 0.8944306930693069, 0.9327970297029703, 0.9258663366336634, 0.936509900990099, 0.9295792079207921, 0.9337871287128713, 0.9191831683168317, 0.9386138613861386, 0.943440594059406, 0.9116336633663367, 0.950990099009901, 0.9584158415841584, 0.9517326732673267, 0.9516089108910891, 0.9206683168316832, 0.9621287128712871, 0.9579207920792079, 0.9334158415841585, 0.9522277227722772, 0.9688118811881188, 0.9573019801980198, 0.9382425742574257, 0.9659653465346535, 0.961509900990099, 0.9602722772277228, 0.9535891089108911, 0.9568069306930693, 0.9653465346534653, 0.9689356435643565, 0.9521039603960396, 0.9659653465346535, 0.965470297029703, 0.9502475247524752, 0.9617574257425743, 0.9660891089108911, 0.9724009900990099, 0.9722772277227723, 0.9403465346534653, 0.975, 0.9790841584158416, 0.9806930693069307, 0.967450495049505, 0.968440594059406, 0.9689356435643565, 0.976980198019802, 0.9527227722772277, 0.974009900990099, 0.9711633663366337, 0.9669554455445545, 0.9792079207920792, 0.9712871287128713, 0.961509900990099, 0.975990099009901, 0.9743811881188119, 0.9774752475247525, 0.9556930693069307, 0.9861386138613861, 0.9805693069306931, 0.9660891089108911, 0.9811881188118812, 0.9736386138613862, 0.9716584158415842, 0.9715346534653465, 0.973019801980198, 0.9774752475247525, 0.9887376237623763, 0.9679455445544555, 0.9790841584158416, 0.9803217821782179, 0.9742574257425742, 0.9699257425742575, 0.9845297029702971, 0.9757425742574257, 0.9820544554455446, 0.9763613861386139, 0.9725247524752475, 0.9790841584158416, 0.9837871287128713, 0.974009900990099, 0.9737623762376237, 0.9771039603960396, 0.9794554455445544, 0.9875, 0.9800742574257426, 0.966460396039604, 0.9842821782178218, 0.9847772277227723, 0.9788366336633664, 0.9797029702970297, 0.9845297029702971, 0.9810643564356436, 0.9795792079207921, 0.9709158415841584, 0.9826732673267327, 0.9896039603960396, 0.9857673267326733, 0.9668316831683168, 0.9886138613861386, 0.9908415841584158, 0.9860148514851486, 0.9727722772277227, 0.9846534653465346, 0.9856435643564356, 0.9858910891089109, 0.9829207920792079, 0.9801980198019802, 0.9860148514851486, 0.9831683168316832, 0.9790841584158416, 0.9834158415841584, 0.988490099009901, 0.9841584158415841, 0.9863861386138614, 0.9799504950495049, 0.9850247524752476, 0.9835396039603961, 0.9852722772277228, 0.9794554455445544, 0.9887376237623763, 0.9861386138613861, 0.9813118811881189, 0.9815594059405941, 0.9875, 0.9875, 0.9831683168316832, 0.9845297029702971, 0.9871287128712871, 0.9790841584158416, 0.9876237623762376, 0.9926980198019802, 0.9850247524752476, 0.9860148514851486, 0.9850247524752476, 0.9879950495049505, 0.9764851485148515, 0.9876237623762376, 0.9892326732673268, 0.9909653465346535, 0.9851485148514851, 0.9836633663366336, 0.9902227722772278, 0.9907178217821783, 0.9858910891089109, 0.9806930693069307, 0.9915841584158416, 0.9846534653465346, 0.9922029702970298, 0.9897277227722773, 0.975990099009901, 0.9840346534653466, 0.9908415841584158, 0.9910891089108911, 0.988490099009901, 0.9918316831683168, 0.9834158415841584, 0.986509900990099, 0.9825495049504951, 0.988490099009901, 0.9951732673267327, 0.9862623762376238, 0.9872524752475248, 0.9867574257425743, 0.9905940594059406, 0.9853960396039604, 0.9873762376237624, 0.9875, 0.9887376237623763, 0.9889851485148515, 0.9920792079207921, 0.986509900990099, 0.9912128712871288, 0.9883663366336634, 0.9902227722772278, 0.9877475247524753, 0.9897277227722773, 0.9857673267326733, 0.9896039603960396, 0.9900990099009901, 0.9910891089108911, 0.9871287128712871, 0.9899752475247525, 0.9889851485148515, 0.9896039603960396, 0.9846534653465346, 0.990470297029703, 0.9923267326732673, 0.9888613861386139, 0.9919554455445545, 0.9912128712871288, 0.9917079207920793, 0.9871287128712871, 0.9910891089108911, 0.9902227722772278, 0.9879950495049505, 0.9887376237623763, 0.9879950495049505, 0.9907178217821783, 0.9965346534653465, 0.9883663366336634, 0.9853960396039604, 0.9862623762376238, 0.9925742574257426, 0.9948019801980198, 0.9873762376237624, 0.9896039603960396, 0.9925742574257426, 0.9883663366336634, 0.9931930693069307, 0.9887376237623763, 0.9926980198019802, 0.991460396039604, 0.9900990099009901, 0.9893564356435643, 0.9926980198019802, 0.9915841584158416, 0.9898514851485148, 0.9893564356435643, 0.9919554455445545, 0.9931930693069307, 0.9910891089108911, 0.9863861386138614, 0.9920792079207921, 0.9948019801980198, 0.9853960396039604, 0.9933168316831683, 0.9905940594059406, 0.9919554455445545, 0.9908415841584158, 0.9855198019801981, 0.9923267326732673, 0.9935643564356436, 0.993440594059406, 0.9933168316831683, 0.9931930693069307, 0.988490099009901, 0.9910891089108911, 0.9929455445544555, 0.9886138613861386, 0.993440594059406, 0.9896039603960396, 0.991460396039604, 0.9955445544554455, 0.994059405940594, 0.9897277227722773, 0.9902227722772278, 0.993440594059406, 0.9920792079207921, 0.9913366336633663, 0.9931930693069307, 0.9920792079207921, 0.9908415841584158, 0.9928217821782178, 0.995049504950495, 0.986509900990099, 0.9882425742574258, 0.9926980198019802, 0.9939356435643565, 0.9917079207920793, 0.9922029702970298, 0.9913366336633663, 0.9959158415841585, 0.994430693069307, 0.9834158415841584, 0.9907178217821783, 0.9939356435643565, 0.9917079207920793, 0.9939356435643565, 0.9945544554455445, 0.9918316831683168, 0.9910891089108911, 0.9948019801980198, 0.9923267326732673, 0.9928217821782178, 0.9877475247524753, 0.9943069306930693, 0.9920792079207921, 0.9918316831683168, 0.9922029702970298, 0.9961633663366337, 0.9943069306930693, 0.9871287128712871, 0.9920792079207921, 0.995049504950495, 0.9965346534653465, 0.994430693069307, 0.9903465346534653, 0.9912128712871288, 0.9922029702970298, 0.9941831683168317, 0.9941831683168317, 0.9903465346534653, 0.9912128712871288, 0.993440594059406, 0.9943069306930693, 0.9923267326732673, 0.9923267326732673, 0.9930693069306931, 0.993440594059406, 0.9956683168316832, 0.991460396039604, 0.9941831683168317, 0.9943069306930693, 0.989480198019802, 0.9933168316831683, 0.994430693069307, 0.9923267326732673, 0.9920792079207921, 0.9949257425742575, 0.9966584158415842, 0.989480198019802, 0.9923267326732673, 0.994059405940594, 0.9961633663366337, 0.9957920792079208, 0.9920792079207921, 0.9929455445544555, 0.993440594059406, 0.9896039603960396, 0.9943069306930693, 0.9957920792079208, 0.9915841584158416, 0.990470297029703, 0.9943069306930693, 0.996410891089109, 0.995049504950495, 0.996410891089109, 0.9913366336633663, 0.9903465346534653, 0.9929455445544555, 0.9949257425742575, 0.9969059405940595, 0.9928217821782178, 0.9929455445544555, 0.9956683168316832, 0.9956683168316832, 0.994430693069307, 0.9899752475247525, 0.995049504950495, 0.9966584158415842, 0.991460396039604, 0.9938118811881188, 0.9935643564356436, 0.993440594059406, 0.9939356435643565, 0.994059405940594, 0.9951732673267327, 0.995420792079208, 0.9913366336633663, 0.9930693069306931, 0.9939356435643565, 0.9956683168316832, 0.9952970297029703, 0.995420792079208, 0.9930693069306931, 0.9930693069306931, 0.9948019801980198, 0.993440594059406, 0.9930693069306931, 0.9956683168316832, 0.9912128712871288, 0.9985148514851485, 0.9969059405940595, 0.9881188118811881, 0.9949257425742575, 0.9952970297029703, 0.9941831683168317, 0.9966584158415842, 0.993440594059406, 0.9913366336633663, 0.9926980198019802, 0.9952970297029703, 0.9969059405940595, 0.995049504950495, 0.9920792079207921, 0.9959158415841585, 0.9977722772277228, 0.99740099009901, 0.986509900990099, 0.9941831683168317, 0.9935643564356436, 0.9972772277227723, 0.9941831683168317, 0.9971534653465347, 0.993440594059406, 0.9931930693069307, 0.9956683168316832, 0.9952970297029703, 0.9920792079207921, 0.9938118811881188, 0.9967821782178218, 0.9945544554455445, 0.9967821782178218, 0.9939356435643565, 0.9967821782178218, 0.9919554455445545, 0.995049504950495, 0.9946782178217822, 0.9966584158415842, 0.994059405940594, 0.9948019801980198, 0.9988861386138614, 0.994430693069307, 0.994059405940594, 0.9941831683168317, 0.9941831683168317, 0.9961633663366337, 0.9977722772277228, 0.995049504950495, 0.9918316831683168, 0.9935643564356436, 0.9966584158415842, 0.9965346534653465, 0.9972772277227723, 0.993440594059406, 0.9922029702970298, 0.997029702970297, 0.9965346534653465, 0.9957920792079208, 0.9935643564356436, 0.996039603960396, 0.9938118811881188, 0.9967821782178218, 0.9951732673267327, 0.9908415841584158, 0.996039603960396, 0.998019801980198, 0.996410891089109, 0.9951732673267327, 0.994059405940594, 0.9941831683168317, 0.996410891089109, 0.997029702970297, 0.9948019801980198, 0.9930693069306931, 0.9951732673267327, 0.9966584158415842, 0.9929455445544555, 0.9971534653465347, 0.9943069306930693, 0.9939356435643565, 0.9955445544554455, 0.9972772277227723, 0.9985148514851485, 0.9943069306930693, 0.9930693069306931, 0.9957920792079208, 0.9936881188118812, 0.9956683168316832, 0.995049504950495, 0.9948019801980198, 0.9956683168316832, 0.9955445544554455, 0.9967821782178218, 0.9955445544554455, 0.9931930693069307, 0.9945544554455445, 0.9982673267326733, 0.9965346534653465, 0.9959158415841585, 0.9955445544554455, 0.9948019801980198, 0.995420792079208, 0.9977722772277228, 0.9955445544554455, 0.9928217821782178, 0.995420792079208, 0.99740099009901, 0.997029702970297, 0.9939356435643565, 0.9955445544554455, 0.9967821782178218, 0.9952970297029703, 0.9943069306930693, 0.996039603960396, 0.9933168316831683, 0.9981435643564357, 0.995420792079208, 0.996039603960396, 0.9969059405940595, 0.9957920792079208, 0.9948019801980198, 0.0, 0.41955445544554454, 0.7042079207920792, 0.7868811881188119, 0.8017326732673268, 0.8282178217821782, 0.8387376237623763, 0.8532178217821782, 0.8597772277227723, 0.8685643564356436, 0.8800742574257425, 0.8717821782178218, 0.9023514851485148, 0.9060643564356435, 0.8948019801980198, 0.9016089108910891, 0.9165841584158416, 0.9004950495049505, 0.9168316831683169, 0.925990099009901, 0.9216584158415841, 0.9120049504950495, 0.925, 0.9346534653465347, 0.9198019801980198, 0.9263613861386139, 0.9403465346534653, 0.9388613861386138, 0.9318069306930693, 0.9455445544554455, 0.9294554455445545, 0.9481435643564357, 0.9565594059405941, 0.9271039603960396, 0.9551980198019802, 0.968069306930693, 0.9542079207920792, 0.9214108910891089, 0.9737623762376237, 0.9660891089108911, 0.9246287128712871, 0.9596534653465346, 0.9737623762376237, 0.9731435643564357, 0.9584158415841584, 0.960519801980198, 0.9495049504950495, 0.9566831683168316, 0.9655940594059406, 0.968440594059406, 0.9517326732673267, 0.9706683168316832, 0.9790841584158416, 0.944059405940594, 0.9738861386138614, 0.9753712871287129, 0.969430693069307, 0.968069306930693, 0.9693069306930693, 0.9488861386138614, 0.9612623762376238, 0.9754950495049505, 0.9707920792079208, 0.9475247524752475, 0.9800742574257426, 0.9792079207920792, 0.9622524752475248, 0.9648514851485148, 0.9724009900990099, 0.9738861386138614, 0.9678217821782178, 0.9714108910891089, 0.9699257425742575, 0.9689356435643565, 0.9753712871287129, 0.973019801980198, 0.9724009900990099, 0.966460396039604, 0.9797029702970297, 0.9758663366336634, 0.9834158415841584, 0.9596534653465346, 0.9662128712871287, 0.9857673267326733, 0.9793316831683169, 0.9701732673267327, 0.9766089108910891, 0.9762376237623762, 0.9748762376237624, 0.9757425742574257, 0.9804455445544554, 0.972029702970297, 0.9696782178217822, 0.9803217821782179, 0.9805693069306931, 0.9846534653465346, 0.9600247524752475, 0.9747524752475247, 0.9868811881188119, 0.9825495049504951, 0.9780940594059406, 0.9820544554455446, 0.9846534653465346, 0.9717821782178218, 0.9754950495049505, 0.9775990099009901, 0.9846534653465346, 0.9743811881188119, 0.9757425742574257, 0.9808168316831684, 0.9867574257425743, 0.9834158415841584, 0.9662128712871287, 0.9787128712871287, 0.9912128712871288, 0.9831683168316832, 0.9754950495049505, 0.9784653465346534, 0.9850247524752476, 0.9867574257425743, 0.9787128712871287, 0.9857673267326733, 0.9715346534653465, 0.9834158415841584, 0.9892326732673268, 0.9893564356435643, 0.9717821782178218, 0.9819306930693069, 0.9851485148514851, 0.9851485148514851, 0.9788366336633664, 0.9745049504950495, 0.9853960396039604, 0.9850247524752476, 0.9852722772277228, 0.9818069306930693, 0.9835396039603961, 0.9799504950495049, 0.9778465346534654, 0.9872524752475248, 0.9862623762376238, 0.9772277227722772, 0.9850247524752476, 0.9839108910891089, 0.9850247524752476, 0.9863861386138614, 0.9821782178217822, 0.9846534653465346, 0.9823019801980198, 0.9844059405940594, 0.9768564356435644, 0.9882425742574258, 0.9922029702970298, 0.9763613861386139, 0.986509900990099, 0.9862623762376238, 0.9920792079207921, 0.9853960396039604, 0.9830445544554456, 0.9933168316831683, 0.9851485148514851, 0.9785891089108911, 0.9873762376237624, 0.9883663366336634, 0.9844059405940594, 0.9840346534653466, 0.9815594059405941, 0.989480198019802, 0.9898514851485148, 0.9855198019801981, 0.9877475247524753, 0.9790841584158416, 0.9821782178217822, 0.9918316831683168, 0.9891089108910891, 0.9851485148514851, 0.9810643564356436, 0.9840346534653466, 0.9877475247524753, 0.9918316831683168, 0.9883663366336634, 0.9856435643564356, 0.9868811881188119, 0.9891089108910891, 0.9876237623762376, 0.9851485148514851, 0.9847772277227723, 0.9861386138613861, 0.9929455445544555, 0.9852722772277228, 0.9857673267326733, 0.9919554455445545, 0.9832920792079208, 0.9866336633663366, 0.9876237623762376, 0.9846534653465346, 0.9905940594059406, 0.9851485148514851, 0.9899752475247525, 0.9875, 0.9871287128712871, 0.9886138613861386, 0.9852722772277228, 0.9923267326732673, 0.9889851485148515, 0.9909653465346535, 0.9827970297029703, 0.988490099009901, 0.9929455445544555, 0.9870049504950495, 0.989480198019802, 0.989480198019802, 0.9832920792079208, 0.993440594059406, 0.9918316831683168, 0.988490099009901, 0.9883663366336634, 0.9925742574257426, 0.9862623762376238, 0.9893564356435643, 0.9939356435643565, 0.9853960396039604, 0.9868811881188119, 0.9886138613861386, 0.9886138613861386, 0.9912128712871288, 0.9897277227722773, 0.9872524752475248, 0.9905940594059406, 0.992450495049505, 0.9905940594059406, 0.9855198019801981, 0.993440594059406, 0.990470297029703, 0.9898514851485148, 0.986509900990099, 0.9886138613861386, 0.991460396039604, 0.9909653465346535, 0.9888613861386139, 0.9929455445544555, 0.9900990099009901, 0.9910891089108911, 0.9852722772277228, 0.9887376237623763, 0.9922029702970298, 0.9939356435643565, 0.9920792079207921, 0.9876237623762376, 0.9860148514851486, 0.9918316831683168, 0.9931930693069307, 0.9922029702970298, 0.9881188118811881, 0.9899752475247525, 0.9931930693069307, 0.9907178217821783, 0.9917079207920793, 0.9913366336633663, 0.9882425742574258, 0.9918316831683168, 0.9930693069306931, 0.9893564356435643, 0.990470297029703, 0.9856435643564356, 0.9909653465346535, 0.9926980198019802, 0.9918316831683168, 0.9917079207920793, 0.9922029702970298, 0.9920792079207921, 0.9872524752475248, 0.9929455445544555, 0.9939356435643565, 0.993440594059406, 0.9929455445544555, 0.9918316831683168, 0.9925742574257426, 0.9856435643564356, 0.9917079207920793, 0.991460396039604, 0.996410891089109, 0.9913366336633663, 0.9893564356435643, 0.9897277227722773, 0.9936881188118812, 0.992450495049505, 0.993440594059406, 0.9857673267326733, 0.9926980198019802, 0.995420792079208, 0.9951732673267327, 0.9876237623762376, 0.9933168316831683, 0.9935643564356436, 0.9909653465346535, 0.9972772277227723, 0.9883663366336634, 0.9889851485148515, 0.9928217821782178, 0.9929455445544555, 0.9949257425742575, 0.9907178217821783, 0.9899752475247525, 0.9923267326732673, 0.995049504950495, 0.9910891089108911, 0.993440594059406, 0.9887376237623763, 0.9931930693069307, 0.9919554455445545, 0.9946782178217822, 0.996039603960396, 0.9915841584158416, 0.9909653465346535, 0.9876237623762376, 0.9956683168316832, 0.9938118811881188, 0.9930693069306931, 0.9918316831683168, 0.990470297029703, 0.9945544554455445, 0.995420792079208, 0.989480198019802, 0.9941831683168317, 0.9919554455445545, 0.9936881188118812, 0.9909653465346535, 0.9908415841584158, 0.9928217821782178, 0.9941831683168317, 0.9920792079207921, 0.994059405940594, 0.994430693069307, 0.9915841584158416, 0.9965346534653465, 0.9920792079207921, 0.988490099009901, 0.9949257425742575, 0.9936881188118812, 0.9933168316831683, 0.9928217821782178, 0.9902227722772278, 0.995049504950495, 0.9881188118811881, 0.9957920792079208, 0.9948019801980198, 0.9941831683168317, 0.9903465346534653, 0.993440594059406, 0.9936881188118812, 0.9982673267326733, 0.9918316831683168, 0.9893564356435643, 0.9931930693069307, 0.9930693069306931, 0.9971534653465347, 0.9935643564356436, 0.9941831683168317, 0.991460396039604, 0.994430693069307, 0.995420792079208, 0.9907178217821783, 0.9938118811881188, 0.9929455445544555, 0.9945544554455445, 0.9961633663366337, 0.9925742574257426, 0.9900990099009901, 0.9962871287128713, 0.9952970297029703, 0.9929455445544555, 0.9929455445544555, 0.991460396039604, 0.9936881188118812, 0.9936881188118812, 0.994430693069307, 0.9948019801980198, 0.9948019801980198, 0.9897277227722773, 0.9949257425742575, 0.9933168316831683, 0.996039603960396, 0.9946782178217822, 0.9929455445544555, 0.9946782178217822, 0.9931930693069307, 0.9939356435643565, 0.9959158415841585, 0.9935643564356436, 0.994059405940594, 0.9917079207920793, 0.9956683168316832, 0.9951732673267327, 0.9945544554455445, 0.9902227722772278, 0.9969059405940595, 0.9946782178217822, 0.9936881188118812, 0.993440594059406, 0.9962871287128713, 0.9935643564356436, 0.994059405940594, 0.9949257425742575, 0.995049504950495, 0.9931930693069307, 0.9925742574257426, 0.9952970297029703, 0.995049504950495, 0.995420792079208, 0.9912128712871288, 0.9923267326732673, 0.9966584158415842, 0.992450495049505, 0.9967821782178218, 0.9935643564356436, 0.9946782178217822, 0.9965346534653465, 0.9922029702970298, 0.9936881188118812, 0.9962871287128713, 0.9967821782178218, 0.9920792079207921, 0.9939356435643565, 0.994059405940594, 0.9982673267326733, 0.9936881188118812, 0.9912128712871288, 0.9938118811881188, 0.996410891089109, 0.9976485148514852, 0.9969059405940595, 0.9917079207920793, 0.9926980198019802, 0.9972772277227723, 0.9959158415841585, 0.9935643564356436, 0.9935643564356436, 0.9933168316831683, 0.9967821782178218, 0.9930693069306931, 0.9972772277227723, 0.9952970297029703, 0.9925742574257426, 0.996039603960396, 0.9961633663366337, 0.994430693069307, 0.9957920792079208, 0.996039603960396, 0.9943069306930693, 0.9935643564356436, 0.9943069306930693, 0.9957920792079208, 0.9939356435643565, 0.995049504950495, 0.9959158415841585, 0.9946782178217822, 0.994059405940594, 0.9951732673267327, 0.9936881188118812, 0.9967821782178218, 0.9957920792079208, 0.9915841584158416, 0.9955445544554455, 0.9966584158415842, 0.9961633663366337, 0.995420792079208, 0.994430693069307, 0.9967821782178218, 0.994430693069307, 0.9922029702970298, 0.995420792079208, 0.9969059405940595, 0.992450495049505, 0.996410891089109, 0.9965346534653465, 0.9939356435643565, 0.9959158415841585, 0.9965346534653465, 0.9952970297029703, 0.9965346534653465, 0.9938118811881188, 0.9945544554455445, 0.9943069306930693, 0.996410891089109, 0.9956683168316832, 0.9943069306930693, 0.995420792079208, 0.9941831683168317, 0.9978960396039604, 0.9949257425742575, 0.9936881188118812, 0.995420792079208, 0.995420792079208, 0.9967821782178218, 0.9956683168316832, 0.9926980198019802, 0.9943069306930693, 0.9952970297029703, 0.9959158415841585, 0.9956683168316832
gen_val_loss: 0.0045252773, 2.7682154, 4.4339967, 5.242852, 5.761262, 6.434574, 6.8248534, 8.603017, 7.7129517, 9.391827, 9.390971, 8.582453, 8.927569, 9.689357, 11.568691, 15.012876, 11.223435, 11.937678, 9.532513, 12.521753, 12.713649, 10.701234, 11.543612, 11.564173, 12.081355, 12.776858, 13.727044, 12.357568, 17.566154, 13.601758, 13.126782, 12.407825, 12.945551, 18.508953, 11.817457, 15.955288, 15.701419, 14.341218, 15.025178, 16.075256, 13.896533, 16.50604, 13.745111, 15.981181, 18.394043, 18.534304, 16.284615, 14.066231, 17.029716, 14.153354, 16.638687, 15.330996, 18.718021, 17.464817, 19.580542, 15.268002, 24.326084, 18.441454, 20.135767, 19.081928, 19.494774, 21.76735, 18.3236, 15.24675, 15.848496, 20.822432, 22.594662, 18.32125, 18.800566, 21.225885, 18.68553, 17.377968, 18.927412, 22.336628, 20.441338, 22.961885, 22.038637, 25.310572, 23.545435, 20.27614, 26.79224, 23.60737, 26.165771, 23.599678, 21.163645, 21.55777, 25.404394, 24.210371, 25.275581, 26.651587, 26.675383, 24.981857, 28.32103, 23.63004, 27.280478, 27.802868, 26.016531, 26.299454, 25.590523, 24.864216, 29.547056, 28.753206, 29.068459, 35.163544, 33.812595, 31.278645, 34.869633, 31.72412, 33.34491, 31.425217, 31.231556, 34.48775, 33.666267, 38.05271, 35.984993, 40.555202, 35.638283, 34.189224, 32.69532, 32.699413, 40.728954, 36.71499, 36.413345, 37.190765, 43.583557, 40.762543, 35.91422, 43.289486, 41.14799, 45.331062, 42.299397, 39.11272, 39.204945, 37.45629, 41.482822, 37.83542, 40.33661, 51.806416, 41.7234, 42.238976, 45.002403, 47.89464, 41.145, 44.408085, 43.27529, 39.860516, 45.34799, 47.695866, 41.633476, 42.473526, 44.323326, 45.68325, 45.70623, 41.457138, 44.87401, 46.342297, 49.79352, 50.657494, 49.16345, 47.09982, 44.94702, 48.641567, 59.497566, 52.932827, 54.50896, 59.49675, 54.6972, 47.922993, 59.42923, 60.143116, 54.9687, 56.06373, 58.22442, 57.71031, 59.189682, 66.13627, 53.322365, 59.176487, 59.343777, 64.81733, 65.663284, 59.128242, 60.166603, 60.1015, 63.71477, 68.07854, 62.577957, 63.03757, 64.53061, 66.25091, 58.73836, 68.26775, 65.88167, 68.064804, 60.118332, 66.66879, 65.05495, 74.81961, 81.79145, 69.98519, 72.10408, 68.099014, 65.7449, 70.54642, 71.07614, 68.2482, 65.753, 71.27655, 69.04685, 70.183914, 68.455154, 65.80302, 85.34647, 76.261086, 73.07652, 74.87913, 73.56934, 73.73215, 85.80744, 77.212654, 77.45956, 76.240326, 80.93795, 70.446785, 82.18675, 82.789314, 82.0399, 80.46825, 85.5556, 83.99966, 86.07243, 84.11984, 81.35963, 94.03525, 86.8355, 78.45125, 88.031204, 81.89149, 73.52198, 79.31109, 90.64574, 79.457695, 84.796005, 96.96719, 106.43894, 91.50081, 92.537964, 92.44367, 94.25319, 94.98578, 93.64455, 88.58894, 93.9212, 96.42326, 97.9903, 93.32272, 93.76863, 99.292656, 92.2123, 112.564606, 93.239494, 94.5694, 95.85922, 99.1885, 111.47296, 103.37602, 101.148224, 97.39619, 96.05985, 96.33565, 98.744064, 98.06424, 99.28891, 118.50231, 99.85647, 121.19664, 100.6946, 108.86364, 101.455635, 102.16461, 102.929596, 104.28692, 112.115746, 117.341835, 123.71804, 122.15615, 117.04418, 115.86703, 124.75419, 122.27365, 116.405266, 115.619606, 131.80449, 115.96487, 133.07478, 125.917145, 131.34952, 109.17341, 118.74167, 114.60631, 121.17991, 122.18216, 128.32323, 140.0492, 122.69604, 120.070076, 129.32175, 136.58516, 133.28012, 133.91008, 148.17935, 162.13664, 151.38907, 152.41304, 135.88101, 132.98917, 132.99661, 131.69196, 126.95869, 125.56655, 130.87267, 132.50998, 148.62445, 126.65578, 130.1939, 147.22552, 152.54663, 152.61462, 138.7266, 161.58589, 148.90535, 144.6403, 160.28528, 141.28926, 140.29646, 142.13742, 159.6798, 168.1845, 140.93839, 166.81721, 158.7507, 167.29082, 162.20819, 154.8113, 151.56908, 152.36731, 160.18715, 171.8147, 165.82767, 168.33377, 178.74098, 156.36667, 156.12073, 176.05406, 164.63248, 154.94829, 173.94405, 158.45041, 148.10947, 163.89917, 162.87982, 153.12254, 163.21129, 182.74037, 169.429, 167.78801, 180.43893, 174.8809, 183.37271, 192.91475, 186.42876, 195.88385, 173.27718, 193.22784, 183.96053, 177.99211, 180.86983, 177.60564, 185.63104, 197.79047, 200.23488, 170.38243, 188.65254, 200.55754, 185.44542, 185.67767, 221.56207, 171.39203, 185.64383, 189.39403, 190.78304, 181.40324, 190.5344, 189.81326, 186.78554, 198.94606, 198.1781, 213.07532, 220.21199, 205.90976, 222.5887, 194.08722, 210.91289, 231.99515, 197.8269, 201.78896, 226.09477, 237.55452, 214.95259, 221.0295, 207.21646, 197.81961, 196.25041, 199.05618, 216.79594, 220.36981, 216.8088, 206.30244, 204.36462, 215.47034, 210.09973, 199.50389, 222.51056, 247.81398, 215.91594, 212.01527, 217.66852, 210.89583, 229.9156, 231.94514, 212.28171, 233.07324, 222.26549, 243.34384, 233.5712, 213.61748, 216.56541, 208.89899, 206.00465, 219.71246, 233.31966, 207.14337, 249.21861, 235.32114, 239.72975, 234.69588, 246.13875, 226.93782, 237.27292, 250.58179, 234.17212, 242.49046, 262.10095, 220.17677, 262.85114, 261.21213, 244.44421, 261.23373, 241.06897, 238.6602, 247.59589, 254.02264, 240.61234, 255.82187, 242.39482, 273.7397, 260.61838, 271.5888, 297.75803, 268.3704, 270.946, 254.0469, 251.358, 268.45923, 251.72115, 291.40515, 274.0517, 243.73003, 265.18826, 290.78867, 253.14703, 273.17102, 255.48909, 264.32394, 277.60803, 279.46655, 287.51233, 275.90344, 293.53284, 247.22014, 255.57033, 290.44528, 273.2068, 294.52603, 288.37482, 271.06424, 285.39526, 267.3855, 269.27664, 254.38704, 291.2193, 0.0038174859, 4.389354, 5.948767, 6.0051427, 9.090755, 8.954978, 9.909139, 11.447682, 10.682827, 10.283751, 11.202745, 9.563384, 10.121626, 11.276258, 13.68241, 12.022934, 13.691089, 14.56576, 13.21833, 14.591819, 15.373465, 10.265053, 14.706151, 15.418181, 14.506501, 16.578693, 14.028945, 15.141663, 15.254534, 14.657062, 15.382908, 14.545729, 15.093616, 15.681787, 15.459922, 14.502415, 14.722109, 17.330938, 15.578716, 14.867256, 17.543186, 19.179003, 16.839163, 17.269072, 15.444126, 15.903817, 15.522021, 17.91637, 19.73553, 18.757505, 19.009329, 17.909866, 16.432348, 19.833878, 16.600283, 17.728573, 20.50206, 18.045675, 21.060673, 23.338533, 19.580545, 20.955471, 17.251175, 19.501612, 21.701649, 17.890554, 20.370884, 22.078037, 20.141134, 21.84345, 21.89151, 25.410864, 22.254442, 22.619946, 22.2978, 22.014427, 25.42551, 23.74829, 22.18819, 23.349005, 23.360907, 28.55817, 23.92578, 24.358831, 26.763664, 24.526371, 24.559101, 24.378263, 28.717333, 26.91193, 27.033785, 31.359201, 30.53688, 26.32176, 26.010832, 29.990713, 32.543324, 33.68189, 33.021442, 34.180763, 30.319593, 27.42011, 26.256418, 32.08036, 37.067886, 30.447176, 28.673777, 33.259686, 34.37735, 34.131783, 34.49531, 32.771095, 34.279507, 32.488388, 33.122536, 34.95338, 27.312857, 33.472603, 32.50993, 32.95245, 34.363415, 39.37396, 38.14588, 35.39743, 34.485, 34.317226, 35.81212, 35.06632, 33.657658, 34.809853, 33.52691, 42.369736, 36.605453, 39.35738, 39.315395, 37.858013, 41.57758, 44.284813, 40.20434, 38.663284, 38.22642, 42.19865, 35.938793, 36.116306, 38.357273, 41.589073, 38.153137, 41.26571, 39.898914, 46.3771, 42.579777, 43.883926, 43.420147, 48.318928, 44.58485, 48.98796, 50.0025, 42.091293, 41.026306, 42.764687, 48.290295, 50.307297, 51.562664, 44.83718, 46.516895, 53.694077, 55.99772, 48.545578, 49.843533, 42.315853, 42.368187, 58.100044, 52.120323, 50.443584, 60.42197, 47.62494, 70.315994, 58.688118, 59.754612, 54.470886, 50.58705, 57.108303, 54.59205, 64.50735, 49.914864, 51.271286, 53.05993, 56.85103, 55.097855, 59.593796, 56.132957, 58.492447, 57.938217, 52.766586, 63.91288, 60.782536, 59.90129, 58.048096, 60.61494, 64.99399, 62.01886, 69.92313, 65.94943, 67.63927, 62.601982, 65.22625, 68.55452, 64.544044, 65.56823, 69.4877, 62.9569, 66.25984, 66.04977, 71.01496, 76.50034, 73.04461, 68.87538, 68.75318, 69.06284, 75.63212, 74.77877, 82.30996, 74.10998, 78.436264, 70.20279, 71.917984, 73.09425, 77.55512, 77.463554, 78.85343, 81.04416, 79.66569, 73.55676, 75.00911, 79.81162, 84.4801, 75.72455, 83.59122, 82.526054, 85.76081, 85.21707, 73.129364, 84.305916, 84.106514, 86.55484, 81.970795, 78.61159, 82.078705, 78.69013, 81.49822, 93.20436, 93.1659, 92.71429, 88.95642, 90.279976, 85.213776, 79.69921, 83.24979, 90.86949, 94.2715, 88.41144, 86.78793, 88.54446, 88.23815, 118.41244, 101.741325, 106.053, 97.734604, 97.6257, 97.29707, 93.637825, 103.56528, 96.51882, 89.72025, 97.22849, 92.23956, 105.317276, 106.345245, 110.603134, 107.964806, 108.14046, 103.199356, 106.73441, 98.75613, 108.607506, 100.69884, 105.37211, 105.014824, 102.836494, 93.31818, 102.79934, 101.2031, 115.28715, 111.90079, 102.877625, 112.4629, 109.00294, 104.60136, 115.08626, 113.74525, 117.138336, 107.51071, 120.5567, 122.547714, 112.74902, 116.666115, 117.78054, 117.609726, 123.93428, 125.4401, 115.706604, 111.17106, 105.033104, 103.6315, 114.10437, 130.28084, 119.39872, 124.329094, 124.135704, 130.16447, 129.3131, 126.00627, 114.55488, 134.18495, 131.28102, 132.0943, 127.45656, 132.75258, 150.93741, 129.59142, 139.10211, 133.50056, 137.6222, 143.45233, 136.31651, 134.95232, 130.23952, 135.74092, 125.34373, 137.12877, 134.46402, 142.94745, 134.91212, 141.10187, 130.52232, 132.98323, 138.6272, 145.00546, 139.20737, 138.32486, 146.10997, 142.47688, 146.97795, 160.29442, 158.40941, 156.3569, 150.03732, 135.41139, 146.94913, 139.36697, 158.32127, 158.43095, 154.7606, 142.89682, 166.4249, 153.227, 151.01927, 144.03209, 166.59456, 161.46414, 161.88115, 172.67337, 151.84528, 164.112, 151.80762, 161.82806, 163.50127, 172.54526, 170.07674, 170.94437, 182.87946, 162.79153, 166.38077, 188.68832, 165.6524, 164.73528, 167.28636, 163.33174, 166.48619, 165.01831, 169.11197, 163.72974, 170.74109, 165.0322, 171.22656, 187.29071, 179.65895, 196.42198, 180.97986, 183.8942, 176.69539, 183.33841, 188.34607, 187.41556, 174.54057, 180.21236, 172.30319, 191.10118, 171.9708, 197.06277, 167.86043, 182.7451, 201.04228, 212.35844, 199.3296, 207.65433, 197.23787, 186.0779, 204.05956, 199.39479, 193.30879, 198.94032, 192.89365, 202.13815, 200.00977, 209.53131, 198.40257, 224.02737, 208.70735, 193.04218, 189.175, 189.7177, 218.49367, 200.46034, 213.12466, 208.7233, 214.44234, 210.8774, 215.58333, 226.04242, 212.12181, 220.76176, 227.96718, 222.8187, 237.1437, 220.46675, 208.09076, 214.17284, 207.42566, 219.70413, 234.26971, 217.90686, 218.58626, 234.15877, 214.51988, 228.8646, 221.57347, 223.6187, 241.13913, 211.95685, 223.068, 229.04329, 236.97414, 228.3941, 218.72151, 248.38257, 242.21318, 241.04782, 249.15073, 254.41972, 234.4034, 239.36044, 238.83502, 227.81688, 245.40007, 230.93823, 244.09395, 223.37798, 261.5237, 257.2855, 242.38985, 266.57242, 246.00134, 233.43625, 256.98923, 247.15182, 235.86789, 248.12039, 235.80742, 242.10692, 244.6888, 228.0368, 247.83794, 230.92479, 233.1941, 243.02902, 254.0253, 251.3054, 239.8144, 242.11234, 254.81708
disc_val_loss: 5.570874, 1.0818641, 0.98817426, 0.58267856, 0.81593543, 0.5491013, 0.37975955, 0.56963384, 0.6845771, 0.6158537, 0.49519137, 0.46413246, 0.5871805, 0.44459343, 0.5768041, 0.40207115, 0.44608358, 0.5967064, 0.44329438, 0.4012807, 0.37489593, 0.31851995, 0.6213593, 0.44888964, 0.5303208, 0.48276952, 0.4829996, 0.7076475, 0.93670946, 1.146577, 0.37883013, 0.48834574, 0.38826528, 1.0514356, 0.44688556, 0.34718654, 0.6741871, 0.6938041, 0.3319172, 0.47783563, 1.1336468, 0.7438919, 0.6474555, 0.49220315, 1.0195068, 0.68252546, 0.51465607, 0.33536223, 0.43670455, 0.5606991, 0.42789394, 0.58639836, 0.9219991, 0.58156943, 0.45951518, 0.50139916, 1.5896678, 0.76287377, 0.6418585, 0.605394, 0.44493312, 0.81501067, 0.8952239, 0.41134977, 1.1185675, 0.55663645, 0.65560466, 0.6513321, 0.7956645, 0.7964882, 0.67104036, 0.694865, 0.73713857, 0.7570168, 1.8324144, 0.7550716, 0.67975765, 0.9713706, 0.65571123, 0.9516453, 0.86792046, 0.66686463, 0.6791075, 0.9666843, 1.1820081, 0.70512, 1.2043861, 1.1284647, 0.8410009, 1.8595991, 0.94721323, 0.84939355, 0.8083561, 0.92896247, 1.0277542, 1.1982716, 0.9422043, 0.82670265, 1.3568825, 1.1287276, 0.9230107, 0.9644479, 0.82110137, 1.3833125, 1.2462778, 0.83520806, 1.0086117, 1.1106999, 1.1021261, 1.1249726, 0.98483354, 1.327314, 1.2029426, 1.1556058, 1.0082427, 1.1942343, 1.542647, 1.0587642, 0.91819805, 1.4634901, 1.860392, 1.707982, 1.2285819, 3.067815, 2.8014405, 1.2248704, 1.8948162, 2.6506774, 1.7879087, 1.454777, 1.8187459, 1.3523737, 1.3634783, 1.2432423, 1.8153578, 1.3670819, 1.240846, 2.2078254, 1.7821249, 1.8377035, 1.6637233, 1.6503725, 1.7957243, 1.8239917, 1.4257959, 1.7742534, 2.2795913, 1.7602993, 2.0427222, 1.4427822, 2.2599154, 1.9038026, 1.5498465, 2.132893, 2.0722804, 1.5760667, 1.7153838, 3.1831663, 3.0717874, 2.2580922, 1.7881199, 2.0421717, 4.0588045, 1.7760829, 2.6453354, 2.6442978, 1.9068074, 3.3609865, 3.2229228, 2.3411238, 2.4139814, 2.140908, 1.8855458, 2.0491533, 2.6041937, 2.8395123, 2.726341, 1.7833405, 1.980508, 2.4654682, 2.9397218, 2.1812806, 2.5039928, 2.7225611, 3.2055333, 2.824343, 2.56744, 2.496676, 2.0798595, 2.4141955, 2.1873617, 2.1380625, 2.417028, 3.012318, 3.0971143, 2.5066776, 4.018233, 2.8293273, 2.9708135, 2.8628206, 2.9206717, 3.029767, 2.7352693, 3.6883295, 3.3838303, 4.67798, 3.2475698, 3.2501335, 2.4187086, 2.952964, 3.6252716, 2.3960133, 4.847249, 3.412417, 4.8948684, 4.5827594, 3.5459285, 3.191851, 4.4493647, 3.2680502, 5.6734986, 3.5254068, 3.0859919, 4.242838, 3.2212055, 3.497644, 3.677658, 3.2205312, 4.599012, 4.513673, 3.9394011, 4.5993457, 3.5866785, 5.063551, 3.7411938, 4.254855, 4.011715, 3.7463374, 3.1295736, 5.6588993, 3.8781748, 3.6525197, 3.8846586, 5.0789695, 4.1768484, 4.226266, 4.138898, 4.7913594, 5.2213993, 4.2669096, 3.5819302, 3.6302912, 2.7632751, 4.789828, 3.6820624, 4.023943, 5.5270042, 4.8280954, 4.788839, 4.793308, 4.122248, 6.3655567, 3.656916, 5.5731864, 3.9969096, 4.897587, 5.2432504, 4.174113, 5.288321, 3.887116, 4.9726825, 4.743837, 4.0510397, 5.0936537, 3.743953, 6.233922, 6.1895947, 5.882373, 5.745754, 4.731286, 3.7896678, 5.5709677, 6.671407, 3.9678106, 4.824086, 5.941447, 5.4158645, 5.4209256, 4.1665297, 4.0565863, 4.270968, 4.977373, 7.2061524, 5.2739787, 5.7174616, 5.427431, 6.70102, 6.476368, 5.9009895, 6.099128, 6.9130664, 7.015352, 4.78999, 6.1351476, 6.0761714, 5.491715, 6.152791, 6.486664, 5.7997255, 5.417246, 8.198225, 6.9479585, 5.0017886, 6.2912416, 4.9806676, 6.281353, 5.688881, 6.141599, 7.0590353, 6.627374, 7.664702, 6.5257225, 7.861973, 8.013114, 6.0079246, 5.300538, 7.449917, 6.9667773, 5.872825, 6.8172674, 5.420929, 7.5777793, 8.153764, 7.4340525, 6.6340103, 5.8683534, 7.8793626, 9.102963, 7.499341, 7.1140623, 7.9625163, 7.355009, 6.3863883, 6.8606725, 6.107802, 7.2106695, 6.7882123, 9.465298, 6.744434, 8.285885, 7.472915, 7.2770524, 6.5024, 6.779035, 10.1454735, 10.21564, 9.72549, 8.395349, 10.354094, 6.966901, 7.713667, 7.257531, 8.183892, 12.382181, 6.855338, 7.9149275, 10.508953, 9.491793, 7.545659, 7.4900246, 9.969078, 7.8343725, 8.256981, 11.677448, 6.721618, 8.886631, 9.621118, 12.6784315, 7.459863, 9.306908, 7.785804, 6.675118, 7.337874, 7.248848, 8.211712, 10.690726, 9.512676, 10.02101, 8.300359, 9.570182, 10.211438, 7.8105063, 8.590146, 7.9567866, 8.861129, 8.067232, 9.50185, 10.8557005, 12.160094, 10.451609, 9.260268, 10.475052, 9.625143, 10.863873, 9.135273, 9.868096, 9.2406, 12.858564, 9.76616, 12.841839, 11.176384, 8.343913, 10.084861, 9.876098, 11.220028, 14.930982, 10.392987, 9.239464, 11.21044, 8.584534, 9.446764, 9.495218, 11.338685, 12.07776, 10.356793, 11.94196, 9.781911, 13.611214, 11.665456, 10.5431795, 12.610695, 10.00934, 11.345974, 16.47061, 11.6, 10.617674, 11.876498, 10.853148, 9.520694, 12.789697, 10.374928, 12.2793, 12.097722, 12.092447, 10.906119, 9.331522, 9.786119, 9.0614805, 13.542317, 10.965574, 10.96466, 12.078304, 14.281419, 11.162003, 12.436515, 16.628946, 12.741296, 11.831136, 16.986256, 15.157038, 13.00116, 12.635233, 10.716093, 17.736921, 11.754222, 11.197147, 12.8961, 11.831532, 15.627441, 12.784003, 15.339419, 18.05675, 13.65829, 13.805785, 15.2957535, 15.647431, 16.892654, 13.514229, 13.078784, 17.110592, 15.472323, 13.940336, 12.576158, 16.551638, 14.063057, 16.479248, 12.350107, 12.776074, 12.545423, 11.406876, 12.736177, 15.019438, 16.343145, 17.302189, 17.68368, 13.796104, 14.102408, 14.140014, 12.139987, 14.027492, 11.90347, 5.557056, 0.77418655, 0.57918686, 0.60802454, 0.548505, 0.46589324, 0.45601496, 0.8490204, 0.43015084, 0.3721653, 0.33789933, 0.6478141, 0.40679514, 0.4212835, 0.29711553, 0.3913094, 0.5498155, 0.6638782, 0.3784698, 0.3270433, 0.4534357, 0.4793974, 0.42367822, 0.46252233, 0.5762483, 0.5715774, 0.5515111, 0.35977355, 0.45947197, 0.35176778, 0.62334245, 0.5098863, 0.41258472, 0.38384804, 0.2803642, 0.356408, 1.0657762, 0.41457295, 0.24446617, 0.4445243, 0.6084134, 0.50960356, 0.36467305, 0.3882699, 0.31266215, 0.30976677, 1.640822, 0.3859121, 0.48678088, 2.0814078, 0.5820198, 0.6898487, 0.45081946, 0.6336517, 0.51323426, 0.571003, 0.5998073, 0.5339108, 0.7144231, 0.88207084, 0.6316439, 0.45893887, 2.5205288, 0.61212504, 0.51336414, 0.6597574, 0.90073764, 0.5297371, 0.9716242, 0.61647695, 0.5617897, 0.62209564, 0.70779204, 0.5579609, 0.482001, 0.99107534, 0.8403749, 0.76608944, 0.9169639, 0.5751073, 0.7620099, 1.1611314, 0.7065268, 0.64310014, 0.73317426, 0.9041121, 1.0712243, 1.4127713, 0.7402087, 0.8631798, 0.8475452, 1.9674925, 0.9522595, 0.62233055, 0.66678536, 0.91873634, 4.127765, 1.0154082, 1.0439405, 0.80196714, 0.750072, 0.7949071, 1.2582773, 0.987676, 2.014347, 0.7387977, 0.8789858, 1.5645497, 1.0902567, 0.8879123, 0.76589745, 0.78094065, 1.861119, 1.0919539, 0.93243235, 1.3380157, 1.6610087, 1.0370384, 0.90631366, 1.1698148, 1.5353471, 1.6625667, 1.3899263, 1.2040371, 1.2829093, 1.652931, 1.6731554, 1.0361683, 1.5070338, 1.5160563, 2.168475, 1.4816574, 1.8949814, 1.6804732, 1.2966412, 1.4280262, 1.7379209, 3.4518757, 2.7673025, 2.436047, 1.6352017, 2.200757, 1.5002953, 2.4611666, 1.3320394, 1.5621825, 2.1693306, 1.9375602, 1.5766964, 2.3007617, 1.8925283, 1.3497466, 1.5438923, 1.8980658, 2.5762358, 1.8436583, 2.3222115, 2.493784, 1.8163502, 1.2759616, 3.1842835, 1.9057369, 2.2613404, 1.997732, 1.8694358, 2.5533345, 2.1568935, 1.5447079, 1.7065902, 3.9740484, 2.6716404, 2.2811713, 1.8334036, 1.846853, 2.403504, 2.1219132, 3.0395231, 1.7438682, 2.0468907, 2.0513732, 2.1678522, 5.1193924, 2.5433364, 1.9512678, 2.9579966, 2.4560695, 2.185567, 2.0974865, 2.075539, 3.4311705, 1.6092967, 1.9683759, 2.9352572, 3.6670132, 3.2433643, 3.0833263, 2.353162, 4.284019, 2.8897693, 2.573864, 2.8517606, 2.1397855, 2.4807198, 2.4090416, 2.2846007, 2.8228495, 2.5147407, 4.3468213, 2.5757174, 2.3359852, 2.2567408, 2.5506759, 2.138182, 2.371737, 2.4554753, 4.542796, 2.4865842, 3.624895, 3.36458, 2.842485, 2.5484557, 4.6677647, 2.861906, 3.3936157, 2.7303061, 3.1373968, 2.9612217, 5.3050275, 2.889996, 4.2502785, 2.9834065, 2.8523557, 2.4221704, 5.2492094, 4.4353237, 2.8589447, 2.6363575, 3.1983669, 5.0484595, 3.967311, 7.966497, 3.2338884, 3.3239882, 4.676158, 4.6794314, 2.9724154, 3.6806479, 4.391997, 5.1410403, 4.3474846, 3.0811129, 4.895758, 3.4566967, 4.9544244, 3.412483, 3.2413516, 7.9799476, 3.4099681, 4.221029, 3.0039985, 3.8074076, 3.5743005, 3.5781105, 5.1577654, 6.0060935, 6.001373, 5.5230446, 4.426999, 4.1618075, 6.2310343, 3.3347, 3.3052912, 5.955835, 3.15816, 4.8242145, 4.020975, 5.5108514, 3.8864982, 6.4274035, 4.6686583, 5.373592, 4.8643622, 5.839581, 4.6050982, 4.265063, 4.8935995, 4.0490456, 5.6697745, 5.319422, 4.421227, 4.465689, 4.178317, 5.230526, 4.5938373, 5.3866663, 6.0437117, 3.467769, 6.235191, 5.044262, 4.157054, 3.7351565, 4.1146607, 4.8116674, 6.6531744, 4.284238, 5.4369054, 4.053629, 6.03367, 6.259283, 5.9701605, 4.639933, 3.8040128, 4.484313, 6.747723, 5.997655, 5.8302016, 6.303351, 8.848699, 4.92435, 4.898667, 11.91368, 4.8435917, 5.2327003, 5.9906106, 5.8593965, 4.8557353, 8.212649, 5.826162, 7.193409, 6.327472, 6.89826, 6.445279, 6.77766, 5.410072, 5.0150743, 5.64059, 5.3429775, 6.2559953, 6.5228157, 10.567216, 5.835641, 6.2327714, 5.0674334, 7.8452396, 5.7612247, 5.7608867, 5.3598332, 5.880486, 6.0412006, 5.659454, 5.679879, 8.030655, 7.3562284, 5.6856384, 6.4178123, 6.2135777, 6.896981, 6.750686, 6.0892677, 7.0807405, 5.9704256, 9.877379, 6.747122, 9.741617, 7.7225604, 7.4883113, 7.3556685, 10.570458, 7.693676, 7.823699, 7.983587, 8.713321, 8.989606, 9.241663, 7.589858, 7.5420785, 8.469137, 9.619211, 7.473199, 10.060779, 8.030053, 7.3896093, 11.143968, 8.600138, 6.721271, 7.642903, 9.840191, 8.0058975, 7.9675803, 7.5857806, 8.404517, 7.859863, 9.329836, 8.055905, 8.585104, 9.312755, 8.532584, 10.083679, 9.880305, 8.992527, 9.400182, 6.9164066, 8.650134, 7.860915, 9.793296, 8.6478, 7.289265, 8.460469, 10.093581, 11.898513, 8.253452, 8.788581, 11.2082, 10.802805, 9.034604, 9.73249, 9.787774, 10.748146, 12.369763, 7.9374614, 8.300827, 10.342098, 10.759742, 12.310276, 10.413574, 9.5609865, 11.863828, 15.555205, 11.438952, 8.409741, 7.4164667, 10.156127, 14.985262, 9.74642, 8.4922285, 9.960098, 12.773959, 12.4286, 9.304929, 10.276235, 8.145298, 8.35358, 11.053792, 12.252936, 14.767834, 10.394206, 8.686387, 9.393904, 10.935731, 8.29371, 15.85042, 9.703153, 7.2662115, 12.780338, 9.41216, 9.557358, 10.123588, 8.933081, 16.935097, 9.3073, 14.681076, 9.56047, 12.93751, 10.059008, 8.091336, 10.589004, 11.83263, 9.074482, 18.358414, 11.900548, 10.880144, 11.689816, 15.613918, 13.126496, 11.739843, 11.698459, 11.981954, 11.189849, 17.83788, 10.988367, 10.537496, 11.635481, 9.757371, 9.31293, 11.547264, 13.261576, 16.295397, 14.576361, 10.73971, 11.951349, 11.044151, 15.425241, 12.029635, 12.043542, 10.500347, 11.258056, 14.225451, 13.056415, 15.423847, 12.742605, 9.595782
disc_val_acc: 0.0, 0.5426587301587301, 0.6607142857142857, 0.7688492063492064, 0.7286706349206349, 0.8035714285714286, 0.8561507936507936, 0.814484126984127, 0.8030753968253969, 0.814484126984127, 0.8616071428571429, 0.8492063492063492, 0.814484126984127, 0.8482142857142857, 0.8387896825396826, 0.8824404761904762, 0.871031746031746, 0.847718253968254, 0.8799603174603174, 0.8819444444444444, 0.8869047619047619, 0.9057539682539683, 0.8313492063492064, 0.8878968253968254, 0.8804563492063492, 0.8973214285714286, 0.8725198412698413, 0.847718253968254, 0.8214285714285714, 0.8020833333333334, 0.9067460317460317, 0.8804563492063492, 0.904265873015873, 0.8268849206349206, 0.8893849206349206, 0.9151785714285714, 0.8665674603174603, 0.8442460317460317, 0.9241071428571429, 0.9032738095238095, 0.8189484126984127, 0.8799603174603174, 0.878968253968254, 0.904265873015873, 0.8720238095238095, 0.8829365079365079, 0.9027777777777778, 0.9206349206349206, 0.9171626984126984, 0.8973214285714286, 0.9186507936507936, 0.8953373015873016, 0.8774801587301587, 0.9047619047619048, 0.9270833333333334, 0.9092261904761905, 0.8244047619047619, 0.8943452380952381, 0.9092261904761905, 0.9221230158730159, 0.9305555555555556, 0.8824404761904762, 0.8888888888888888, 0.9295634920634921, 0.8625992063492064, 0.9181547619047619, 0.9092261904761905, 0.9012896825396826, 0.9017857142857143, 0.9012896825396826, 0.9201388888888888, 0.8938492063492064, 0.9067460317460317, 0.9166666666666666, 0.8377976190476191, 0.9017857142857143, 0.9206349206349206, 0.9017857142857143, 0.9236111111111112, 0.8953373015873016, 0.9146825396825397, 0.9246031746031746, 0.9250992063492064, 0.9072420634920635, 0.8898809523809523, 0.9246031746031746, 0.8908730158730159, 0.9067460317460317, 0.9186507936507936, 0.8596230158730159, 0.9126984126984127, 0.9161706349206349, 0.9216269841269841, 0.9161706349206349, 0.9057539682539683, 0.8953373015873016, 0.9087301587301587, 0.9255952380952381, 0.8948412698412699, 0.9092261904761905, 0.9171626984126984, 0.9241071428571429, 0.9246031746031746, 0.9052579365079365, 0.9136904761904762, 0.9280753968253969, 0.9280753968253969, 0.9171626984126984, 0.9107142857142857, 0.9226190476190477, 0.9211309523809523, 0.902281746031746, 0.9191468253968254, 0.9270833333333334, 0.9250992063492064, 0.9275793650793651, 0.9112103174603174, 0.9280753968253969, 0.9370039682539683, 0.8963293650793651, 0.8993055555555556, 0.8908730158730159, 0.9231150793650794, 0.8680555555555556, 0.8745039682539683, 0.9300595238095238, 0.8983134920634921, 0.8824404761904762, 0.9002976190476191, 0.9250992063492064, 0.9077380952380952, 0.9186507936507936, 0.9226190476190477, 0.9290674603174603, 0.9117063492063492, 0.9171626984126984, 0.9310515873015873, 0.9126984126984127, 0.9136904761904762, 0.9097222222222222, 0.9146825396825397, 0.9236111111111112, 0.9047619047619048, 0.9146825396825397, 0.9241071428571429, 0.9181547619047619, 0.9002976190476191, 0.9126984126984127, 0.9092261904761905, 0.9166666666666666, 0.8943452380952381, 0.9112103174603174, 0.9260912698412699, 0.9037698412698413, 0.9151785714285714, 0.9211309523809523, 0.9236111111111112, 0.8859126984126984, 0.8854166666666666, 0.9087301587301587, 0.9156746031746031, 0.9166666666666666, 0.8859126984126984, 0.9201388888888888, 0.9146825396825397, 0.9002976190476191, 0.9131944444444444, 0.8779761904761905, 0.876984126984127, 0.9136904761904762, 0.9131944444444444, 0.9191468253968254, 0.9181547619047619, 0.9290674603174603, 0.9146825396825397, 0.9141865079365079, 0.9052579365079365, 0.9320436507936508, 0.9275793650793651, 0.9117063492063492, 0.9027777777777778, 0.9221230158730159, 0.9117063492063492, 0.9047619047619048, 0.9186507936507936, 0.9176587301587301, 0.910218253968254, 0.9141865079365079, 0.9206349206349206, 0.9216269841269841, 0.9241071428571429, 0.9260912698412699, 0.9186507936507936, 0.9122023809523809, 0.902281746031746, 0.9216269841269841, 0.9007936507936508, 0.9131944444444444, 0.9226190476190477, 0.9186507936507936, 0.9231150793650794, 0.9146825396825397, 0.9206349206349206, 0.9052579365079365, 0.9107142857142857, 0.8898809523809523, 0.9072420634920635, 0.9136904761904762, 0.9250992063492064, 0.9196428571428571, 0.8933531746031746, 0.9265873015873016, 0.8809523809523809, 0.9141865079365079, 0.8943452380952381, 0.8928571428571429, 0.908234126984127, 0.9191468253968254, 0.9027777777777778, 0.9166666666666666, 0.8978174603174603, 0.9146825396825397, 0.9350198412698413, 0.9002976190476191, 0.9265873015873016, 0.9216269841269841, 0.9146825396825397, 0.9186507936507936, 0.8988095238095238, 0.9037698412698413, 0.9201388888888888, 0.9002976190476191, 0.9216269841269841, 0.8933531746031746, 0.9181547619047619, 0.9032738095238095, 0.9097222222222222, 0.9151785714285714, 0.9206349206349206, 0.8958333333333334, 0.9176587301587301, 0.9166666666666666, 0.9191468253968254, 0.9047619047619048, 0.9176587301587301, 0.9141865079365079, 0.9201388888888888, 0.9122023809523809, 0.8988095238095238, 0.9196428571428571, 0.9246031746031746, 0.9186507936507936, 0.9365079365079365, 0.9122023809523809, 0.9285714285714286, 0.9146825396825397, 0.8998015873015873, 0.9181547619047619, 0.9117063492063492, 0.9161706349206349, 0.9191468253968254, 0.8898809523809523, 0.9265873015873016, 0.9067460317460317, 0.9206349206349206, 0.9211309523809523, 0.9126984126984127, 0.9260912698412699, 0.9037698412698413, 0.9275793650793651, 0.9037698412698413, 0.9201388888888888, 0.9246031746031746, 0.9191468253968254, 0.9241071428571429, 0.9077380952380952, 0.8968253968253969, 0.9032738095238095, 0.9047619047619048, 0.9171626984126984, 0.9221230158730159, 0.9077380952380952, 0.902281746031746, 0.9290674603174603, 0.9201388888888888, 0.904265873015873, 0.9241071428571429, 0.9216269841269841, 0.9255952380952381, 0.935515873015873, 0.9280753968253969, 0.9171626984126984, 0.9087301587301587, 0.9092261904761905, 0.9181547619047619, 0.9236111111111112, 0.902281746031746, 0.9117063492063492, 0.9236111111111112, 0.9067460317460317, 0.910218253968254, 0.9122023809523809, 0.9300595238095238, 0.9151785714285714, 0.9166666666666666, 0.9146825396825397, 0.9072420634920635, 0.9146825396825397, 0.9250992063492064, 0.9270833333333334, 0.9067460317460317, 0.9122023809523809, 0.9360119047619048, 0.9176587301587301, 0.9320436507936508, 0.9112103174603174, 0.9250992063492064, 0.9226190476190477, 0.8963293650793651, 0.9126984126984127, 0.9077380952380952, 0.9275793650793651, 0.9087301587301587, 0.9117063492063492, 0.9196428571428571, 0.9246031746031746, 0.9176587301587301, 0.9270833333333334, 0.9206349206349206, 0.9226190476190477, 0.9315476190476191, 0.9146825396825397, 0.9126984126984127, 0.9047619047619048, 0.9166666666666666, 0.9275793650793651, 0.9112103174603174, 0.9057539682539683, 0.9151785714285714, 0.9236111111111112, 0.9146825396825397, 0.9221230158730159, 0.9315476190476191, 0.9196428571428571, 0.9255952380952381, 0.9196428571428571, 0.9241071428571429, 0.9087301587301587, 0.9290674603174603, 0.9181547619047619, 0.9206349206349206, 0.9107142857142857, 0.9241071428571429, 0.9236111111111112, 0.9037698412698413, 0.904265873015873, 0.90625, 0.9191468253968254, 0.8943452380952381, 0.9196428571428571, 0.9250992063492064, 0.9171626984126984, 0.9216269841269841, 0.8958333333333334, 0.9196428571428571, 0.9181547619047619, 0.90625, 0.9136904761904762, 0.9216269841269841, 0.9221230158730159, 0.90625, 0.9211309523809523, 0.90625, 0.9117063492063492, 0.9280753968253969, 0.9166666666666666, 0.9117063492063492, 0.8938492063492064, 0.9285714285714286, 0.9191468253968254, 0.9310515873015873, 0.9250992063492064, 0.9211309523809523, 0.9345238095238095, 0.9211309523809523, 0.9087301587301587, 0.9260912698412699, 0.908234126984127, 0.9250992063492064, 0.9176587301587301, 0.9136904761904762, 0.9196428571428571, 0.9211309523809523, 0.9315476190476191, 0.9216269841269841, 0.9370039682539683, 0.9241071428571429, 0.9191468253968254, 0.908234126984127, 0.9181547619047619, 0.9295634920634921, 0.9166666666666666, 0.9181547619047619, 0.9166666666666666, 0.9246031746031746, 0.9241071428571429, 0.9206349206349206, 0.9047619047619048, 0.9231150793650794, 0.9032738095238095, 0.9122023809523809, 0.9270833333333334, 0.9236111111111112, 0.9285714285714286, 0.9136904761904762, 0.9092261904761905, 0.9141865079365079, 0.9290674603174603, 0.9126984126984127, 0.9310515873015873, 0.9246031746031746, 0.9186507936507936, 0.9186507936507936, 0.90625, 0.9176587301587301, 0.9176587301587301, 0.9260912698412699, 0.9136904761904762, 0.9166666666666666, 0.9156746031746031, 0.9037698412698413, 0.9181547619047619, 0.9191468253968254, 0.9012896825396826, 0.9211309523809523, 0.9226190476190477, 0.9112103174603174, 0.9141865079365079, 0.9260912698412699, 0.9176587301587301, 0.9340277777777778, 0.9176587301587301, 0.9141865079365079, 0.9126984126984127, 0.9236111111111112, 0.9250992063492064, 0.9241071428571429, 0.9290674603174603, 0.9117063492063492, 0.9186507936507936, 0.9255952380952381, 0.9255952380952381, 0.90625, 0.9206349206349206, 0.9221230158730159, 0.902281746031746, 0.9176587301587301, 0.9250992063492064, 0.8968253968253969, 0.9131944444444444, 0.9226190476190477, 0.9196428571428571, 0.9295634920634921, 0.8953373015873016, 0.9126984126984127, 0.9246031746031746, 0.9161706349206349, 0.9250992063492064, 0.9166666666666666, 0.9255952380952381, 0.9151785714285714, 0.9007936507936508, 0.9221230158730159, 0.9186507936507936, 0.9146825396825397, 0.9206349206349206, 0.9072420634920635, 0.908234126984127, 0.9191468253968254, 0.9107142857142857, 0.9231150793650794, 0.9236111111111112, 0.9290674603174603, 0.9151785714285714, 0.9161706349206349, 0.9112103174603174, 0.9305555555555556, 0.9221230158730159, 0.9295634920634921, 0.9231150793650794, 0.9206349206349206, 0.9131944444444444, 0.9206349206349206, 0.9107142857142857, 0.9161706349206349, 0.9221230158730159, 0.9295634920634921, 0.9270833333333334, 0.9246031746031746, 0.9166666666666666, 0.935515873015873, 0.0, 0.6775793650793651, 0.7971230158730159, 0.7762896825396826, 0.8125, 0.8373015873015873, 0.8432539682539683, 0.7708333333333334, 0.8581349206349206, 0.878968253968254, 0.8898809523809523, 0.8229166666666666, 0.8571428571428571, 0.8695436507936508, 0.9012896825396826, 0.8784722222222222, 0.8497023809523809, 0.8368055555555556, 0.8983134920634921, 0.9146825396825397, 0.8700396825396826, 0.8621031746031746, 0.8913690476190477, 0.8918650793650794, 0.8611111111111112, 0.8635912698412699, 0.8720238095238095, 0.8978174603174603, 0.8804563492063492, 0.9092261904761905, 0.8700396825396826, 0.8700396825396826, 0.8948412698412699, 0.9012896825396826, 0.9171626984126984, 0.9206349206349206, 0.8174603174603174, 0.9112103174603174, 0.9330357142857143, 0.904265873015873, 0.8819444444444444, 0.9052579365079365, 0.9112103174603174, 0.9201388888888888, 0.9122023809523809, 0.9345238095238095, 0.7366071428571429, 0.9250992063492064, 0.8983134920634921, 0.8025793650793651, 0.9007936507936508, 0.8814484126984127, 0.9156746031746031, 0.8973214285714286, 0.9012896825396826, 0.9151785714285714, 0.8973214285714286, 0.9146825396825397, 0.8953373015873016, 0.8948412698412699, 0.90625, 0.9236111111111112, 0.7316468253968254, 0.910218253968254, 0.9295634920634921, 0.9037698412698413, 0.8938492063492064, 0.9250992063492064, 0.8978174603174603, 0.9067460317460317, 0.9171626984126984, 0.9176587301587301, 0.8993055555555556, 0.9290674603174603, 0.9300595238095238, 0.8948412698412699, 0.904265873015873, 0.9122023809523809, 0.8978174603174603, 0.9285714285714286, 0.9052579365079365, 0.8958333333333334, 0.9107142857142857, 0.9305555555555556, 0.9196428571428571, 0.9107142857142857, 0.9037698412698413, 0.8650793650793651, 0.9151785714285714, 0.9097222222222222, 0.9226190476190477, 0.847718253968254, 0.902281746031746, 0.9310515873015873, 0.9280753968253969, 0.9171626984126984, 0.7867063492063492, 0.9166666666666666, 0.910218253968254, 0.9285714285714286, 0.9226190476190477, 0.9201388888888888, 0.8888888888888888, 0.9176587301587301, 0.8576388888888888, 0.9275793650793651, 0.9181547619047619, 0.8794642857142857, 0.9112103174603174, 0.9246031746031746, 0.939484126984127, 0.9260912698412699, 0.8799603174603174, 0.9161706349206349, 0.9300595238095238, 0.9092261904761905, 0.8799603174603174, 0.9196428571428571, 0.9260912698412699, 0.9176587301587301, 0.8993055555555556, 0.8933531746031746, 0.908234126984127, 0.9241071428571429, 0.9117063492063492, 0.8998015873015873, 0.8978174603174603, 0.9300595238095238, 0.910218253968254, 0.9057539682539683, 0.8680555555555556, 0.9181547619047619, 0.8918650793650794, 0.8883928571428571, 0.9226190476190477, 0.9176587301587301, 0.8968253968253969, 0.8387896825396826, 0.8834325396825397, 0.875, 0.9002976190476191, 0.8943452380952381, 0.9226190476190477, 0.8834325396825397, 0.9231150793650794, 0.9201388888888888, 0.8908730158730159, 0.9027777777777778, 0.9126984126984127, 0.9107142857142857, 0.9057539682539683, 0.9310515873015873, 0.9161706349206349, 0.9077380952380952, 0.8953373015873016, 0.9112103174603174, 0.908234126984127, 0.8923611111111112, 0.9087301587301587, 0.9305555555555556, 0.8625992063492064, 0.9161706349206349, 0.8973214285714286, 0.9126984126984127, 0.9126984126984127, 0.9012896825396826, 0.9201388888888888, 0.9280753968253969, 0.9295634920634921, 0.8407738095238095, 0.8963293650793651, 0.9107142857142857, 0.9221230158730159, 0.9246031746031746, 0.9017857142857143, 0.9072420634920635, 0.9007936507936508, 0.9221230158730159, 0.9260912698412699, 0.9166666666666666, 0.9122023809523809, 0.8467261904761905, 0.9077380952380952, 0.9216269841269841, 0.8888888888888888, 0.9057539682539683, 0.9146825396825397, 0.9201388888888888, 0.9231150793650794, 0.8963293650793651, 0.9320436507936508, 0.9300595238095238, 0.9112103174603174, 0.873015873015873, 0.8918650793650794, 0.9052579365079365, 0.9265873015873016, 0.8735119047619048, 0.9122023809523809, 0.9107142857142857, 0.9141865079365079, 0.9216269841269841, 0.9260912698412699, 0.9285714285714286, 0.9211309523809523, 0.9156746031746031, 0.9231150793650794, 0.8824404761904762, 0.9285714285714286, 0.9226190476190477, 0.9201388888888888, 0.9136904761904762, 0.9241071428571429, 0.9300595238095238, 0.9176587301587301, 0.8849206349206349, 0.9221230158730159, 0.8938492063492064, 0.9097222222222222, 0.9221230158730159, 0.9250992063492064, 0.8913690476190477, 0.9226190476190477, 0.9166666666666666, 0.9166666666666666, 0.9141865079365079, 0.9241071428571429, 0.8844246031746031, 0.9255952380952381, 0.9032738095238095, 0.9211309523809523, 0.9280753968253969, 0.939484126984127, 0.8764880952380952, 0.8998015873015873, 0.9221230158730159, 0.9255952380952381, 0.9156746031746031, 0.8888888888888888, 0.9112103174603174, 0.8298611111111112, 0.9131944444444444, 0.9122023809523809, 0.8983134920634921, 0.9047619047619048, 0.9300595238095238, 0.9122023809523809, 0.8928571428571429, 0.8814484126984127, 0.9007936507936508, 0.9231150793650794, 0.8963293650793651, 0.9156746031746031, 0.8913690476190477, 0.9186507936507936, 0.9186507936507936, 0.8670634920634921, 0.9191468253968254, 0.9126984126984127, 0.9290674603174603, 0.9246031746031746, 0.9330357142857143, 0.9211309523809523, 0.908234126984127, 0.8958333333333334, 0.9067460317460317, 0.9126984126984127, 0.910218253968254, 0.9231150793650794, 0.9002976190476191, 0.9270833333333334, 0.9280753968253969, 0.9126984126984127, 0.9260912698412699, 0.9092261904761905, 0.9241071428571429, 0.9007936507936508, 0.9255952380952381, 0.9072420634920635, 0.9211309523809523, 0.9181547619047619, 0.9246031746031746, 0.908234126984127, 0.9092261904761905, 0.9280753968253969, 0.9161706349206349, 0.9315476190476191, 0.9181547619047619, 0.902281746031746, 0.9186507936507936, 0.9236111111111112, 0.9211309523809523, 0.9156746031746031, 0.9206349206349206, 0.9067460317460317, 0.9171626984126984, 0.9310515873015873, 0.8928571428571429, 0.9206349206349206, 0.9305555555555556, 0.9295634920634921, 0.9275793650793651, 0.9186507936507936, 0.9057539682539683, 0.9255952380952381, 0.910218253968254, 0.9320436507936508, 0.9067460317460317, 0.9117063492063492, 0.9206349206349206, 0.9201388888888888, 0.9360119047619048, 0.9136904761904762, 0.8923611111111112, 0.9067460317460317, 0.9196428571428571, 0.9077380952380952, 0.8938492063492064, 0.9191468253968254, 0.9280753968253969, 0.8705357142857143, 0.9345238095238095, 0.9231150793650794, 0.9241071428571429, 0.9181547619047619, 0.9290674603174603, 0.9072420634920635, 0.9226190476190477, 0.9191468253968254, 0.9156746031746031, 0.9122023809523809, 0.9146825396825397, 0.9156746031746031, 0.9295634920634921, 0.9260912698412699, 0.9241071428571429, 0.9176587301587301, 0.9161706349206349, 0.9161706349206349, 0.8943452380952381, 0.9285714285714286, 0.9226190476190477, 0.9290674603174603, 0.8943452380952381, 0.9186507936507936, 0.9265873015873016, 0.9305555555555556, 0.9226190476190477, 0.9181547619047619, 0.9260912698412699, 0.9325396825396826, 0.9156746031746031, 0.9146825396825397, 0.9226190476190477, 0.9241071428571429, 0.9315476190476191, 0.9146825396825397, 0.9171626984126984, 0.9325396825396826, 0.9171626984126984, 0.9300595238095238, 0.9007936507936508, 0.9280753968253969, 0.8878968253968254, 0.9146825396825397, 0.9241071428571429, 0.9156746031746031, 0.8849206349206349, 0.9176587301587301, 0.9186507936507936, 0.904265873015873, 0.9131944444444444, 0.8943452380952381, 0.910218253968254, 0.9226190476190477, 0.9181547619047619, 0.9265873015873016, 0.9151785714285714, 0.9166666666666666, 0.9092261904761905, 0.9201388888888888, 0.9156746031746031, 0.9047619047619048, 0.9156746031746031, 0.9231150793650794, 0.9156746031746031, 0.9057539682539683, 0.9176587301587301, 0.9171626984126984, 0.9260912698412699, 0.9260912698412699, 0.9250992063492064, 0.9265873015873016, 0.9171626984126984, 0.9280753968253969, 0.9241071428571429, 0.9236111111111112, 0.9126984126984127, 0.9146825396825397, 0.9146825396825397, 0.9097222222222222, 0.9345238095238095, 0.9236111111111112, 0.9345238095238095, 0.9151785714285714, 0.9285714285714286, 0.9246031746031746, 0.9300595238095238, 0.9136904761904762, 0.90625, 0.9131944444444444, 0.9275793650793651, 0.908234126984127, 0.9186507936507936, 0.9231150793650794, 0.9265873015873016, 0.9211309523809523, 0.9072420634920635, 0.90625, 0.9260912698412699, 0.9295634920634921, 0.9191468253968254, 0.9156746031746031, 0.9092261904761905, 0.9221230158730159, 0.9181547619047619, 0.9107142857142857, 0.8923611111111112, 0.9141865079365079, 0.9246031746031746, 0.9340277777777778, 0.9122023809523809, 0.8958333333333334, 0.9241071428571429, 0.9265873015873016, 0.9181547619047619, 0.9047619047619048, 0.902281746031746, 0.9275793650793651, 0.9290674603174603, 0.9345238095238095, 0.9305555555555556, 0.9181547619047619, 0.9126984126984127, 0.910218253968254, 0.9176587301587301, 0.9370039682539683, 0.9290674603174603, 0.9161706349206349, 0.9389880952380952, 0.9037698412698413, 0.9275793650793651, 0.9345238095238095, 0.9206349206349206, 0.9280753968253969, 0.9260912698412699, 0.9255952380952381, 0.9325396825396826, 0.8948412698412699, 0.9255952380952381, 0.9191468253968254, 0.9305555555555556, 0.9236111111111112, 0.9236111111111112, 0.9384920634920635, 0.9285714285714286, 0.9141865079365079, 0.9325396825396826, 0.90625, 0.9141865079365079, 0.9176587301587301, 0.9226190476190477, 0.9032738095238095, 0.9136904761904762, 0.9255952380952381, 0.9216269841269841, 0.9181547619047619, 0.9166666666666666, 0.8978174603174603, 0.9255952380952381, 0.9246031746031746, 0.9250992063492064, 0.9340277777777778, 0.9365079365079365, 0.9166666666666666, 0.9216269841269841, 0.8993055555555556, 0.9097222222222222, 0.9260912698412699, 0.9221230158730159, 0.9231150793650794, 0.8988095238095238, 0.9226190476190477, 0.9181547619047619, 0.9196428571428571, 0.9255952380952381, 0.9146825396825397, 0.9236111111111112, 0.9201388888888888, 0.9236111111111112, 0.9345238095238095

